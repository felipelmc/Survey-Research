[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pesquisa de Survey",
    "section": "",
    "text": "Surveys são, hoje, uma das mais difundidas ferramentas de pesquisa social. Por meio da aplicação de questionários estruturados a uma amostra de determinada população, governos estimam taxas de desemprego e de pobreza; políticos definem estratégias de campanha; institutos de pesquisa identificam tendências de comportamento de diferentes grupos sociais; e empresas investigam a satisfação de seus clientes e testam ações de marketing. A proposta deste curso é oferecer uma introdução panorâmica a este tipo de pesquisa. [ementa]\n\nProfessor: Fernando Meireles (IESP-UERJ) [homepage]\nSemestre: 2025.1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>index.html</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html",
    "href": "aulas/aula-2.html",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "",
    "text": "2.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#anotações-das-leituras",
    "href": "aulas/aula-2.html#anotações-das-leituras",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "",
    "text": "2.1.1 Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., & Tourangeau, R. (2011). Survey methodology. John Wiley & Sons. Cap. 1.\n\nThere are four perspectives on surveys that are worth describing: the purposes to which surveys were put, the development of question design, the development of sampling methods, and the development of data collection methods. (p. 15)\n\n\nHowever, the market research and polling organizations were doing large numbers of interviews, using newly hired people with no special background in the social sciences. Of necessity, researchers needed to specify more carefully the information sought by the survey. Further, researchers found that small changes in the wording of an attitude question sometimes had unusually large effects on the answers. (p. 16)\n\n\nAlthough the theory of probability was established in the 18th century, its application to practical sample survey work was delayed until the 20th century. The first applications were the taking of a “1 in N” systematic selection from census returns. There were “probability samples”; that is, every record had a known nonzero chance of selection into the sample. (p. 17)\n\n\nAlthough all police agencies in the United States could be asked to report statistics from their record systems, it was financially impossible to ask all persons in the United States to report their individual victimizations. If a “representative” sample could be identified, this would make the victimization survey possible. Thus, in contrast to the record data, the survey would be subject to “sampling error” (i.e., errors in statistics because of the omission of some persons in the population). (p. 20)\n\n\nThe methodological work suggested that the survey would tend to underreport less important crimes, apparently because they were difficult to remember. Finally, both systems have trouble with repeated incidents of the same character. (p. 20)\n\n\nThe sample is restricted to persons who are household members, excluding the homeless, those in institutions, and in group quarters. The sample is clustered into hundreds of different sample areas (usually counties or groups of counties) and the sample design is repeated samples of households from the same areas over the years of the study. (p. 21)\n\n\nEach month, the Surveys of Consumers (SOC) telephones a sample of the phone numbers, locates the household numbers among them, and selects one adult from those in the households. The target is the full household population, but to reduce costs, only persons in households with telephones are eligible. […]. After the data are collected, there are statistical adjustments made to the estimates in an attempt to repair the omission of nontelephone households and nonresponse. (p. 25-26)\n\n\nThe design features of the surveys are tailored to fit their purposes. The target populations for these surveys are varied – for example, the NCVS aims to describe adults and children age 12 or older, whereas the CES survey has a target population of employers. The NCVS, NSDUH, and NAEP surveys all draw samples of households in two steps, first sampling geographic areas, then sampling from created lists of households within those areas. The Surveys of Consumers and BRFSS draw samples of telephone numbers from all possible numbers within working area codes and exchanges. Then the numbers are screened to find those associated with households. (p. 34)\n\n\nSurvey methodology seeks to identify principles about the design, collection, processing, and analysis of surveys that are linked to the cost and quality of survey estimates. This means that the field focuses on improving quality within cost constraints, or, alternatively, reducing costs for some fixed level of quality. “Quality” is defined within a framework labeled the total survey error paradigm. (p. 35)\n\n\nBecause survey methodology has this inherently multidisciplinary nature, it has only recently developed as a unified field. (p. 35)\n\n\nIn a similar way, in some cases there are only imperfect solutions for survey design problems. All the approaches available to solve a particular problem may have pluses and minuses. The survey methodologist has to decide which of a set of imperfect options is best. Again, the total survey error approach is to consider the various ways that the options will affect the quality of the resulting data and choose the one that, on balance, will produce the most valuable data. (p. 37)\n\n\n\n2.1.2 Zaller, J., & Feldman, S. (1992). A simple theory of the survey response: Answering questions versus revealing preferences. American Journal of Political Science, 579-616.\n\nVirtually, all public opinion research proceeds on the assumption that citizens possess reasonably well formed attitudes on major political issues and that surveys are passive measures of these attitudes. The standard view is that when survey respondents say they favor X they are simply describing a preexisting state of feeling favorably toward X.\nAccumulating evidence on the vagaries of mass political attitudes, however, has made this view increasingly dubious. If, as is well known, prople are asked the same question in a series of interviews, their attitude reports are highly changeable. Many, as much evidence also shows, react strongly to the context in which the questions are asked, to the order in which options are presented, and to wholly nonsubstantive changes in question wording. These phenomena are more than methodological curiosities; they raise serious doubts about what public opinion surveys measure.\nIn view of this, we propose a new undestanding of the mass survey response. Most citizens, we argue, simply do not possess performed attitudes at the level of specificity demanded in surveys. Rather, they carry around in their heads a mix of only partially consistent ideas and considerations. When questioned, they call to mind a sample of these ideas, including an oversample of ideas made salient by the questionnaiere and other recent events, and use them to choose among the options offered. But their choices do not, in most cases, reflect anything that can be described as true attitudes; rather, they reflect the thoughts that are most accessible in memory at the moment of response. (p. 579-580)\n\n\nOne of the most unsettling findings of opinion research has been the discovery of a large component of randomness in most people’s answers to survey questions. If the same people are asked the same question in repeated interviews, only about half of them give the same answers. (p. 580)\n\n\nThe literature on response effects thus makes it clear that survey questions do not simply measure public opinion. They also shape and channel it by the manner in which they frame issues, order the alternatives, and otherwise set the context of the question. THis has led researchers to a conclusion that seems indisputable but that is fundamentally at odds with the assumptions of most political scientists about the nature of political attitudes: namely, people do not merely reveal preexisting attitudes on surveys; to some considerable extent, people are using the questionnaire to decide what their “attitudes” are (Bishop, Oldendick, and Tuchfarber 1984;; Zaller 1984; Feldman 1990).\n\n\nCurrent attitude models seem quite irrelevant to these observations. The reason for Sartori’s vacillation is not, as students of Converse might say, that he has no opinion on this question, nor is it that, as users of measurement error models might say, Sartori has a “true attitude” that Hochschild is unable to measure reliably. It is rather that Sartori has conflicting opinions, or at least conflicting considerations, that lead him to give different responses at different times, depending on how he thinks about the issue. (p. 584)\n\n\nAXIOM 1: The ambivalence axiom. Most people possess opposing considerations on most issues, that is, considerations that might lead them to decide the issue either way. (p. 585)\nAXIOM 2: The response axiom. Individuals answer survey questions by averaging across the considerations that happen to be salient at the moment of response, where saliency is determined by the accessbility axiom. (p. 586)\nAXIOM 3: The accessibility axiom. The accessibility of any given consideration depends on a stochastic sampling process, where considerations that have been recently thought about are somewhat more likely to be sampled. (p. 586)\n\n\nFor a case in which a person devotes great thought and attention to an issue, Axiom 3 implies that there may be multiple considerations salient in memory at the moment of answering questions about the issue and hence many considerations to be averaged across. But a person who rarely thinks about an issue and who is confronted by an interview situatin that requires a succession of quick answers (Feldman 1990) may have only one consideration immediately available in memory, in which case the averaging rule reduces to answering on the basis of a single “top-of-the-head” consideration, as suggested by Taylor and Fiske. (p. 586)\n\n\nThe two types of probes [page 587] are clearly not equivalent. The “retrospective” probes, which were posed after people had answered the question in the normal way, were designed to find out what exactly was on people’s minds at the moment of response. The “prospective” or “stop-and-think” probes, on the other hand, were designed to indice people to search their memories more carefully than they ordinarily would for pertinent considerations. Note that the stop-and-think probes do not raise new ideas or push the respondent in a particular direction; they simply require the respondent to say explicitly what meaning he or she attaches to the defining phrases of the question. (p. 587-588)\n\n\nResearchers have long known that different people can answer identical questions as if they concerned different topics. What the vignette of the vacillating teacher shows is that the same person can answer the same question at different times as if it involved different topics. This can happen, according to the model, because the considerations that determine people’s survey answers vary across interviews. Thus, people can give strongly felt, contradictory survey responses without either changing their mix of feelings on the issue or consciously experiencing any ambivalence or conflict – if the particular considerations that determine their survey responses have shifted. (p. 594)\n\n\nIf, as the model claims, individuals possess competing considerations on most issues, and if they answer on the basis of whatever ideas happen to be at the top of their minds at the moment of response, one would expect a fair amount of over-time instability in people’s attitude reports (Deduction 4). The reason is that the consideration(s) that are stochastically accessible at one interview might not be so prominent in the next. This inference is strongly supported by a mass of existing evidence (e.g., Table 1). (p. 597)\n\n\nThis finding indicates that the conflict most responsible for response instability is conflict that occurs across rather than within interviews and that respondents are often unaware of their conflict as they answer questions. Thus, the vacillating teacher exhibited no conflict over government services within either interview, but substantial conflict across interviews. Most likely, once the teacher began to view the government services item through the prism of either “bloated government” or “education crisis”, he or she fell into a mindset that blocked thinking about the other point of view. (p. 600)\n\n\nThis type of question order effect is also explainable from the model. Having had their ideological orientations made salient to them just prior to answering policy items, those respondents who possess such orientations are more likely to rely on them as a consideration in formulating responses to subsequent policy questions, thereby making those responses more strongly correlated with their ideological positions and hence also more ideologically consistent with one another (Deduction 16). (p. 603)\n\n\nOur intent in designing the stop-and-think probes was to create such an inducement. By requiring individuals to discuss the elements of a question before answering it, we were inducing them to call to mind and take account of a wider range of ideas than they normally would. We therefore expected that responses following the stop-and-think treatment would be, all else equal, more reliable indicators of the set of underlying considerations than responses made in the standard way, that is, in the retrospective condition (Deduction 17). (p. 603)\n\n\n1. Dependence of attitude reports on probabilistic memory search. Because attitude reports are based on memory searches that are both probabilistic and incomplete, attitude reports tend to be (1) unstable over time; (2) centered on the mean of the underlying considerations; and (3) correlated with the outcomes of memory searches (Deductions 3-5). This is also why people who are more conflicted in their underlying considerations are more unstable in their closed-ended survey responses (Deduction 8).\n2. Effects of ideas recently made salient. The notion that individuals’ survey responses can be deflected in the direction of ideas made recently salient has been used to explain question order effects, endorsement effects, race-of-interviewer effects, reference group effects, question framing effects, and TV news priming effects (Deductions 9-16).\n\nEffects of thought on attitude reports. The notion that thinking about an issue, as gauged by general levels of political awareness, enables people to recall a large numnber of considerations and hence to make more reliable responses has been used to explain why the public as a whole is more stable on “doorstep” issues (Deducions 6, 7). It also explains why more politically aware persons, and persons especially concerned about an issue, are able to recall more thoughts relevant to ir (Deductions 1, 2). Finally, the notion that greater thought makes attitude reports more reliable has been invoked, with only limited success, to explain the effects of extra thought at the moment of responding to an issue (Deduction 17). (p. 607)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#anotações-de-aula",
    "href": "aulas/aula-2.html#anotações-de-aula",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "2.2 Anotações de aula",
    "text": "2.2 Anotações de aula\nSurveys ao longo do tempo mostram flutuações relativamente bizarras ao longo do tempo. Certo: sabemos que a população brasileira não atende telefone, pessoas não confiam em pesquisas, etc. É verdade, há uma série de problemas. Mas, no fim das contas e antes disso tudo, survey são uma maneira de interação humana. E, com isso, geram nas pessoas uma série de reações que impactam nas respostas.\nConverse é considerado um dos fundadores da Escola de Michigan, e um dos principais estudiosos de opinião pública. É um dos maiores cientistas políticos de todos os tempos. Num texto de 1964, ele tenta explicar sua visão de opinião pública – “a maioria das pessoas não têm estruturas latentes, ou reais e fixas, sobre a maioria dos objetos de atitude” (slide, Converse 1964).\nZaller e Feldman, em 1992, propõem uma teoria de resposta a surveys que é contrária a essa visão de Converse. Eles dizem que as pessoas têm opiniões, mas a formação das respostas é plausivelmente um processo on the spot que depende de priming, framing e, mais importante, da capacidade de recuperação de informações (Zaller e Feldman 1992).\nAinda assim, os autores concordam na medida em que entendem que há um componente estocástico na resposta de surveys. Por um lado, Converse acha que as pessoas não têm opiniões formadas. Por outro lado, Zaller e Feldman acreditam que o componente estocástico varia de acordo com algumas questões, incluindo a atenção do respondente, se é uma pessoa mais ou menos informada, etc.\nO grande mérito dos surveys foi a possibilidade de perguntar para as pessoas sobre opiniões e atitudes. O desafio maior é entender o que as pessoas pensam, sentem e como elas vão agir.\n\n2.2.1 Converse (1964)\nAcredita que há constraints lógicos, psicológicos, sociais, heurísticos. As opiniões são amarradas de forma consistente. Mas a maior parte das pessoas não tem esse sistema de crenças estruturados, então elas respondem, mas não respondem em termos ideológicos e introduzem um componente aleatório\nEssa questão é fundamental porque, se pensarmos segundo Converse, perguntar para a pessoa se ela “é de esquerda ou de direita” sequer faz sentido, porque as pessoas não pensam em termos ideológicos. Diferentemente de uma visão espacial da formação de preferências (i.e., Downs), sistemas de crenças são estáveis.\nImplicações:\n\nMaioria das pessoas não têm preferências ideológicas reais\nSurveys superestimam posições políticas\nSurveys são mais úteis quando agregam diferentes mensurações possíveis de uma dimensão\n\n\nParte da variância de pesquisas de survey tem a ver com a formação de respostas, não com erros amostrais.\n\n\n\n2.2.2 Zaller e Feldman (1992)\nA maioria das pessoas têm opiniões conflitantes sobre uma mesma questão e as acessam com algum nível de aleatoriedade. Ao longo do tempo, as pessoas vão recebendo “considerações” da elite política e da mídia (principalmente), de maneira que há questões que ficam mais acessíveis na memória do respondente. Isso acaba compondo a resposta que a pessoa dá a uma pergunta de survey. Para o Zaller, é assim que as pessoas formam opiniões.\nQuando as pessoas são entrevistadas, são levadas a refletir sobre questões abstratas. De acordo com o processo de recepção de informações, as pessoas formam suas opiniões e acessam essas informações de maneira aleatória – sendo que o que está “mais freco” tem mais chance de ser acessado.\nPessoas que têm maior acessibilidade de informações conseguem considerar mais coisas.\nZaller está de acordo com a ideia de que:\n\nPesoas recebem informações de elites proporcionalmente à atenção\nAceitação depende de predisposição, atenção e reflexão (e.g., motivated reasoning). Isso, para Zaller, é fonte de polarização que vem das elites políticas1\nAo responder surveys, pessoas sorteiam informações disponíveis e as ponderam por saliência\n\nImplicações:\n\nPriming e framing influencia a opinião pública porque afeta a recepção e saliência de informações que podem ser perguntadas em surveys\nMesmo pessoas bem informadas podem ter opiniões instáveis porque a acessibilidade de informações é estocástica\nPessoas com maior atenção política tendem a rejeitar informações que não estão de acordo com suas preferências\n\nUma visualização do modelo é imaginar que pessoas têm tendêndias centrais e variação aleatória em suas respostas.\n\n\n2.2.3 Escola de Michigan\nPara a maioria das pessoas, pertencimento a um grupo partidário é a principal fonte de informação e atitudes, o que estrutura suas identidades e oposições.\nAinda que a intenção de voto, avaliação de governo, etc., sejam partialmente aleatórios, a identificação partidária é um anchor para as respostas. Ou seja, as coisas “partem daqui”.\nA identidade social partidária é estável e duradoura. Então, “com qual partido você se identifica mais?” faz mais sentido do que “você é de esquerda ou de direita?”. É claro, apoiar \\(x\\) ou \\(y\\) não determina comportamentos e opiniões completamente, mas estrutura a forma como as pessoas pensam. A questão de identidade partidária costuma resumir várias outras e ser o melhor preditor de voto. E, melhor ainda, essa é uma questão nada abstrata.\n\nDica: Surveys funcionam melhor quando você pergunta coisas paupáveis, mas também coisas que vinculam pessoas a grupos. Os grupos aos quais elas pertencem já permite deduzir uma série de coisas sobre elas.\n\nLivros importantes: - The American Voter (1960) - Partisan Hearts and Minds (2002)\n\n\n2.2.4 Heurística\nNum survey, quando não tenho informação sobre algo, posso usar algum atalho cognitivo para me ajudar a me posicionar. Por exemplo, se eu não sei nada sobre um candidato, posso votar no candidato do meu partido. Se o jornal que eu leio diz que o candidato é bom, eu posso votar nele.\n\n\n2.2.5 Top of mind\nO fato de que respostas a surveys dependem do recall e agregação de considerações é algo útil para mensurar a prevalência destas em um dado momento no tempo. Métricas comuns em pesquisas de mercado.\n\nVocê está gripado. De pronto, aí, sem pensar muito, qual é o remédio que você toma? A resposta é o que está no top of mind, provavelmente Benegripe.\n\n\n\n2.2.6 Estimação de preferências\nHá métodos de estimação de preferências latentes a partir de respostas a diferentes questões de surveys. Exemplo:\n\nbasicspace\nAldrich-McKelvey scaling",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#footnotes",
    "href": "aulas/aula-2.html#footnotes",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "",
    "text": "Essa ideia tende a ser endossada pela literatura mais recente sobre polarização política e fake news. Além disso, as estratégias para despolarizar a sociedade passam por tentar reduzir a polarização das próprias elites políticas, que, hoje, são as principais fontes de informação (ou motivated reasoning) para a população.↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html",
    "href": "aulas/aula-3.html",
    "title": "3  Erros em survey",
    "section": "",
    "text": "3.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#anotações-das-leituras",
    "href": "aulas/aula-3.html#anotações-das-leituras",
    "title": "3  Erros em survey",
    "section": "",
    "text": "3.1.1 Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., & Tourangeau, R. (2011). Survey methodology. John Wiley & Sons. Cap. 2.\n\nSample surveys combine the answers of individual respondents in statistical computing steps to construct statistics describing all persons in the sample. At this point, a survey is one step away from its goal – the description of characteristics of a larger population from which the sample was drawn. (p. 40)\n\n\nThere two inferential steps are central to the two needed characteristics of a survey:\n\nAnswers people give must accurately describe characteristics of the respondents.\nThe subset of persons participating in the survey must have characteristics similar to those of a larger population.\n\nWhen either of these two conditions is not met, the survey statistics are subject to “error”. The use of the term “error” does not imply mistakes in the colloquial sense. Instead, it refers to deviations of what is desired in the survey process from what is attained. “Measurement errors” or “errors of observation” will pertain to deviations from answers given to a survey question and the underlying attribute being measured. “Errors of nonobservation” will pertain to the deviations of a statistic estimated on a sample from that on the full population. (p. 40-41)\n\n\n[About constructs] Some constructs are more abstract than others. The Survey of Consumers (SOC) measures short-term optimism about one’s financial status. This is an attitudinal state of the person, which cannot be directly observed by another person. It is internal to the person, perhaps having aspects that are highly variable within or across persons […]. In contrast, the National Survey on Drug Use and Health (NSDUH) measures consumption of beer in the last month. (p. 42)\n\n\nHowever, survey measurements are often questions posed to a respondent, using words […]. The critical task for measurement is to design questions that produce answers reflecting perfectly the constructs we are trying to measure. These questions can be communicated orally […] or visually […]. Sometimes, however, they are observations made by the interviewer. […]. (p. 42-43)\n\n\nSometimes, the responses are provided as part of the question, and the task of the respondent is to choose from the proffered categories. Other times, only the question is presented, and the respondents must generate an answer in their own words. Sometimes, a respondent fails to provide a response to a measurement attempt. This complicates the computation of statistics involving that measure. (p. 43)\n\nO texto comenta sobre detecção de anomalias on the fly, permitindo que o respondente corrija o erro. Por exemplo, se o usuário indica que nasceu em 1890.\n\nThe frame population is the set of target population members that has a chance to be selected into the survey sample. In a simple case, the “sampling frame” is a list of all units (e.g., people and employers) in the target population. Sometimes, however, the sampling frame is a set of units imperfectly linked to the population members. FOr example, the SOC has as its target population the U.S. adult household population. It uses as its sampling frame a list of telephone numbers. (p. 44)\n\n\nA sample is selected from a sampling frame. This sample is the group from which measurements will be sought. In many cases, the sample will be only a very small fraction of the sampling frame (and, therefore, of the target population). (p. 44)\n\nAgora, pensando do ponto de vista de qualidade de um survey…:\n\nThe job of a survey designer is to minimize error in survey statistics by making design and estimation choices that minimize the gap between two successive stages of the survey process. This framework is sometimes labeled the “total survey error” framework or “total survey error” paradigm. (p. 47)\n\n\nIn short, the underlying target attribute we are attempting to measure is \\(\\mu_i\\), but instead we use an imperfect indicator, \\(Y_i\\), which departs from the target because of imperfections in the measurement. When we apply the measurement, there are problems of administration. Instead of obtaining the answer \\(Y_i\\), we obtain instead \\(y_i\\), the response to the measurement. We attempt to repair the weakness of the measurement through an editing step, and obtain as a result \\(y_{ip}\\), which we call the edited response (the subscript \\(p\\) stands for “postdata collection”). (p. 47)\n\n\nIn statistical terms, the notion of validity lies at the level of an individual respondent. It notes that the construct (even though it may not be easily observed or observed at all) has some value associated with the \\(i\\)th person in the population, traditionally labeled as \\(\\mu_i\\), implying the “true value” of the construct for the \\(i\\)th person. When a specific measure of \\(Y\\) is administered, simple psychometric measurement theory notes that the result is not \\(\\mu_i\\), but something else:\n\\(Y_i = \\mu_i + \\epsilon_i\\) (p. 48)\n\n\nFor example, the answer to a survey question about how many times one has been victimized in the last six months is viewed as just one incident of the application of that question to a specific respondent. In the language of psychometric measurement theory, each survey is one trial of an infinite number of trials. […]. We do not really administer the test many times; instead, we envision that the one test might have achieved different outcomes from the same person over conceptually independent trials. (p. 48)\n\n\nValidity is the correlation of the measurement, \\(Y_i\\), and the true value, \\(\\mu_i\\), measured over all possible trials and persons: \\(\\mathbb{E}[Y_{it} - \\bar{Y} (\\mu_i - \\mu)] / [\\sqrt{\\mathbb{E}(Y_{it} - \\bar{Y}]^2)} \\sqrt{\\mathbb{E}(\\mu_i - \\mu)^2}]\\) (p. 48)\n[…] When \\(y\\) and \\(\\mu\\) covary, moving up and down in tandem, the measurement has high construct validity. A valid measure of an underlying construct is one that is perfecly correlated to the construct. (p. 48)\n\n\nTo the extent that such response behavior is common and systematic across administrations of the question, there arises a discrepancy between the respondent mean response and the true sample mean. (p. 49)\n\nOs autores comentam também sobre fontes de erro na etapa de codificação de respostas abertas. “The processing or editing deviation is simply \\((y_{ip} - y_{i})\\).” (p. 50)\nAlguns detalhes são importantes sobre a sampling frame. Em particular, se os bancos de dados utilizados não são atualizados com frequência, podemos esbarrar em problemas de não observação. Por exemplo, se o banco de dados não é atualizado, podemos acabar ligando para pessoas que já morreram ou mudaram de endereço. (p. 51)\n\nIn statistical terms for a sample mean, coverage bias can be described as a function of two terms: the proportion of the target population not covered by the sampling frame, and the difference between the covered and noncovered population. […] […] For example, for many statistics on the U.S. household population, telephone frames describe the population well, chiefly because the proportion of nontelephone households is very small, about 5% of the total population. Imagine that we used the Surveys of Consumers, a telephone survey, to measure the mean years of education, and the telephone households had a mean of 14.3 years. Among nontelephone households, which were missed due to this being a telephone survey, the mean education level is 11.2 years. Although the nontelephone households have a much lower mean, the bias in the covered mean is:\n\\(\\bar{Y}_C - \\bar{Y} = 0.05(14.3 - 11.2) = 0.16\\)\nor, in other words, we would expect the sampling frame to have a mean years of education of 14.3 years versus the target population mean of 14.1 years. (p. 51)\n\n\nFor example, the National Crime Victimization Survey sample starts with the entire set of 3067 counties within the United States. It separates the counties by population size, region, and correlates of criminal activity, forming separate groups or strata. In each stratum, giving each county a chance of selection, it selects sample counties or groups of counties, totaling 237. All the sample persons in the survey will come from those geographic areas. Each month of the sample selects about 8300 households in the selected areas and attempts interviews with their members. (p. 52)\n\n\nAs with all the other survey errors, there are two types of sampling error: sampling bias and sampling variance. Sampling bias arises when some members of the sampling frame are given no chance (or reduced chance) of selection. […]. Sampling variance arises because, given the design for the sample, by chance many different sets of frame elements could be drawn. (p. 52)\n\n\nThe extent of the error due to sampling is a function of four basic principles of the design:\n\nWhether all sampling frame elements have known, nonzero chances of selection into the sample (called “probability sampling”)\nWhether the sample is designed to control the representation of key sub-populations in the sample (called “stratification”)\nWhether individual elements are drawn directly and independently or in groups (called “element” or “cluster” samples)\nHow large a sample of elements is selected\n\n\nA variância amostral da média é dada por: \\(\\dfrac{\\sum^S_{s=1} (\\bar{y_s} - \\bar{y_c})^2}{S}\\)\n\nNonresponse error arises when the value of statistics computed based only on respondent data differ from those based on the entire sample data. For example, if the students who are absent on the day of the NAEP measurement have lower knowledge in the mathematical or verbal constructs being measured, then NAEP socres suffer nonresponse bias, they systematically overestimate the knowledge of the entire sampling frame. If the nonresponse rate is very high, then the amount of the overestimation could be severe. (p. 53)\n\nEm resumo: os componentes de qualidade de survey são provenientes de erros de observação e de não observação. Erros de observação incluem gaps entre construtos, medidas, respostas e respostas editadas. Erros de não observação incluem erros de cobertura, amostrais e não resposta. (p. 54)\n\nSample surveys rely on two types of inference – from the questions to constructs, and from the sample statistics to the population statistics. The inference involves two coordinated sets of steps: obtaining answers to questions constructed to mirror the constructs, and identifying and measuring sample units that form a microcos of the target population.\nDespite all efforts, each of the steps is subject to imperfections, producing statistical errors in survey statistics. The errors involving the gap between the measures and the construct are issues of validity. The errors arising during the application of the measures are called “measurement errors”. Editing and processing errors can arise during efforts to prepare the data for statistical analysis. Coverage errors arise when enumerating the target population using a sampling frame. Sampling errors stem from surveys measuring only a subset of the frame population. The failure to measure all sample persons on all measures creates nonresponse error. “Adjustment errors” arise in the construction of statistical estimators to describe the full target population. All of these error sources can have varying effects on different statistics from the same survey. (p. 56)\n\n\n\n3.1.2 Berinsky, A. J. (2017). Measuring public opinion with surveys. Annual review of political science, 20(1), 309–329.\nO artigo enfrenta a seguinte questão: “How can we best gauge the political opinions of citizenry?”, isto é, como podemos medir melhor a opinião pública? Trata-se de uma pergunta fundamental para qualquer sociedade democrática.\n\nThere are two basic choices we make when conducting a poll: which people to interview and what questions to ask them. These choices seem simple, but neither is straightforward. The strategies one chooses can greatly affect the answers one gets. Thus, the voice of the people, as reflected in polls, is profoundly shaped by the decisions we make as researchers. (p. 16.2)\n\n\n3.1.2.1 Whom to ask?\n\nFor most of the twwentieth century, these procedures included face-to-face multi-stage designs and telephone interviewing by random-digit dialing. These methods, though not SRS [simple random sampling], can be conducted in ways that allow researchers to approximate SRS. Statistical methods have been developed to account for the design components of modern survey sampling, such as clustering and stratification (Kish 1965, Lohr 2010). (p. 16.3)\n\nNos Estados Unidos, mais ou menos em meados da década de 1970, mais de 90% das casas tinham telefone, e as taxas de resposta eram altas. No entanto, no fim dos anos 1990 as taxas de resposta começaram a cair de maneira vertiginosa. Além disso, telefones de casa passaram a ser menos utilizados em relação a celulares.\n\nCell phones are tied to individuals, whereas landlines are attached to households. Blending samples with these two types of points of contact is no easy feat. For instance, many individuals are reachable by both cell phone and landline. People who appear in both frames will have a higher probability of inclusion. (p. 16.6).\nIt is extremely difficult to define a target population that we would like to represent with our sample. Hillygus (2016) rightly points out that there exists no list of all internet users we can draw upon analogous to the list of telephone numbers. As a result, research over the internet is often conducted using nonprobability samples (although, of course, just as non-online polls are no inherently probability samples, online polls do not necessarily have to be nonprobability samples). (p. 16.6)\n\nHá também outras estratégias de amostragem, como a Address-Based Sampling (ABS), que utiliza endereços postais como base para a amostragem.\nAlém disso, outro problema envolve o fato de que as pessoas simplesmente estão menos dispostas a responder (cooperar) a pesquisas. Isso pode ser um problema, pois as pessoas que não respondem podem ter características diferentes daquelas que respondem e levar a vieses nas estimativas.\n\nThe central challenge of selecting respondents to interview is to minimize the differences between the people we ask our questions and the people we miss. For instance, the fundamental problem with continuing to use landlines to sample survey respondents is that the vast majority of people who should be in our sample are not in out sample – either because we cannot reach them or because they refuse to answer our questions once we contact them. From this perspective, the proper way to fix the problem is to consider how the people we can reach differ from the people we cannot reach – and account for those differences through weighting. We can adjust our samples, but in order to arrive at an accurate picture of the public will, we must measure the differences between respondents and nonrespondents to particular surveys. (p. 16.7-16.8)\n\n\n[…] probability and nonprobability samples are fundamentally different, and a variety of factors need to be considered when comparing methods of gathering responses. It is important to continue to recognize these differences, even in an era when probability response rates are collapsing to the low single digits. (p. 16.8)\n\n\nProbability samples are built on a strong inferential foundation because no matter how low the response rate, all the sampled units are actively recruited and encouraged to participate in the survey. This is not true for nonprobability samples. (p. 16.8)\n\n\n\n3.1.2.2 What to ask?\nQue tipo de informação podemos efetivamente extrair de perguntas feitas em surveys?\n\nWhen choosing the questions we ask, we need to be cognizant of the fact that almost anyone will answer that question, even if they have little basis for their answer. Under such circumstances, how should we think of surveys responses?\n\n\nZaller (1992, p. 49) argues that individuals answer survey questions off the top of their head by ‘averaging across the considerations that are immediately salient to them’ due to their personal characteristics and political experiences at the time of the survey interview. Survey responses therefore are a summary judgment over the mass of considerations – reasons for favoring one side of a controversity rather than another – that happen to be on their mind when they answer a particular questions. (p. 16.9)\n\n\nBishop (2008) contends that responses to survey questions do not represent actual opinions about the specific policy issues being probed by pollsters, but are an ‘illusion’ based on public ignorance of politics and aided by vague polling questions and variations in question form, wording, and context. Thus, for Bishop, measuring and aggregating survey responses is a meaningless exercise. (p. 16.10)\n\n\nSurvey questions may, as Bishop argues, force most people to ‘make up’ answers on the spot, but there made-up answeres are still meaningful because they reflect individuals’ politically relevant considerations – their underlying distribuion of preferences over the policies of government. (p. 16.10)\n\n\nThe argument in the previous section is that, at the individual level, survey responses are meaningful when the measurement instrument is calibrated at a moderate level of specificity: not so general as to be empty of content and not so specific that they risk falling into the trap described by Bishop, creating opinions where none exist. (p. 16.11)\n\n\nSingle-question approach: uma única pergunta é feita e trazer questões práticas para que pessoas menos envolvidas com política sejam capazes de responder;\nAggregation approach: uma série de perguntas que, combinadas, trazer uma única medida de opinião pública.\n\n\nAlthough the use of general questions should in theory force respondents to evaluate politics at a broader frame of reference, in practice, such questions might induce noisy survey responses. Research in psychology has shown that people sometimes answer general survey questions with a specific frame of reference in mind – one that can differ both across people and across situations. (p. 16.15)\n\n\nThe strategy of achieving generality by aggregating across questions for each individual respondent comes with potential problems as well. The strategy of achieving generality by aggregating across questions for each individual respondent comes with potential problems as well. Broockman (2016) criticizes the use of ideological scales that compute the opinions of individuals by taking their average position on a variety of items—whether a straight average or the weighted average implied by factor analysis or IRT models. He argues that these scales are flawed measures of individual preferences because they inappropriately aggregate items across policy domains. (p. 16.15)\n\n\nMy advice is simple. We should ask general survey questions that demand little specific expertise on the part of survey respondents, and we should combine these questions into measures of opinion in theoretically meaningful ways. Of course, this is not a hard and fast rule. On some issues—those in the public eye—it is possible to gather meaningful data on the public’s specific preferences. But as a general guiding principle, measurement should always go hand in hand with an argument about the validity of that measurement strategy. (p. 16.16)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#anotações-de-aula",
    "href": "aulas/aula-3.html#anotações-de-aula",
    "title": "3  Erros em survey",
    "section": "3.2 Anotações de aula",
    "text": "3.2 Anotações de aula\nA coisa talvez mais importante do survey é que o resultado é proveninente de um processo de interação – e, ao criar uma interação, ele deflagra um certo processo cognitivo. Além disso, a maioria das pessoas não tem opiniões estáticas sobre a maioria dos assuntos de opinião pública – daí, a resposta é uma média ponderada das últimas informações que a pessoa se lembra. De fato, pessoas que se interessam e se informam mais sobre política tendem a ter opiniões mais consistentes.\nAgora, do ponto de vista de um curso formal de survey, o outro lado é falar do Total Survey Error. As duas grandes famílias de erro são os erros de mensuração (isto é, que diz respeito ao que eu quero saber e como eu extraio a informação), e os erros de amostragem (qual é a população-alvo? Como selecionamos a amostra?).\nO capítulo do Groves dá os conceitos básicos de survey: - população - unidade de análise - amostra - quadro amostral: para eu poder fazer inferência sobre uma população, eu preciso de um jeito de definí-la (até porque eu preciso acessá-la). Trata-se do que eu acesso da população – é a parte objetiva do que eu quero acessar da população.1 - desenho amostral - estimação - inferência\nTeoricamente, o mundo possui um certo processo gerador de dados que a gente desconhece e, com inferência, a gente consegue chegar o mais perto possível de como esse processo funciona. A estatística, por outro lado, tem um trabalho mais fácil porque ela se importa com a população, e não com o processo gerador de dados. Não me importa a fórmula; me importa extrapolar a partir de uma amostra. É claro, mesmo a ideia de “população” é um construto que a gente assume que podemos alcançar, mas isso não é necessariamente verdade.\nÉ impossível fazer pesquisa amostral sem quadro amostral.\nNo caso de pesquisa amostral no Brasil, o estado-da-arte envolve sorteios no nível dos territórios. Usando a Malha de Setores Censitários (bem granular, duas ou três quadras), podemos sortear territórios. O melhor que podemos fazer é ir às residências e procurar as pessoas, porque não temos um banco frequentemente atualizado e claro da população adulta. Outro quadro amostral possível é sortear listas telefônicas, assumindo que a maioria das pessoas tem telefone.\nDatafolha, por outro lado, usa pontos de fluxo. Dividimos a unidade territorial em uma série de partes, e procuramos os lugares onde passam mais pessoas. De fato, o lugar onde as pessoas passam no dia a dia. Então não existe listagem das pessoas, mas há uma listagem dos lugares onde as pessoas passam (então, de certa maneira, existe um componente “aleatório”).\nAgora, sobre desenho amostral, temos uma coisa mais “alto nível”. É a receita do bolo. O plano amostral é algo mais objetivo – sorteei os setores censitários etc. E a inferência, no fim das contas, é o que permite a gente extrapolar os dados da amostra para a população, calculando determinadas estatísticas sob incerteza.\n\n3.2.1 Notação básica\nTemos uma população de tamanho \\(N\\) na qual cada pessoa \\(i\\) tem:\n\num valor latente \\(\\mu_i\\)\num valor passível de observação, “fixo”, \\(Y_i\\)\n\nA média da população é:\n\\[\n\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i\n\\]\nEm surveys, extraímos uma amostra \\(S\\) de tamanho \\(n\\), com \\(n \\leq N\\), para obter respostas de uma amostra (provavelmente com erro de mensuração), \\(y_i\\). Daí podemos ajustá-las, chegando a \\(y_{ip}\\), que é a resposta editada. A média amostral é:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n\\]\nNo fim das contas, calculamos uma estatística amostral.\nPodemos ter também um indicador de inclusão da pessoa \\(i\\) na amostra:\n\\[\nI_i = I(i \\in S)\n\\]\nSendo que \\(\\pi_i\\) é a probabilidade de inclusão do indivíduo \\(i\\) na amostra. A probabilidade de inclusão é dada por:\n\\[\n\\pi_i = \\text{Pr}(i \\in S)\n\\]\n\\(y_i\\) é uma variável aleatória que depende do sorteio da amostra.\nAgora, quando o desenho é não probabilístico, a probabilidade de uma pessoa ser sorteada (isto é, \\(\\pi_i\\)) é desconhecida.\n\n\n3.2.2 Probabilidade de inclusão\nEm populações grandes, a probabilidade de inclusão em uma amostra de qualquer \\(i\\) é geralmente ínfima. Imagine uma pessoa com \\(\\pi_i = 0.25\\):\n\\[\nw_i = \\frac{1}{\\pi_i} = \\frac{1}{0.25} = 4\n\\]\nEla representa 4 pessoas na população. Se \\(y_i = 10\\), ela contribui \\(4 \\times 10 = 40\\) para a estimativa do total. \\(w_i\\) é frequentemente chamado de peso da pessoa \\(i\\).\n\nA ideia de que conseguimos saber a probabilidade de inclusão de uma pessoa na amostra nos permite dizer quantas pessoas aquela pessoa representa na população.\n\n\n\n3.2.3 Estimador de Horvitz-Thompson\nPara estimar um total populacional, somamos os valores observados na amostra ponderados pelos seus pesos, \\(w_i\\). Se eu sei quantas pessoas cada uma das pessoas na amostra reprensenta, então eu consigo estimar o total populacional. Peso também pode ser chamado de fator de expansão.\n\\[\n\\hat{T}_{HT} = \\sum_{i=1}^{n} w_i y_i = \\sum_{i=1}^{n} \\frac{y_i}{\\pi_i}\n\\]\nMesmo usando HT, a estimativa do total pode diferir de \\(T\\) na população; com amostras aleatórias, cada realização é ligeiramente diferente. Isso é a variância. Podemos repetir 1000 vezes o sorteio da amostra de \\(n=3\\), obtendo algo próximo de uma distribuição normal.\n\n\n3.2.4 Erros\nErros nada mais são do que diferenças entre estimativa amostral e parâmetro populacional (assumindo, por ora, amostras aleatórias com sorteios independentes de uma população infinita). A quantidade de uma pesquisa depende da proximidade entre \\(\\bar{y}\\) e \\(\\bar{Y}\\).\nUma forma teórica de quantificar o erro total para uma estatística específica:\nErro quadrático médio, que combina viés e variância:\n\\[\n\\text{MSE}(\\bar{y}) = \\text{Var}(\\bar{y}) + \\text{Bias}^2(\\bar{y})\n\\]\n\n\\(\\text{Var}(\\bar{y})\\) é a variância amostral, que mede a variabilidade da média amostral em torno da média populacional.\n\\(\\text{Bias}(\\bar{y})\\) é a diferença entre a média amostral e a média populacional.\n\nPodemos ter variância dentro da amostra (isto é, entre os respondentes) e variância entre amostras (isto é, entre diferentes realizações).\n\n3.2.4.1 Variância e tamanho amostral\nUma população infinita gerada por:\n\\[\n\\begin{align*}\n&y_i = \\mu + \\epsilon_i \\\\\n&\\epsilon_i \\sim N(0, \\sigma^2)\n\\end{align*}\n\\]\nCom \\(\\mu = 100\\) e \\(\\sigma = 5\\):\n\nset.seed(123)\n\n# populacao\nn &lt;- 2000\n\n# media\nmu &lt;- 100\n\n# desvio padrao\nsigma &lt;- 5\n\n# gerando a populacao\ny &lt;- rnorm(n, mu, sigma)\n\n# histograma da distribuicao adicionando uma linha na media\nhist(y, breaks = 20, main = \"Distribuição da população\", xlab = \"y\", ylab = \"Frequência\")\nabline(v = mu, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nAgora, se temos viés na amostra, podemos ter uma média amostral diferente da média populacional:\n\\[\n\\begin{align*}\n&y_i = \\mu + \\epsilon_i + \\text{viés} \\\\\n&\\epsilon_i \\sim N(0, \\sigma^2) \\\\\n&\\text{viés} = 5\n\\end{align*}\n\\]\nNesse caso, a média amostral é 105:\n\n# amostra\nn &lt;- 2000\nmu &lt;- 100\nsigma &lt;- 5\n\n# amostra com viés\ny_biased &lt;- rnorm(n, mu + 5, sigma)\n\n# histograma da distribuicao adicionando uma linha na media\nhist(y_biased, breaks = 20, main = \"Distribuição da amostra com viés\", xlab = \"y\", ylab = \"Frequência\")\nabline(v = mu, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Total Survey Error\nO TSE é um framework sobre os dois grandes processos que geram erros: mensuração e amostragem (Groves e Lyberg, 2010). Há uma série de fontes para erros: erros de mensuração, de cobertura, de pós-ajuste (pós-estratificação), de não-resposta, amostral, enfim.\nErros de mensuração: quando a resposta coletada \\(y_i\\) difere de \\(Y_i\\)\n\nInterpretação errada da pergunta\nEsquecimento\nAnotação errada\nVergonha de revelar uma opinião\nRespostas aleatórias\n\nErro de cobertura: quando o quadro amostral não abarca toda a população-alvo. Ou melhor, quando o quadro amostral não efetivamente se ajusta à população-alvo. Algumas fontes comuns são registros desatualizados, exclusão de domicílios em áreas remotas ou inacessíveis, pessoas sem telefone ou sem internet, entre outros.\n\nSubcobertura\nSobrecobertura\n\nErro de não resposta: parte da amostra não responde. Se não respondentes diferem sistematicamente dos respondentes, há viés de não resposta. O que importa é a diferença entre quem responde e quem não responde.\n\nUnit nonresponse: pessoa não responde nada\nItem nonresponse: pessoa responde parcialmente (isto é, não responde a todas as perguntas)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#footnotes",
    "href": "aulas/aula-3.html#footnotes",
    "title": "3  Erros em survey",
    "section": "",
    "text": "No Brasil, a gente trata principalmente do CNEFE – o Cadastro Nacional de Endereços para Fins Estatísticos. É daqui que tiramos as “listas” para o quadro amostral e chegue perto da população de domicílios.↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  }
]