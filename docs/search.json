[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pesquisa de Survey",
    "section": "",
    "text": "1 Apresentação do curso\n\nSurveys são, hoje, uma das mais difundidas ferramentas de pesquisa social. Por meio da aplicação de questionários estruturados a uma amostra de determinada população, governos estimam taxas de desemprego e de pobreza; políticos definem estratégias de campanha; institutos de pesquisa identificam tendências de comportamento de diferentes grupos sociais; e empresas investigam a satisfação de seus clientes e testam ações de marketing. A proposta deste curso é oferecer uma introdução panorâmica a este tipo de pesquisa. [ementa]\n\nProfessor: Fernando Meireles (IESP-UERJ) [homepage]\nSemestre: 2025.1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Apresentação do curso</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html",
    "href": "aulas/aula-2.html",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "",
    "text": "2.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#anotações-das-leituras",
    "href": "aulas/aula-2.html#anotações-das-leituras",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "",
    "text": "2.1.1 Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., & Tourangeau, R. (2011). Survey methodology. John Wiley & Sons. Cap. 1.\n\nThere are four perspectives on surveys that are worth describing: the purposes to which surveys were put, the development of question design, the development of sampling methods, and the development of data collection methods. (p. 15)\n\n\nHowever, the market research and polling organizations were doing large numbers of interviews, using newly hired people with no special background in the social sciences. Of necessity, researchers needed to specify more carefully the information sought by the survey. Further, researchers found that small changes in the wording of an attitude question sometimes had unusually large effects on the answers. (p. 16)\n\n\nAlthough the theory of probability was established in the 18th century, its application to practical sample survey work was delayed until the 20th century. The first applications were the taking of a “1 in N” systematic selection from census returns. There were “probability samples”; that is, every record had a known nonzero chance of selection into the sample. (p. 17)\n\n\nAlthough all police agencies in the United States could be asked to report statistics from their record systems, it was financially impossible to ask all persons in the United States to report their individual victimizations. If a “representative” sample could be identified, this would make the victimization survey possible. Thus, in contrast to the record data, the survey would be subject to “sampling error” (i.e., errors in statistics because of the omission of some persons in the population). (p. 20)\n\n\nThe methodological work suggested that the survey would tend to underreport less important crimes, apparently because they were difficult to remember. Finally, both systems have trouble with repeated incidents of the same character. (p. 20)\n\n\nThe sample is restricted to persons who are household members, excluding the homeless, those in institutions, and in group quarters. The sample is clustered into hundreds of different sample areas (usually counties or groups of counties) and the sample design is repeated samples of households from the same areas over the years of the study. (p. 21)\n\n\nEach month, the Surveys of Consumers (SOC) telephones a sample of the phone numbers, locates the household numbers among them, and selects one adult from those in the households. The target is the full household population, but to reduce costs, only persons in households with telephones are eligible. […]. After the data are collected, there are statistical adjustments made to the estimates in an attempt to repair the omission of nontelephone households and nonresponse. (p. 25-26)\n\n\nThe design features of the surveys are tailored to fit their purposes. The target populations for these surveys are varied – for example, the NCVS aims to describe adults and children age 12 or older, whereas the CES survey has a target population of employers. The NCVS, NSDUH, and NAEP surveys all draw samples of households in two steps, first sampling geographic areas, then sampling from created lists of households within those areas. The Surveys of Consumers and BRFSS draw samples of telephone numbers from all possible numbers within working area codes and exchanges. Then the numbers are screened to find those associated with households. (p. 34)\n\n\nSurvey methodology seeks to identify principles about the design, collection, processing, and analysis of surveys that are linked to the cost and quality of survey estimates. This means that the field focuses on improving quality within cost constraints, or, alternatively, reducing costs for some fixed level of quality. “Quality” is defined within a framework labeled the total survey error paradigm. (p. 35)\n\n\nBecause survey methodology has this inherently multidisciplinary nature, it has only recently developed as a unified field. (p. 35)\n\n\nIn a similar way, in some cases there are only imperfect solutions for survey design problems. All the approaches available to solve a particular problem may have pluses and minuses. The survey methodologist has to decide which of a set of imperfect options is best. Again, the total survey error approach is to consider the various ways that the options will affect the quality of the resulting data and choose the one that, on balance, will produce the most valuable data. (p. 37)\n\n\n\n2.1.2 Zaller, J., & Feldman, S. (1992). A simple theory of the survey response: Answering questions versus revealing preferences. American Journal of Political Science, 579-616.\n\nVirtually, all public opinion research proceeds on the assumption that citizens possess reasonably well formed attitudes on major political issues and that surveys are passive measures of these attitudes. The standard view is that when survey respondents say they favor X they are simply describing a preexisting state of feeling favorably toward X.\nAccumulating evidence on the vagaries of mass political attitudes, however, has made this view increasingly dubious. If, as is well known, prople are asked the same question in a series of interviews, their attitude reports are highly changeable. Many, as much evidence also shows, react strongly to the context in which the questions are asked, to the order in which options are presented, and to wholly nonsubstantive changes in question wording. These phenomena are more than methodological curiosities; they raise serious doubts about what public opinion surveys measure.\nIn view of this, we propose a new undestanding of the mass survey response. Most citizens, we argue, simply do not possess performed attitudes at the level of specificity demanded in surveys. Rather, they carry around in their heads a mix of only partially consistent ideas and considerations. When questioned, they call to mind a sample of these ideas, including an oversample of ideas made salient by the questionnaiere and other recent events, and use them to choose among the options offered. But their choices do not, in most cases, reflect anything that can be described as true attitudes; rather, they reflect the thoughts that are most accessible in memory at the moment of response. (p. 579-580)\n\n\nOne of the most unsettling findings of opinion research has been the discovery of a large component of randomness in most people’s answers to survey questions. If the same people are asked the same question in repeated interviews, only about half of them give the same answers. (p. 580)\n\n\nThe literature on response effects thus makes it clear that survey questions do not simply measure public opinion. They also shape and channel it by the manner in which they frame issues, order the alternatives, and otherwise set the context of the question. THis has led researchers to a conclusion that seems indisputable but that is fundamentally at odds with the assumptions of most political scientists about the nature of political attitudes: namely, people do not merely reveal preexisting attitudes on surveys; to some considerable extent, people are using the questionnaire to decide what their “attitudes” are (Bishop, Oldendick, and Tuchfarber 1984;; Zaller 1984; Feldman 1990).\n\n\nCurrent attitude models seem quite irrelevant to these observations. The reason for Sartori’s vacillation is not, as students of Converse might say, that he has no opinion on this question, nor is it that, as users of measurement error models might say, Sartori has a “true attitude” that Hochschild is unable to measure reliably. It is rather that Sartori has conflicting opinions, or at least conflicting considerations, that lead him to give different responses at different times, depending on how he thinks about the issue. (p. 584)\n\n\nAXIOM 1: The ambivalence axiom. Most people possess opposing considerations on most issues, that is, considerations that might lead them to decide the issue either way. (p. 585)\nAXIOM 2: The response axiom. Individuals answer survey questions by averaging across the considerations that happen to be salient at the moment of response, where saliency is determined by the accessbility axiom. (p. 586)\nAXIOM 3: The accessibility axiom. The accessibility of any given consideration depends on a stochastic sampling process, where considerations that have been recently thought about are somewhat more likely to be sampled. (p. 586)\n\n\nFor a case in which a person devotes great thought and attention to an issue, Axiom 3 implies that there may be multiple considerations salient in memory at the moment of answering questions about the issue and hence many considerations to be averaged across. But a person who rarely thinks about an issue and who is confronted by an interview situatin that requires a succession of quick answers (Feldman 1990) may have only one consideration immediately available in memory, in which case the averaging rule reduces to answering on the basis of a single “top-of-the-head” consideration, as suggested by Taylor and Fiske. (p. 586)\n\n\nThe two types of probes [page 587] are clearly not equivalent. The “retrospective” probes, which were posed after people had answered the question in the normal way, were designed to find out what exactly was on people’s minds at the moment of response. The “prospective” or “stop-and-think” probes, on the other hand, were designed to indice people to search their memories more carefully than they ordinarily would for pertinent considerations. Note that the stop-and-think probes do not raise new ideas or push the respondent in a particular direction; they simply require the respondent to say explicitly what meaning he or she attaches to the defining phrases of the question. (p. 587-588)\n\n\nResearchers have long known that different people can answer identical questions as if they concerned different topics. What the vignette of the vacillating teacher shows is that the same person can answer the same question at different times as if it involved different topics. This can happen, according to the model, because the considerations that determine people’s survey answers vary across interviews. Thus, people can give strongly felt, contradictory survey responses without either changing their mix of feelings on the issue or consciously experiencing any ambivalence or conflict – if the particular considerations that determine their survey responses have shifted. (p. 594)\n\n\nIf, as the model claims, individuals possess competing considerations on most issues, and if they answer on the basis of whatever ideas happen to be at the top of their minds at the moment of response, one would expect a fair amount of over-time instability in people’s attitude reports (Deduction 4). The reason is that the consideration(s) that are stochastically accessible at one interview might not be so prominent in the next. This inference is strongly supported by a mass of existing evidence (e.g., Table 1). (p. 597)\n\n\nThis finding indicates that the conflict most responsible for response instability is conflict that occurs across rather than within interviews and that respondents are often unaware of their conflict as they answer questions. Thus, the vacillating teacher exhibited no conflict over government services within either interview, but substantial conflict across interviews. Most likely, once the teacher began to view the government services item through the prism of either “bloated government” or “education crisis”, he or she fell into a mindset that blocked thinking about the other point of view. (p. 600)\n\n\nThis type of question order effect is also explainable from the model. Having had their ideological orientations made salient to them just prior to answering policy items, those respondents who possess such orientations are more likely to rely on them as a consideration in formulating responses to subsequent policy questions, thereby making those responses more strongly correlated with their ideological positions and hence also more ideologically consistent with one another (Deduction 16). (p. 603)\n\n\nOur intent in designing the stop-and-think probes was to create such an inducement. By requiring individuals to discuss the elements of a question before answering it, we were inducing them to call to mind and take account of a wider range of ideas than they normally would. We therefore expected that responses following the stop-and-think treatment would be, all else equal, more reliable indicators of the set of underlying considerations than responses made in the standard way, that is, in the retrospective condition (Deduction 17). (p. 603)\n\n\n1. Dependence of attitude reports on probabilistic memory search. Because attitude reports are based on memory searches that are both probabilistic and incomplete, attitude reports tend to be (1) unstable over time; (2) centered on the mean of the underlying considerations; and (3) correlated with the outcomes of memory searches (Deductions 3-5). This is also why people who are more conflicted in their underlying considerations are more unstable in their closed-ended survey responses (Deduction 8).\n2. Effects of ideas recently made salient. The notion that individuals’ survey responses can be deflected in the direction of ideas made recently salient has been used to explain question order effects, endorsement effects, race-of-interviewer effects, reference group effects, question framing effects, and TV news priming effects (Deductions 9-16).\n\nEffects of thought on attitude reports. The notion that thinking about an issue, as gauged by general levels of political awareness, enables people to recall a large numnber of considerations and hence to make more reliable responses has been used to explain why the public as a whole is more stable on “doorstep” issues (Deducions 6, 7). It also explains why more politically aware persons, and persons especially concerned about an issue, are able to recall more thoughts relevant to ir (Deductions 1, 2). Finally, the notion that greater thought makes attitude reports more reliable has been invoked, with only limited success, to explain the effects of extra thought at the moment of responding to an issue (Deduction 17). (p. 607)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#anotações-de-aula",
    "href": "aulas/aula-2.html#anotações-de-aula",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "2.2 Anotações de aula",
    "text": "2.2 Anotações de aula\nSurveys ao longo do tempo mostram flutuações relativamente bizarras ao longo do tempo. Certo: sabemos que a população brasileira não atende telefone, pessoas não confiam em pesquisas, etc. É verdade, há uma série de problemas. Mas, no fim das contas e antes disso tudo, survey são uma maneira de interação humana. E, com isso, geram nas pessoas uma série de reações que impactam nas respostas.\nConverse é considerado um dos fundadores da Escola de Michigan, e um dos principais estudiosos de opinião pública. É um dos maiores cientistas políticos de todos os tempos. Num texto de 1964, ele tenta explicar sua visão de opinião pública – “a maioria das pessoas não têm estruturas latentes, ou reais e fixas, sobre a maioria dos objetos de atitude” (slide, Converse 1964).\nZaller e Feldman, em 1992, propõem uma teoria de resposta a surveys que é contrária a essa visão de Converse. Eles dizem que as pessoas têm opiniões, mas a formação das respostas é plausivelmente um processo on the spot que depende de priming, framing e, mais importante, da capacidade de recuperação de informações (Zaller e Feldman 1992).\nAinda assim, os autores concordam na medida em que entendem que há um componente estocástico na resposta de surveys. Por um lado, Converse acha que as pessoas não têm opiniões formadas. Por outro lado, Zaller e Feldman acreditam que o componente estocástico varia de acordo com algumas questões, incluindo a atenção do respondente, se é uma pessoa mais ou menos informada, etc.\nO grande mérito dos surveys foi a possibilidade de perguntar para as pessoas sobre opiniões e atitudes. O desafio maior é entender o que as pessoas pensam, sentem e como elas vão agir.\n\n2.2.1 Converse (1964)\nAcredita que há constraints lógicos, psicológicos, sociais, heurísticos. As opiniões são amarradas de forma consistente. Mas a maior parte das pessoas não tem esse sistema de crenças estruturados, então elas respondem, mas não respondem em termos ideológicos e introduzem um componente aleatório\nEssa questão é fundamental porque, se pensarmos segundo Converse, perguntar para a pessoa se ela “é de esquerda ou de direita” sequer faz sentido, porque as pessoas não pensam em termos ideológicos. Diferentemente de uma visão espacial da formação de preferências (i.e., Downs), sistemas de crenças são estáveis.\nImplicações:\n\nMaioria das pessoas não têm preferências ideológicas reais\nSurveys superestimam posições políticas\nSurveys são mais úteis quando agregam diferentes mensurações possíveis de uma dimensão\n\n\nParte da variância de pesquisas de survey tem a ver com a formação de respostas, não com erros amostrais.\n\n\n\n2.2.2 Zaller e Feldman (1992)\nA maioria das pessoas têm opiniões conflitantes sobre uma mesma questão e as acessam com algum nível de aleatoriedade. Ao longo do tempo, as pessoas vão recebendo “considerações” da elite política e da mídia (principalmente), de maneira que há questões que ficam mais acessíveis na memória do respondente. Isso acaba compondo a resposta que a pessoa dá a uma pergunta de survey. Para o Zaller, é assim que as pessoas formam opiniões.\nQuando as pessoas são entrevistadas, são levadas a refletir sobre questões abstratas. De acordo com o processo de recepção de informações, as pessoas formam suas opiniões e acessam essas informações de maneira aleatória – sendo que o que está “mais freco” tem mais chance de ser acessado.\nPessoas que têm maior acessibilidade de informações conseguem considerar mais coisas.\nZaller está de acordo com a ideia de que:\n\nPesoas recebem informações de elites proporcionalmente à atenção\nAceitação depende de predisposição, atenção e reflexão (e.g., motivated reasoning). Isso, para Zaller, é fonte de polarização que vem das elites políticas1\nAo responder surveys, pessoas sorteiam informações disponíveis e as ponderam por saliência\n\nImplicações:\n\nPriming e framing influencia a opinião pública porque afeta a recepção e saliência de informações que podem ser perguntadas em surveys\nMesmo pessoas bem informadas podem ter opiniões instáveis porque a acessibilidade de informações é estocástica\nPessoas com maior atenção política tendem a rejeitar informações que não estão de acordo com suas preferências\n\nUma visualização do modelo é imaginar que pessoas têm tendêndias centrais e variação aleatória em suas respostas.\n\n\n2.2.3 Escola de Michigan\nPara a maioria das pessoas, pertencimento a um grupo partidário é a principal fonte de informação e atitudes, o que estrutura suas identidades e oposições.\nAinda que a intenção de voto, avaliação de governo, etc., sejam partialmente aleatórios, a identificação partidária é um anchor para as respostas. Ou seja, as coisas “partem daqui”.\nA identidade social partidária é estável e duradoura. Então, “com qual partido você se identifica mais?” faz mais sentido do que “você é de esquerda ou de direita?”. É claro, apoiar \\(x\\) ou \\(y\\) não determina comportamentos e opiniões completamente, mas estrutura a forma como as pessoas pensam. A questão de identidade partidária costuma resumir várias outras e ser o melhor preditor de voto. E, melhor ainda, essa é uma questão nada abstrata.\n\nDica: Surveys funcionam melhor quando você pergunta coisas paupáveis, mas também coisas que vinculam pessoas a grupos. Os grupos aos quais elas pertencem já permite deduzir uma série de coisas sobre elas.\n\nLivros importantes: - The American Voter (1960) - Partisan Hearts and Minds (2002)\n\n\n2.2.4 Heurística\nNum survey, quando não tenho informação sobre algo, posso usar algum atalho cognitivo para me ajudar a me posicionar. Por exemplo, se eu não sei nada sobre um candidato, posso votar no candidato do meu partido. Se o jornal que eu leio diz que o candidato é bom, eu posso votar nele.\n\n\n2.2.5 Top of mind\nO fato de que respostas a surveys dependem do recall e agregação de considerações é algo útil para mensurar a prevalência destas em um dado momento no tempo. Métricas comuns em pesquisas de mercado.\n\nVocê está gripado. De pronto, aí, sem pensar muito, qual é o remédio que você toma? A resposta é o que está no top of mind, provavelmente Benegripe.\n\n\n\n2.2.6 Estimação de preferências\nHá métodos de estimação de preferências latentes a partir de respostas a diferentes questões de surveys. Exemplo:\n\nbasicspace\nAldrich-McKelvey scaling",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#footnotes",
    "href": "aulas/aula-2.html#footnotes",
    "title": "2  O que são e para que servem pesquisas de survey",
    "section": "",
    "text": "Essa ideia tende a ser endossada pela literatura mais recente sobre polarização política e fake news. Além disso, as estratégias para despolarizar a sociedade passam por tentar reduzir a polarização das próprias elites políticas, que, hoje, são as principais fontes de informação (ou motivated reasoning) para a população.↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que são e para que servem pesquisas de survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html",
    "href": "aulas/aula-3.html",
    "title": "3  Erros em survey",
    "section": "",
    "text": "3.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#anotações-das-leituras",
    "href": "aulas/aula-3.html#anotações-das-leituras",
    "title": "3  Erros em survey",
    "section": "",
    "text": "3.1.1 Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., & Tourangeau, R. (2011). Survey methodology. John Wiley & Sons. Cap. 2.\n\nSample surveys combine the answers of individual respondents in statistical computing steps to construct statistics describing all persons in the sample. At this point, a survey is one step away from its goal – the description of characteristics of a larger population from which the sample was drawn. (p. 40)\n\n\nThere two inferential steps are central to the two needed characteristics of a survey:\n\nAnswers people give must accurately describe characteristics of the respondents.\nThe subset of persons participating in the survey must have characteristics similar to those of a larger population.\n\nWhen either of these two conditions is not met, the survey statistics are subject to “error”. The use of the term “error” does not imply mistakes in the colloquial sense. Instead, it refers to deviations of what is desired in the survey process from what is attained. “Measurement errors” or “errors of observation” will pertain to deviations from answers given to a survey question and the underlying attribute being measured. “Errors of nonobservation” will pertain to the deviations of a statistic estimated on a sample from that on the full population. (p. 40-41)\n\n\n[About constructs] Some constructs are more abstract than others. The Survey of Consumers (SOC) measures short-term optimism about one’s financial status. This is an attitudinal state of the person, which cannot be directly observed by another person. It is internal to the person, perhaps having aspects that are highly variable within or across persons […]. In contrast, the National Survey on Drug Use and Health (NSDUH) measures consumption of beer in the last month. (p. 42)\n\n\nHowever, survey measurements are often questions posed to a respondent, using words […]. The critical task for measurement is to design questions that produce answers reflecting perfectly the constructs we are trying to measure. These questions can be communicated orally […] or visually […]. Sometimes, however, they are observations made by the interviewer. […]. (p. 42-43)\n\n\nSometimes, the responses are provided as part of the question, and the task of the respondent is to choose from the proffered categories. Other times, only the question is presented, and the respondents must generate an answer in their own words. Sometimes, a respondent fails to provide a response to a measurement attempt. This complicates the computation of statistics involving that measure. (p. 43)\n\nO texto comenta sobre detecção de anomalias on the fly, permitindo que o respondente corrija o erro. Por exemplo, se o usuário indica que nasceu em 1890.\n\nThe frame population is the set of target population members that has a chance to be selected into the survey sample. In a simple case, the “sampling frame” is a list of all units (e.g., people and employers) in the target population. Sometimes, however, the sampling frame is a set of units imperfectly linked to the population members. FOr example, the SOC has as its target population the U.S. adult household population. It uses as its sampling frame a list of telephone numbers. (p. 44)\n\n\nA sample is selected from a sampling frame. This sample is the group from which measurements will be sought. In many cases, the sample will be only a very small fraction of the sampling frame (and, therefore, of the target population). (p. 44)\n\nAgora, pensando do ponto de vista de qualidade de um survey…:\n\nThe job of a survey designer is to minimize error in survey statistics by making design and estimation choices that minimize the gap between two successive stages of the survey process. This framework is sometimes labeled the “total survey error” framework or “total survey error” paradigm. (p. 47)\n\n\nIn short, the underlying target attribute we are attempting to measure is \\(\\mu_i\\), but instead we use an imperfect indicator, \\(Y_i\\), which departs from the target because of imperfections in the measurement. When we apply the measurement, there are problems of administration. Instead of obtaining the answer \\(Y_i\\), we obtain instead \\(y_i\\), the response to the measurement. We attempt to repair the weakness of the measurement through an editing step, and obtain as a result \\(y_{ip}\\), which we call the edited response (the subscript \\(p\\) stands for “postdata collection”). (p. 47)\n\n\nIn statistical terms, the notion of validity lies at the level of an individual respondent. It notes that the construct (even though it may not be easily observed or observed at all) has some value associated with the \\(i\\)th person in the population, traditionally labeled as \\(\\mu_i\\), implying the “true value” of the construct for the \\(i\\)th person. When a specific measure of \\(Y\\) is administered, simple psychometric measurement theory notes that the result is not \\(\\mu_i\\), but something else:\n\\(Y_i = \\mu_i + \\epsilon_i\\) (p. 48)\n\n\nFor example, the answer to a survey question about how many times one has been victimized in the last six months is viewed as just one incident of the application of that question to a specific respondent. In the language of psychometric measurement theory, each survey is one trial of an infinite number of trials. […]. We do not really administer the test many times; instead, we envision that the one test might have achieved different outcomes from the same person over conceptually independent trials. (p. 48)\n\n\nValidity is the correlation of the measurement, \\(Y_i\\), and the true value, \\(\\mu_i\\), measured over all possible trials and persons: \\(\\mathbb{E}[Y_{it} - \\bar{Y} (\\mu_i - \\mu)] / [\\sqrt{\\mathbb{E}(Y_{it} - \\bar{Y}]^2)} \\sqrt{\\mathbb{E}(\\mu_i - \\mu)^2}]\\) (p. 48)\n[…] When \\(y\\) and \\(\\mu\\) covary, moving up and down in tandem, the measurement has high construct validity. A valid measure of an underlying construct is one that is perfecly correlated to the construct. (p. 48)\n\n\nTo the extent that such response behavior is common and systematic across administrations of the question, there arises a discrepancy between the respondent mean response and the true sample mean. (p. 49)\n\nOs autores comentam também sobre fontes de erro na etapa de codificação de respostas abertas. “The processing or editing deviation is simply \\((y_{ip} - y_{i})\\).” (p. 50)\nAlguns detalhes são importantes sobre a sampling frame. Em particular, se os bancos de dados utilizados não são atualizados com frequência, podemos esbarrar em problemas de não observação. Por exemplo, se o banco de dados não é atualizado, podemos acabar ligando para pessoas que já morreram ou mudaram de endereço. (p. 51)\n\nIn statistical terms for a sample mean, coverage bias can be described as a function of two terms: the proportion of the target population not covered by the sampling frame, and the difference between the covered and noncovered population. […] […] For example, for many statistics on the U.S. household population, telephone frames describe the population well, chiefly because the proportion of nontelephone households is very small, about 5% of the total population. Imagine that we used the Surveys of Consumers, a telephone survey, to measure the mean years of education, and the telephone households had a mean of 14.3 years. Among nontelephone households, which were missed due to this being a telephone survey, the mean education level is 11.2 years. Although the nontelephone households have a much lower mean, the bias in the covered mean is:\n\\(\\bar{Y}_C - \\bar{Y} = 0.05(14.3 - 11.2) = 0.16\\)\nor, in other words, we would expect the sampling frame to have a mean years of education of 14.3 years versus the target population mean of 14.1 years. (p. 51)\n\n\nFor example, the National Crime Victimization Survey sample starts with the entire set of 3067 counties within the United States. It separates the counties by population size, region, and correlates of criminal activity, forming separate groups or strata. In each stratum, giving each county a chance of selection, it selects sample counties or groups of counties, totaling 237. All the sample persons in the survey will come from those geographic areas. Each month of the sample selects about 8300 households in the selected areas and attempts interviews with their members. (p. 52)\n\n\nAs with all the other survey errors, there are two types of sampling error: sampling bias and sampling variance. Sampling bias arises when some members of the sampling frame are given no chance (or reduced chance) of selection. […]. Sampling variance arises because, given the design for the sample, by chance many different sets of frame elements could be drawn. (p. 52)\n\n\nThe extent of the error due to sampling is a function of four basic principles of the design:\n\nWhether all sampling frame elements have known, nonzero chances of selection into the sample (called “probability sampling”)\nWhether the sample is designed to control the representation of key sub-populations in the sample (called “stratification”)\nWhether individual elements are drawn directly and independently or in groups (called “element” or “cluster” samples)\nHow large a sample of elements is selected\n\n\nA variância amostral da média é dada por: \\(\\dfrac{\\sum^S_{s=1} (\\bar{y_s} - \\bar{y_c})^2}{S}\\)\n\nNonresponse error arises when the value of statistics computed based only on respondent data differ from those based on the entire sample data. For example, if the students who are absent on the day of the NAEP measurement have lower knowledge in the mathematical or verbal constructs being measured, then NAEP socres suffer nonresponse bias, they systematically overestimate the knowledge of the entire sampling frame. If the nonresponse rate is very high, then the amount of the overestimation could be severe. (p. 53)\n\nEm resumo: os componentes de qualidade de survey são provenientes de erros de observação e de não observação. Erros de observação incluem gaps entre construtos, medidas, respostas e respostas editadas. Erros de não observação incluem erros de cobertura, amostrais e não resposta. (p. 54)\n\nSample surveys rely on two types of inference – from the questions to constructs, and from the sample statistics to the population statistics. The inference involves two coordinated sets of steps: obtaining answers to questions constructed to mirror the constructs, and identifying and measuring sample units that form a microcos of the target population.\nDespite all efforts, each of the steps is subject to imperfections, producing statistical errors in survey statistics. The errors involving the gap between the measures and the construct are issues of validity. The errors arising during the application of the measures are called “measurement errors”. Editing and processing errors can arise during efforts to prepare the data for statistical analysis. Coverage errors arise when enumerating the target population using a sampling frame. Sampling errors stem from surveys measuring only a subset of the frame population. The failure to measure all sample persons on all measures creates nonresponse error. “Adjustment errors” arise in the construction of statistical estimators to describe the full target population. All of these error sources can have varying effects on different statistics from the same survey. (p. 56)\n\n\n\n3.1.2 Berinsky, A. J. (2017). Measuring public opinion with surveys. Annual review of political science, 20(1), 309–329.\nO artigo enfrenta a seguinte questão: “How can we best gauge the political opinions of citizenry?”, isto é, como podemos medir melhor a opinião pública? Trata-se de uma pergunta fundamental para qualquer sociedade democrática.\n\nThere are two basic choices we make when conducting a poll: which people to interview and what questions to ask them. These choices seem simple, but neither is straightforward. The strategies one chooses can greatly affect the answers one gets. Thus, the voice of the people, as reflected in polls, is profoundly shaped by the decisions we make as researchers. (p. 16.2)\n\n\n3.1.2.1 Whom to ask?\n\nFor most of the twwentieth century, these procedures included face-to-face multi-stage designs and telephone interviewing by random-digit dialing. These methods, though not SRS [simple random sampling], can be conducted in ways that allow researchers to approximate SRS. Statistical methods have been developed to account for the design components of modern survey sampling, such as clustering and stratification (Kish 1965, Lohr 2010). (p. 16.3)\n\nNos Estados Unidos, mais ou menos em meados da década de 1970, mais de 90% das casas tinham telefone, e as taxas de resposta eram altas. No entanto, no fim dos anos 1990 as taxas de resposta começaram a cair de maneira vertiginosa. Além disso, telefones de casa passaram a ser menos utilizados em relação a celulares.\n\nCell phones are tied to individuals, whereas landlines are attached to households. Blending samples with these two types of points of contact is no easy feat. For instance, many individuals are reachable by both cell phone and landline. People who appear in both frames will have a higher probability of inclusion. (p. 16.6).\nIt is extremely difficult to define a target population that we would like to represent with our sample. Hillygus (2016) rightly points out that there exists no list of all internet users we can draw upon analogous to the list of telephone numbers. As a result, research over the internet is often conducted using nonprobability samples (although, of course, just as non-online polls are no inherently probability samples, online polls do not necessarily have to be nonprobability samples). (p. 16.6)\n\nHá também outras estratégias de amostragem, como a Address-Based Sampling (ABS), que utiliza endereços postais como base para a amostragem.\nAlém disso, outro problema envolve o fato de que as pessoas simplesmente estão menos dispostas a responder (cooperar) a pesquisas. Isso pode ser um problema, pois as pessoas que não respondem podem ter características diferentes daquelas que respondem e levar a vieses nas estimativas.\n\nThe central challenge of selecting respondents to interview is to minimize the differences between the people we ask our questions and the people we miss. For instance, the fundamental problem with continuing to use landlines to sample survey respondents is that the vast majority of people who should be in our sample are not in out sample – either because we cannot reach them or because they refuse to answer our questions once we contact them. From this perspective, the proper way to fix the problem is to consider how the people we can reach differ from the people we cannot reach – and account for those differences through weighting. We can adjust our samples, but in order to arrive at an accurate picture of the public will, we must measure the differences between respondents and nonrespondents to particular surveys. (p. 16.7-16.8)\n\n\n[…] probability and nonprobability samples are fundamentally different, and a variety of factors need to be considered when comparing methods of gathering responses. It is important to continue to recognize these differences, even in an era when probability response rates are collapsing to the low single digits. (p. 16.8)\n\n\nProbability samples are built on a strong inferential foundation because no matter how low the response rate, all the sampled units are actively recruited and encouraged to participate in the survey. This is not true for nonprobability samples. (p. 16.8)\n\n\n\n3.1.2.2 What to ask?\nQue tipo de informação podemos efetivamente extrair de perguntas feitas em surveys?\n\nWhen choosing the questions we ask, we need to be cognizant of the fact that almost anyone will answer that question, even if they have little basis for their answer. Under such circumstances, how should we think of surveys responses?\n\n\nZaller (1992, p. 49) argues that individuals answer survey questions off the top of their head by ‘averaging across the considerations that are immediately salient to them’ due to their personal characteristics and political experiences at the time of the survey interview. Survey responses therefore are a summary judgment over the mass of considerations – reasons for favoring one side of a controversity rather than another – that happen to be on their mind when they answer a particular questions. (p. 16.9)\n\n\nBishop (2008) contends that responses to survey questions do not represent actual opinions about the specific policy issues being probed by pollsters, but are an ‘illusion’ based on public ignorance of politics and aided by vague polling questions and variations in question form, wording, and context. Thus, for Bishop, measuring and aggregating survey responses is a meaningless exercise. (p. 16.10)\n\n\nSurvey questions may, as Bishop argues, force most people to ‘make up’ answers on the spot, but there made-up answeres are still meaningful because they reflect individuals’ politically relevant considerations – their underlying distribuion of preferences over the policies of government. (p. 16.10)\n\n\nThe argument in the previous section is that, at the individual level, survey responses are meaningful when the measurement instrument is calibrated at a moderate level of specificity: not so general as to be empty of content and not so specific that they risk falling into the trap described by Bishop, creating opinions where none exist. (p. 16.11)\n\n\nSingle-question approach: uma única pergunta é feita e trazer questões práticas para que pessoas menos envolvidas com política sejam capazes de responder;\nAggregation approach: uma série de perguntas que, combinadas, trazer uma única medida de opinião pública.\n\n\nAlthough the use of general questions should in theory force respondents to evaluate politics at a broader frame of reference, in practice, such questions might induce noisy survey responses. Research in psychology has shown that people sometimes answer general survey questions with a specific frame of reference in mind – one that can differ both across people and across situations. (p. 16.15)\n\n\nThe strategy of achieving generality by aggregating across questions for each individual respondent comes with potential problems as well. The strategy of achieving generality by aggregating across questions for each individual respondent comes with potential problems as well. Broockman (2016) criticizes the use of ideological scales that compute the opinions of individuals by taking their average position on a variety of items—whether a straight average or the weighted average implied by factor analysis or IRT models. He argues that these scales are flawed measures of individual preferences because they inappropriately aggregate items across policy domains. (p. 16.15)\n\n\nMy advice is simple. We should ask general survey questions that demand little specific expertise on the part of survey respondents, and we should combine these questions into measures of opinion in theoretically meaningful ways. Of course, this is not a hard and fast rule. On some issues—those in the public eye—it is possible to gather meaningful data on the public’s specific preferences. But as a general guiding principle, measurement should always go hand in hand with an argument about the validity of that measurement strategy. (p. 16.16)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#anotações-de-aula",
    "href": "aulas/aula-3.html#anotações-de-aula",
    "title": "3  Erros em survey",
    "section": "3.2 Anotações de aula",
    "text": "3.2 Anotações de aula\nA coisa talvez mais importante do survey é que o resultado é proveninente de um processo de interação – e, ao criar uma interação, ele deflagra um certo processo cognitivo. Além disso, a maioria das pessoas não tem opiniões estáticas sobre a maioria dos assuntos de opinião pública – daí, a resposta é uma média ponderada das últimas informações que a pessoa se lembra. De fato, pessoas que se interessam e se informam mais sobre política tendem a ter opiniões mais consistentes.\nAgora, do ponto de vista de um curso formal de survey, o outro lado é falar do Total Survey Error. As duas grandes famílias de erro são os erros de mensuração (isto é, que diz respeito ao que eu quero saber e como eu extraio a informação), e os erros de amostragem (qual é a população-alvo? Como selecionamos a amostra?).\nO capítulo do Groves dá os conceitos básicos de survey: - população - unidade de análise - amostra - quadro amostral: para eu poder fazer inferência sobre uma população, eu preciso de um jeito de definí-la (até porque eu preciso acessá-la). Trata-se do que eu acesso da população – é a parte objetiva do que eu quero acessar da população.1 - desenho amostral - estimação - inferência\nTeoricamente, o mundo possui um certo processo gerador de dados que a gente desconhece e, com inferência, a gente consegue chegar o mais perto possível de como esse processo funciona. A estatística, por outro lado, tem um trabalho mais fácil porque ela se importa com a população, e não com o processo gerador de dados. Não me importa a fórmula; me importa extrapolar a partir de uma amostra. É claro, mesmo a ideia de “população” é um construto que a gente assume que podemos alcançar, mas isso não é necessariamente verdade.\nÉ impossível fazer pesquisa amostral sem quadro amostral.\nNo caso de pesquisa amostral no Brasil, o estado-da-arte envolve sorteios no nível dos territórios. Usando a Malha de Setores Censitários (bem granular, duas ou três quadras), podemos sortear territórios. O melhor que podemos fazer é ir às residências e procurar as pessoas, porque não temos um banco frequentemente atualizado e claro da população adulta. Outro quadro amostral possível é sortear listas telefônicas, assumindo que a maioria das pessoas tem telefone.\nDatafolha, por outro lado, usa pontos de fluxo. Dividimos a unidade territorial em uma série de partes, e procuramos os lugares onde passam mais pessoas. De fato, o lugar onde as pessoas passam no dia a dia. Então não existe listagem das pessoas, mas há uma listagem dos lugares onde as pessoas passam (então, de certa maneira, existe um componente “aleatório”).\nAgora, sobre desenho amostral, temos uma coisa mais “alto nível”. É a receita do bolo. O plano amostral é algo mais objetivo – sorteei os setores censitários etc. E a inferência, no fim das contas, é o que permite a gente extrapolar os dados da amostra para a população, calculando determinadas estatísticas sob incerteza.\n\n3.2.1 Notação básica\nTemos uma população de tamanho \\(N\\) na qual cada pessoa \\(i\\) tem:\n\num valor latente \\(\\mu_i\\)\num valor passível de observação, “fixo”, \\(Y_i\\)\n\nA média da população é:\n\\[\n\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i\n\\]\nEm surveys, extraímos uma amostra \\(S\\) de tamanho \\(n\\), com \\(n \\leq N\\), para obter respostas de uma amostra (provavelmente com erro de mensuração), \\(y_i\\). Daí podemos ajustá-las, chegando a \\(y_{ip}\\), que é a resposta editada. A média amostral é:\n\\[\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n\\]\nNo fim das contas, calculamos uma estatística amostral.\nPodemos ter também um indicador de inclusão da pessoa \\(i\\) na amostra:\n\\[\nI_i = I(i \\in S)\n\\]\nSendo que \\(\\pi_i\\) é a probabilidade de inclusão do indivíduo \\(i\\) na amostra. A probabilidade de inclusão é dada por:\n\\[\n\\pi_i = \\text{Pr}(i \\in S)\n\\]\n\\(y_i\\) é uma variável aleatória que depende do sorteio da amostra.\nAgora, quando o desenho é não probabilístico, a probabilidade de uma pessoa ser sorteada (isto é, \\(\\pi_i\\)) é desconhecida.\n\n\n3.2.2 Probabilidade de inclusão\nEm populações grandes, a probabilidade de inclusão em uma amostra de qualquer \\(i\\) é geralmente ínfima. Imagine uma pessoa com \\(\\pi_i = 0.25\\):\n\\[\nw_i = \\frac{1}{\\pi_i} = \\frac{1}{0.25} = 4\n\\]\nEla representa 4 pessoas na população. Se \\(y_i = 10\\), ela contribui \\(4 \\times 10 = 40\\) para a estimativa do total. \\(w_i\\) é frequentemente chamado de peso da pessoa \\(i\\).\n\nA ideia de que conseguimos saber a probabilidade de inclusão de uma pessoa na amostra nos permite dizer quantas pessoas aquela pessoa representa na população.\n\n\n\n3.2.3 Estimador de Horvitz-Thompson\nPara estimar um total populacional, somamos os valores observados na amostra ponderados pelos seus pesos, \\(w_i\\). Se eu sei quantas pessoas cada uma das pessoas na amostra reprensenta, então eu consigo estimar o total populacional. Peso também pode ser chamado de fator de expansão.\n\\[\n\\hat{T}_{HT} = \\sum_{i=1}^{n} w_i y_i = \\sum_{i=1}^{n} \\frac{y_i}{\\pi_i}\n\\]\nMesmo usando HT, a estimativa do total pode diferir de \\(T\\) na população; com amostras aleatórias, cada realização é ligeiramente diferente. Isso é a variância. Podemos repetir 1000 vezes o sorteio da amostra de \\(n=3\\), obtendo algo próximo de uma distribuição normal.\n\n\n3.2.4 Erros\nErros nada mais são do que diferenças entre estimativa amostral e parâmetro populacional (assumindo, por ora, amostras aleatórias com sorteios independentes de uma população infinita). A quantidade de uma pesquisa depende da proximidade entre \\(\\bar{y}\\) e \\(\\bar{Y}\\).\nUma forma teórica de quantificar o erro total para uma estatística específica:\nErro quadrático médio, que combina viés e variância:\n\\[\n\\text{MSE}(\\bar{y}) = \\text{Var}(\\bar{y}) + \\text{Bias}^2(\\bar{y})\n\\]\n\n\\(\\text{Var}(\\bar{y})\\) é a variância amostral, que mede a variabilidade da média amostral em torno da média populacional.\n\\(\\text{Bias}(\\bar{y})\\) é a diferença entre a média amostral e a média populacional.\n\nPodemos ter variância dentro da amostra (isto é, entre os respondentes) e variância entre amostras (isto é, entre diferentes realizações).\n\n3.2.4.1 Variância e tamanho amostral\nUma população infinita gerada por:\n\\[\n\\begin{align*}\n&y_i = \\mu + \\epsilon_i \\\\\n&\\epsilon_i \\sim N(0, \\sigma^2)\n\\end{align*}\n\\]\nCom \\(\\mu = 100\\) e \\(\\sigma = 5\\):\n\nset.seed(123)\n\n# populacao\nn &lt;- 2000\n\n# media\nmu &lt;- 100\n\n# desvio padrao\nsigma &lt;- 5\n\n# gerando a populacao\ny &lt;- rnorm(n, mu, sigma)\n\n# histograma da distribuicao adicionando uma linha na media\nhist(y, breaks = 20, main = \"Distribuição da população\", xlab = \"y\", ylab = \"Frequência\")\nabline(v = mu, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nAgora, se temos viés na amostra, podemos ter uma média amostral diferente da média populacional:\n\\[\n\\begin{align*}\n&y_i = \\mu + \\epsilon_i + \\text{viés} \\\\\n&\\epsilon_i \\sim N(0, \\sigma^2) \\\\\n&\\text{viés} = 5\n\\end{align*}\n\\]\nNesse caso, a média amostral é 105:\n\n# amostra\nn &lt;- 2000\nmu &lt;- 100\nsigma &lt;- 5\n\n# amostra com viés\ny_biased &lt;- rnorm(n, mu + 5, sigma)\n\n# histograma da distribuicao adicionando uma linha na media\nhist(y_biased, breaks = 20, main = \"Distribuição da amostra com viés\", xlab = \"y\", ylab = \"Frequência\")\nabline(v = mu, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Total Survey Error\nO TSE é um framework sobre os dois grandes processos que geram erros: mensuração e amostragem (Groves e Lyberg, 2010). Há uma série de fontes para erros: erros de mensuração, de cobertura, de pós-ajuste (pós-estratificação), de não-resposta, amostral, enfim.\nErros de mensuração: quando a resposta coletada \\(y_i\\) difere de \\(Y_i\\)\n\nInterpretação errada da pergunta\nEsquecimento\nAnotação errada\nVergonha de revelar uma opinião\nRespostas aleatórias\n\nErro de cobertura: quando o quadro amostral não abarca toda a população-alvo. Ou melhor, quando o quadro amostral não efetivamente se ajusta à população-alvo. Algumas fontes comuns são registros desatualizados, exclusão de domicílios em áreas remotas ou inacessíveis, pessoas sem telefone ou sem internet, entre outros.\n\nSubcobertura\nSobrecobertura\n\nErro de não resposta: parte da amostra não responde. Se não respondentes diferem sistematicamente dos respondentes, há viés de não resposta. O que importa é a diferença entre quem responde e quem não responde.\n\nUnit nonresponse: pessoa não responde nada\nItem nonresponse: pessoa responde parcialmente (isto é, não responde a todas as perguntas)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-3.html#footnotes",
    "href": "aulas/aula-3.html#footnotes",
    "title": "3  Erros em survey",
    "section": "",
    "text": "No Brasil, a gente trata principalmente do CNEFE – o Cadastro Nacional de Endereços para Fins Estatísticos. É daqui que tiramos as “listas” para o quadro amostral e chegue perto da população de domicílios.↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Erros em survey</span>"
    ]
  },
  {
    "objectID": "aulas/aula-4.html",
    "href": "aulas/aula-4.html",
    "title": "4  Amostragem I",
    "section": "",
    "text": "4.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Amostragem I</span>"
    ]
  },
  {
    "objectID": "aulas/aula-4.html#anotações-das-leituras",
    "href": "aulas/aula-4.html#anotações-das-leituras",
    "title": "4  Amostragem I",
    "section": "",
    "text": "4.1.1 Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., & Tourangeau, R. (2011). Survey methodology. John Wiley & Sons. Cap. 3-4.\n\n4.1.1.1 Capítulo 3 - Target Populations, Sampling Frames, and Coverage Error\n\nIn short, statistics describing different populations can be collected in a single survey when the populations are linked to units from which measurements are taken.\nAmong common research tools, surveys are unique in their concern about a well-specified population. For example, when conducting a randomized biomedical experiments, the researcher often pays much more attention to the experimental stimulus and the conditions of the measurement than to the identification of the population under study. The implicit assumption in such research is that the chief purpose is identifying the conditions under which the stimulus produces the hypothesized effect. The demonstration that it does so for a variety of types of subjects is secondary. Because surveys evolved as tools to describe fixed, finite populations, survey researchers are specific and explicit about definitions of populations under study. (p. 69)\n\n\nTarget population: grupo de “elementos” sobre os quais a pesquisa de survey deseja fazer inferência usando estatísticas amostrais (sample statistics). A população-alvo é finita em tamanho (isto é, pode ser contada); existem durante um certo período de tempo; e são observáveis.\n\n\nFor example, the target population of many U.S. household surveys is persons 18 years of age or older, “adults” who reside in housing units within the United States. (p. 70)\n\n\nThere are many populations, though, for which lists of individual elements are not readily available. For example, in the United States lists are seldom avaliable in one place for all students attending school in a province or state, inmates in prisions, or even adults living in a specific county. (p. 71)\n\n\nWhen available sampling frames miss the target population partially or entirely, the survey researcher faces two options:\n\nRedefine the target population to fit the frame better\nAdmit the possibility of coverage error in statistics describing the original target population (p. 71)\n\n\n\nA common example of redefining the target population is found in telephone household surveys, where the sample is based on a frame of telephone numbers. Although the desired target population might be all adults living in the U.S. households, the attraction of using the telephone frame may persuade the researcher to alter the target population to adults living in telephone households. Alternatively, the researcher can keep the full household target population and document that approximately 2% of U.S. adults are missed because they have no telephones. (p. 71)\n\nThe match of sampling frame to target population created three potential outcomes:\n\nCoverage - when a target population element is included in the sampling frame\nUndercoverage - when a target population element is not included in the sampling frame\nIneligible (or foreign) units - when a sampling frame element is not a target population element (e.g. business numbers in a frame of residential numbers)\n\nOutros problemas incluem “Duplication” – isto é, vários elementos da framing são mapeados para um único elemento da população alvo e, nesse caso, há sobrerrepresentação de alguns elementos; além disso, há também o problema de “Clustering”, quando múltiplos elementos da população alvo a um único elemento da frame.\n\n4.1.1.1.1 Undercoverage\n\nUndercoverage is the weakness of sampling frames promptiong the greatest fears of coverage error. It threatens to produce errors of nonobservation in survey statistics from failure to include parts of the target population in any survey using the frame. (p. 72)\n\n\nAnother common concern about undercoverage in household surveys stems from the fact that sampling frames for households generally provide identification of the housing unit (through an address or telephone number) but not identifiers for persons within the household.\n\n\n\n4.1.1.1.2 Ineligible Units\n\nSometimes, sampling frames contain elements that are not part of the target population. For example, in telephone number frames, many of the numbers are nonworking or nonresidential numbers, complicating the use of the frame for the target population of households. In area probability surveys, sometimes the map materials contain units outside the target geographical area. (p. 76)\n\n\nWhen interviewers develop frames of household members withing a unit, they often use residence definitions that do not match the meaning of “household” held by the informant. Parents of the students living away from home often think of them as members of the household, yet many survey protocols would place them at college. The informants might tend to exclude persons unrelated to them who rent a room in the housing unit. Studies show that children in shared custody between their father and mother are likely to be duplicated by appearing in each parent’s household list. (p. 76)\n\n\nAlthough undercoverage is a difficult problem, “ineligible” or “foreign” units in the frame can be a less difficult problem to deal with if the problem is not extensive. When foreign units are identified on the frame before selection begins, they can be purged at little cost. […]. For example, it is known that approximately 15% of entries in residential portions of national telephone directories are numbers that are no longer in service. To achieve a sample of 100 telephone households, one could select a sample of \\(100(1 - 0.15) = 118\\) entires from the directory, expecting that 18 are going to be out-of-service numbers. (p. 76)\n\n\n\n4.1.1.1.3 Clustering\n\nA telephone directory lists telephone households in order by surname, given name, and address. When sampling adults from this frame, an immediately obvious problem is the clustering of eligible persons that occurs. “Clusterng” means that multiple elements of the target population are represented by the same frame element. A telephone listing in the directory may have a single or two or more adults living there. (p. 77)\n\n\nOne way to react to clustering of target population elements is by simply selecting all eligible units in the selected telephone households (or all eligible units in a cluster). With this design, the probability of selection of the cluster applies to all elements in the cluster. (p. 77)\n\n\nAfter sample selection, there is one other issue that needs to be addressed in this form of cluster sampling: unequal probabilities of selection. If all frame elements are given equal chances, but one eligible selection is made from each, then elements in large clusters have lower overall probabilities of selection than elements in small clusters. For example, an eligible person chosen in a telephone household containing two eligibles has a chance of one-half of being selected, given that the household was sampled, whereas those in a household with four eligibles have a one-in-four chance. (p. 78)\n\n\n\n4.1.1.1.4 Duplication\n\n“Duplication” means that a single target population element is associated with multiple frame elements. In the telephone survey example, this may arise when a single telephone household has more than one listing in a telephone directory. (p. 79)\n\n\nThe problem that arises with this kind of frame problem is similar to that encountered with clustering. Target population elements with multiple frame units have higher chances of selection and will be overrepresented in the sample, relative to the population. If there is a correlation between duplication and variables of interest, survey estimates will be biased. In survey estimation, the problem is that both the presence of duplication and the correlation between duplication and survey variables are often unknown. (p. 79)\n\n\n\n4.1.1.1.5 Clustering and Duplication\n\nIt is also possible to have frame units mapped to multiple population elements. For example, in telephone household surveys of adults, one may encounter a household with several adults who have multiple entries in the directory. (p. 80)\n\n\n\n4.1.1.1.6 Desenhos de amostragem e seus problemas\nHá vários framing problems relacionados a pesquisas de survey. Há uma série de desenhos alternativos, que também possuem problemas atrelados:\n\nArea frames (list of area units like census tracts or counties): primeiro, um subconjunto de áreas é selecionado; depois, listagens de endereços são geradas para a área selecionada.\nTelephone number frames for households and persons: ao usar linhas fixas, falhamos em cobrir cerca de 20% das unidades residenciais dos Estados Unidos. Além disso, um percentual menor possui mais de uma linha fixa e acabam sendo duplicados. Além disso, com o uso de celulares, essa abordagem torna-se mais complicada – mesmo porque números de telefone celular são associados a uma pessoa, e não a um endereço.\nFrames for Web Surveys of General Populations: “The e-mail frame, however, fails to cover large portions of the household population. It has duplication problems because one person can have many different e-mail addresses, and it has clustering problems because more than one person can share an e-mail address.” (p. 83).\n\n“In short, without a universal frame of e-mail addresses with known links to individual population elements, some survey practices have begun to ignore the frame development step. Without a well-defined sampling frame, the coverage error of resulting estimates is completely unknowable.” (p. 84).\n\n\n4.1.1.1.7 Coverage error\n\nUndercoverage is a difficult problem, and may be an important source of coverage error in surveys. It is important to note, though, that coverage error is a property of sample statistics and estimates made from surveys.\n\nNo caso de uma estimativa de média, o viés de cobertura é dado por:\n\\[\n\\bar{Y}_C - \\bar{Y} = \\dfrac{U}{N} (\\bar{Y}_C - \\bar{Y}_U),\n\\]\none \\(\\bar{Y}\\) é a média da população total, \\(\\bar{Y}_C\\) é a média da população coberta e \\(\\bar{Y}_U\\) é a média da população não coberta. \\(U\\) é o número de unidades não cobertas e \\(N\\) é o número total de unidades. “Thus, the error due to not covering the \\(N-C\\) units left out of the frame is a function of the proportion ‘not covered’ and the difference between means for the covered and the not covered.” (p. 88).\n\n\n4.1.1.1.8 Reduzindo o erro de cobertura\n\nThe Half-Open Interval: “Consider, for example, address or housing unit lists used in household surveys. These lists may become out of date and miss units quickly. They may also have missed housing units that upon closer inspection could be added to the list. Since address lists are typically in a particular geographic order, it is possible to add units to the frame only for selected frame elements, rather than updating the entire list.” (p. 88).\nMultiplicy sampling: “Some frame supplementation methods add elements to a population through network sampling.” (p. 90).\nMultiple frame designs: “For example, an out-of-date set of listings of housing units can be supplemented by a frame of newly constructed housing units obtained from planning departments in governmental units responsible for zoning where sample addressses are located” (p. 91).\n\n“At times, the supplemental frame may cover a completely separate portion of the population. In most cases, though, supplemental frames overlap with the principal frame. In such cases, multiple framing sampling and estimation procedures are employed to correct for unequal probabilities of selection and possibly to yield improved precision for survey estimates.” (p. 91).\n“There are several solutions to the overlap and overrepresentation problem. One is to screen the area household frame. At the doorstep, before an interview is conducted, the interviewer determines whether the household has a fixed-line telephone that would allow it to be reached by telephone. If so, the unit is not selected and no interview is attempted. With this procedure, the overlap is eliminated, and the dual frame sample design has complete coverage of households.\nA second solution is to attempt interviews at all sample households in both frames, but to determine the chance of selection for each household. Households from the nonoverlap portion of the sample, the nontelephone households can be selected from the area frame and, thus, have one chance of selection. Telephone households have two chances, one from the telephone frame and the other from the area household frame. Thus, theis chance of selection is \\(p_{RDD} + p_{area} - p_{RDD} \\times p_{area}\\), where \\(p_{RDD}\\) and \\(p_{area}\\) denote the chances of selection for the RDD and area sample households. A compensatory weight can be computed as the inverse of the probability of selection: \\(1/p_{area}\\) for nontelephone households and \\(1/(p_{RDD} + p_{area} - p_{RDD} \\times p_{area})\\) for telephone households, regardless of which frame was used.\nA third solution was proposed by Hartley (1962) and others. They suggested that the overlap of the frames be used to obtain a more efficient estimator. […]. The telephone and nontelephone domains are combined using a weight that is the proportion of the telefone households in the target population, say \\(W_{tel}\\). The dual frame estimator for this particular example is:\n\\[\n\\bar{y} = (1 - W_{tel}) p_{non-tel} + W_{tel} [ \\theta p_{RDD-tel} + (1 - \\theta) p_{area-tel}],\n\\]\nwhere \\(\\theta\\) is the mixing parameter chosen to maximize precision.” (p. 92).\n\n\n\n4.1.1.2 Capítulo 4 - Sample Design and Sampling Error\n\nHow the sample of a survey is obtained can make a difference. The self-selected nature of the NGS survey, coupled with its placement on the National Geographic Society’s website, probably yielded respondents more interested and active in cultural events (Couper, 2000). (p. 97)\n\n\nThe random sampling mechanism and geographic controls are designed to avoid the selection of a sample that has higher incomes, or fewer members of ethnic or racial minorities, or more females, or any of a number of other distortions when compared to the entire U.S. population. (p. 98)\nEssentially, in its simplest form, the kind of sample selection used in carefully designed surveys has three basic features:\n\nA list, or combinations of lists, of elements in the population (the sampling frame described in Chapter 3)\nChance or random selection of elements from the list(s)\nSome mechanism that assures that key subgroups of the population are represented in the sample (p. 98)\n\n\n\nWhen chance methods, such as tables of random numbers, are applied to all elements of the sampling frame, the samples are referred to as “probability samples”. Probability samples assign each element in the frame a known and nonzero chance to be chosen. These probabilities do not need to be equal. For example, the survey designers may need to overrepresent a small group of the population, such as persons age 70 years and older, in order to have enough of them in the sample to prepare separate estimates for the group. (p. 98)\n\n\nSection 2.3 makes the distinction between fixed errors or biases and variable errors or variance. Both errors can arise through sampling. The basis of both sampling bias and sampling variance is the fact that not all elements of the sampling frame are measured. If there is a systematic failure to observe some elements because of the sample design, this can lead to “sampling bias” in the survey statistics. For example, if some persons in the sampling frame have a zero chance of selection and they have different characteristics on the survey variable, then all samples selected with that design can produce estimates that are too high or too low. Even when all frame elements have the same chance of selection, the same sample design can yield many different samples. They will produce estimates that vary; this variation is the basis of “sampling variance” of the sample statistics. We will use from time to time the word “precision” to denote the levels of variance. (p. 98)\n\n\nIn some sense, you should think of each survey as one realization of a probability sample design among many that could have occurred. It is conventional to describe attributes of the sample realizations using lower case letters. So each sample realization has a mean and a variance of the distribution of \\(y\\) in the sample realization. The sample mean is called \\(\\bar{y}\\), which summarizes all of the different values of \\(y\\) on the sample elements, and the variance of the distribution of \\(y_i\\)’s in the sample realization os labeled as \\(s^2\\). We will describe sample designs in which values of \\(\\bar{y}\\) from one sample realization will be used to estimate \\(\\bar{Y}\\) in the frame population. We will describe sample designs in which values of \\(s^2\\) from one sample realization will be used to estimate \\(S^2\\) in the frame population. (p. 100-101)\n\n\nDo not confuse the different “variances” discussed. Each frame population has its own distribution of \\(Y\\) values, \\(S^2\\), the population element variance, estimated by \\(s^2\\), the sample element variance from any given sample realization. These are each different from the variance of the sample mean, labeled as \\(V(\\bar{y})\\), estimated by using sample realization data, using \\(v(\\bar{y})\\). Capital letters denote frame population quantities; lower case letters generally denote sample quantities.\n\n\n[…] the extent of the error due to sampling is a function of four basic principles of the design:\n\nHow large a sample of elements is selected\nWhat chances of selection into the sample different frame population elements have\nWhether individual elements are drawn directly and independently or in groups (called “element” or “cluster” samples)\nWhether the sample is designed to control the representation of key sub-populations in the sample (called “stratification”) (p. 102)\n\n\n\n4.1.1.2.1 Simple Random Sampling (SRS)\n\nSimple random samples assign an equal probability of selection to each frame element, equal probability to all pairs of frame elements, equal probability to all triplets of frame elements, and so son. (p. 103)\n\n\nTo select an SRS, random numbers can be applied directly to the elements in the list. SRS uses list frames numbered from 1 to \\(N\\). Random numbers from 1 to \\(N\\) are selected from a table of random numbers, and the corresponding population elements are chosen from the list. If by chance the same population element is chosen more than once, we do not select it into the sample more than once. Instead, we keep selecting until we have \\(n\\) distinct elements from the population. (This is called sampling “without replacement”.)\nFrom and SRS, we compute the mean of the variable \\(y\\) as\n\\[\n\\bar{y} = \\dfrac{1}{n} \\sum_{i=1}^{n} y_i,\n\\]\n[…]. The sampling variance of the mean can be estimated directly from one sample realization as\n\\[\nv(\\bar{y}) = \\dfrac{(1 - f)}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\dfrac{(1 - f)}{n} s^2,\n\\]\n[…]. The term \\((1-f)\\) is referred to as the finite population correction, or fpc. THe finite population correction equals the proportion of frame elements not sample or \\(1 - f\\), where \\(f\\) is the sampling fraction. As a factor in all samples selected without replacement, it acts to reduce the sampling variance of statistics. If we have a situation in which the sample is a large fraction of the population, where \\(f\\) is closer to 1, then the fpc acts to decrease the sampling variance. Often, the frame population is so large relative to the sample that the fpc is ignored.\nWhen \\(f\\) is small, the fpc is close to 1, and\n\\[\nv(\\bar{y}) \\approx \\dfrac{1}{n} s^2.\n\\]\n\n\n\n4.1.1.2.2 Cluster Sampling\n\nOne way to construct frames at reasonable cost is to sample clusters of elements and then collect a list of elements only for the selected clusters. And when a sample cluster is selected and visited, it makes sense to interview or collect data from more than one element in each cluster also to save costs.\n\n\nHow do we compute statistics for clustered samples? Let us use NAEP as an illustration. For example, suppose that for the NAEP, we could obtain a list of all \\(A = 40,000\\) 4th grade classrooms in the United States, and that each of those classrooms has exactly \\(B = 25\\) students. We do not have a list of the \\(A \\times B = N = 40,000 \\times 25 = 1,000,000\\) students, but we know that when we get to a classroom, we can easily obtain a list of the 25 students there.\nThe sampling procedure is simple: chose a sample of \\(a\\) classrooms using SRS, visit each classroom, and collect data from all students in each sample classroom. If we select \\(a = 8\\) classrooms, out sample size is \\(n = 8 \\times 25 = 200\\) students. It should be clear that this is not the same kind of sampling as an SRS of elements. There are many SRS samples of size 200 students that cannot possibly be chosen in this kind of a design. For instance, some SRS sample realizations consist of exactly one student in each of 200 classrooms. None of these are possible cluster samples of 200 students from 8 classrooms.\n\nNesse caso, do ponto de vista de notação, a coisa fica um pouco mais complexa porque precisamos levar em conta a natureza agrupada dos dados. Imagine que obtemos para cada um dos 200 estudantes da amostra um score de teste \\(y_{\\alpha \\beta}\\) para o estudante \\(\\beta\\) na sala de aula \\(\\alpha\\), e queremos computar a média do score de teste para a população de estudantes:\n\\[\n\\bar{y} = \\dfrac{1}{N} \\sum_{\\alpha=1}^{A} \\sum_{\\beta=1}^{B} y_{\\alpha \\beta}\n\\]\n\nThe sampling variance of this mean is different from the SRS sampling variance. The randomization in the selection is applied only to the classrooms. They are the sampling units, and depending on which classrooms are chosen, the value of \\(\\bar{y}\\) will vary. In one sense, everything remains the same as SRS, but we treat the clusters as elements in the sample. (p. 108)\nIn this case,\n\\[\nv(\\bar{y}) = \\dfrac{(1 - f)}{A} s^2_a\n\\]\nwhere \\(s^2_a\\) is variability of the mean test scores across the \\(A\\) classrooms. That is,\n\\[\ns^2_a = \\dfrac{1}{A - 1} \\sum_{\\alpha=1}^{A} (\\bar{y}_{\\alpha} - \\bar{y})^2,\n\\]\nwhere \\(\\bar{y}_{\\alpha}\\) is the mean test score in classroom \\(\\alpha\\) (p. 108).\n\n\nThe statistic \\(d^2\\) is referred to as the design effect. It is a widely used tool in survey sampling in the determination of sample size, and in summarizing the effect of having sampled clusters instead of elements. It is defined to be the ration of the sampling variance for a statistic computed under the sample design devided by the sampling variance that would have been obtained from an SRS of exactly the same size. Every statistic in a survey has its own design effect. Different statistics in the same survey will have different magnitudes of the design effect. (p. 109)\n\n\n\n4.1.1.2.3 The Design Effect and Within-Cluster Homogeneity\nÉ importante nos atentarmos para a homogeneidade ou heterogeneidade das observações dentro de cada cluster. Note: se todos os alunos em uma turma tiram a mesma nota no teste, bastaria saber a nota de um único aluno para saber a nota de todos os alunos. Nesses casos, pouca informação nova é adquirida ao entrevistar mais alunos. A medida “roh” (rate of homogeneity) mede a homogeneidade dos clusters.\nEm particular, vale notar que \\(d^2\\) cresce à medida que roh cresce. De fato, podemos escrever \\(d^2\\) como \\(1 + (B - 1) \\rho\\), onde \\(B\\) é o número de elementos dentro de cada cluster. Assim, se \\(\\rho = 0\\), temos \\(d^2 = 1\\) e, se \\(\\rho = 1\\), temos \\(d^2 = B\\).\nPodemos estimar roh como \\(\\dfrac{(d^2 - 1)}{(b - 1)}\\).\n\n4.1.1.2.3.1 Stratification and Stratified Sampling\n\nOn a frame of population elements, assume that there is information for every element on the list that can be used to divide elements into separate groups, or “strata”. Strata are mutually exclusive groups of elements on a sampling frame. Each frame element can be placed in a singular “stratum”. In stratified sampling, independent selections are made from each stratum, one by one. Separate samples are drawn from each such group, using the same selection procedure (such as SRS in each stratum, when the frame lists elements) or using different selection procedures (such as SRS of elements in some strata, and cluster sampling in others). (p. 113)\n\n\nTo obtain estimates for the whole population, results must be combined across strata. One method weights stratum results by the population proportions \\(W_h\\). Suppose we are interested in estimating the mean for the population, and we have computed means \\(\\bar{y}_h\\) for each stratum. The stratified estimate for the population mean is called \\(\\bar{y}_{st}\\) with the subscript st denoting “stratified”. It is computed by:\n\n\\[\n\\bar{y}_{st} = \\sum_{h=1}^{H} W_h \\bar{y}_h = \\text{weighted sum of strata means}\n\\]\n\n\n4.1.1.2.3.2 Summary\n\nMost of the practices of sample design are deduced from theorems in probability sampling theory. Simple random sampling is a simple base design that is used for the comparison of all complex sample designs. There are four features of sample design on which samples vary:\n\nThe number of selected units on which sample statistics are computed (other things being equal, the larger the number of units, the lower the sampling variance of statistics)\nThe use of stratification, which sorts the frame population into different groups that are then sampled separately (other things being equal, the use of stratification decreases the sampling variance of estimates)\nThe use of clustering, which samples groups of elements into the sample simultaneously (other things being equal clustering increases the sampling variance as a function of homogeneity within the clusters on the survey statistics)\nThe assignment of variable selection probabilities to different elements on the frame (if the higher selection probabilities are assigned to groups that exhibit larger variation on the survey variables, this reduces sampling variance; if not, sampling variances can be increased with variable selection probabilities)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Amostragem I</span>"
    ]
  },
  {
    "objectID": "aulas/aula-4.html#anotações-de-aula",
    "href": "aulas/aula-4.html#anotações-de-aula",
    "title": "4  Amostragem I",
    "section": "4.2 Anotações de aula",
    "text": "4.2 Anotações de aula\nEm 1936, George Gallup entrevistou 50.000 eleitores de diferentes estados americanos. Mesmo assim, é a pessoa que chega mais perto de acertar os resultados das eleições, contrastando com a revista Literary Digest, que aplicava formulários “opt-in” e entrevistou 2 milhões de eleitores.\nHá uma explicação ainda muito repetida de que a amostra da Literary Digest tinha uma amostra focada na classe média e, por outro lado, a amostra de Gallup chegava a pessoas mais pobres. No entanto Lusinchi (2012) mostra que, na verdade, não havia diferenças muito claras entre as amostras quando ajustadas — isto é, pobres e ricos votavam mais ou menos da mesma maneira. Daí, a culpa que era delegada ao erro de cobertura passa a ser delegada ao erro de não-resposta: as pessoas que não respondiam à Literary Digest votavam mais no Roosevelt, o que acaba subestimando o apoio a ele na amostra.\nA soma dos erros de survey leva a erros na estimação de uma particular estatística. Por exemplo: se eu quero estimar a renda da população, eu preciso tomar cuidado para evitar uma longa sequência de erros.\n\nNota: fazer uso de dados de pesquisa de survey comparando o resultado entre institutos não é uma boa ideia. Na prática, a maneira como a pergunta é feita tem um efeito grande. Por exemplo, nas pesquisas da AtlasIntel, as respostas tendem a ser mais extremas do que nas pesquisas da Quaest, por exemplo, onde a taxa de avaliações “Regulares” tende a ser mais alta.\n\n\n4.2.1 Amostragem aleatória simples (AAS)\n\nCada elemento \\(i\\) tem a mesma probabilidade de ser incluído na amostra\nA probabilidade de inclusão é \\(\\pi_i = \\dfrac{n}{N}\\), onde \\(n\\) é o número de elementos da amostra e \\(N\\) é o número de elementos da população\nSeleção tipicamente feita sem reposição, i.e., cada \\(i\\) só pode ser selecionado uma vez\nNúmero total de amostras possíveis é dado por1 \\(\\left(\\begin{array}{c} N \\\\ n \\end{array}\\right) = \\dfrac{N!}{n!(N-n)!}\\)\nCada amostra específica tem probabilidade \\(\\mathbb{P}(S) = \\dfrac{1}{\\left(\\begin{array}{c} N \\\\ n \\end{array}\\right)}\\) de ser selecionada\n\n\n4.2.1.1 Um tutorial\nPara uma amostra aleatória simples, o quadro amostral precisa ser uma lista de elementos (ou uma proxy para uma lista de elementos, como uma lista telefônica ou de endereços).\n\nPrimeiro, portanto, definimos a população-alvo e o quadro amostral (e.g., lista de estudantes de ensino médio do RJ).\nDepois, definimos o tamanho amostral (e.g., 100 estudantes).\nE aí sorteamos sem reposição (de fato, não faz sentido sorteio com reposição, porque não faz sentido perguntar para a mesma pessoa mais de uma vez). No R, por exemplo, usamos sample() para escolher \\(n\\) pessoas da lista usando IDs para contabilizar a seleção.\n\nHá prós e contras. Em particular, presencialmente ou pesquisa domiciliar é impossível de fazer dessa maneira. Suponha uma população de 3 pessoas: \\(\\{ Y_1, Y_2, Y_3 \\}\\). Queremos uma amostra com \\(n=2\\). Temos as seguintes combinações possíveis: \\((Y_1, Y_2)\\), \\((Y_1, Y_3)\\) e \\((Y_2, Y_3)\\). Se tivéssemos arranjos, teríamos \\((Y_1, Y_2)\\), \\((Y_1, Y_3)\\), \\((Y_2, Y_1)\\), \\((Y_2, Y_3)\\), \\((Y_3, Y_1)\\) e \\((Y_3, Y_2)\\). No caso da amostragem, não importa se a amostra é \\((Y_1, Y_2)\\) ou \\((Y_2, Y_1)\\), por exemplo. Assim, o denominador remove as duplicatas. No fundo, o denominador mostra quantas vezes cada arranjo é contado. Assim, se o denominador é \\(2\\), significa que cada arranjo é contado duas vezes. Também chamamos isso de coeficiente binomial.\nPara uma AAS, no mais das vezes temos um quadro amostral como uma listagem de pessoas ou uma listagem de números de telefone. Se tivermos uma amostra com \\(n=50\\) e registrar a idade \\(y_i\\) de cada:\n\nCada amostra obtida é uma realização de uma variável aleatória: o plano amostral.\nDistribuições de estatísticas amostrais revelam propriedades de um desenho amostral\n\nVariância de uma estimativa: \\(\\text{Var}(\\bar{y}) = \\mathbb{E}[\\bar{y}^2] - \\mathbb{E}[\\bar{y}]^2\\). Já o viés de uma estimativa é dado por \\(\\text{Bias}(\\bar{y}) = \\mathbb{E}[\\bar{y}] - \\bar{y}\\).\n\n# cria um banco de pessoas com idades entre 18 e 80 anos\npessoas &lt;- data.frame(id = 1:250, idade = sample(18:80, size = 250, replace = TRUE))\n\n# 5 amostras de 50 pessoas\nfor (i in 1:5){\n    x &lt;- sample(pessoas$idade, size = 5, replace = FALSE)\n    print(mean(x))\n}\n\n[1] 45\n[1] 52.2\n[1] 49.2\n[1] 42\n[1] 41.8\n\n\nDe outra maneira, podemos usar a função slice_sample() do dplyr para fazer amostras aleatórias simples. Por exemplo:\n\nlibrary(dplyr)\n\n\nAnexando pacote: 'dplyr'\n\n\nOs seguintes objetos são mascarados por 'package:stats':\n\n    filter, lag\n\n\nOs seguintes objetos são mascarados por 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# 5 amostras de 50 pessoas\nfor (i in 1:5){\n    x &lt;- pessoas %&gt;% slice_sample(n = 5, replace = FALSE)\n    print(mean(x$idade))\n}\n\n[1] 54.8\n[1] 49.4\n[1] 51.4\n[1] 59.6\n[1] 53\n\n\n\n\n\n4.2.2 Amostragem sistemática\nO jeito de selecionar uma amostra não é por sorteio aleatório, mas por “pulos” na lista. A ideia básica é que usamos um quadro amostral ordenado por \\(i\\) e um intervalo fixo, \\(k\\). Suponha que \\(N = 10\\) e queremos uma amostra \\(n=3\\).\nCalculamos \\(k = \\dfrac{N}{n} = \\dfrac{10}{3} \\approx 3\\). Esse é o tamanho do pulo. Assim, escolhemos um ponto inicial aleatório e vamos definindo os elementos. Uma grande vantagem da amostragem sistemática é que ela evita que recrutemos pessoas próximas da cauda da distribuição. Esse tipo de amostragem é extremamente comum – por exemplo, a cada \\(100\\) clientes você entrevista uma.\n\n\n4.2.3 Amostragem estratificada\nUma amostra estratificada é uma amostra aleatória simples de cada estrato estratificado (criado em função de alguma característica da população que seja conhecida — isto é, uma característica que está sendo levada em conta no meu quadro amostral). Não pode haver sobreposição entre os estratos – isto é, uma pessoa deve pertencer a um, e somente a um estrato.\n\nIdeia: dividimos a população em \\(h\\) estratos (e.g., região, tipo de moradia, etc.) e selecionamos uma amostra aleatória simples de cada estrato\nHá vários tipos de alocação possíveis, mas a mais comum é a proporcional: \\(n_h = \\dfrac{N_h}{N} \\times n\\), onde \\(N_h\\) é o número de elementos do estrato \\(h\\) e \\(N\\) é o número total de elementos da população. Assim, a amostra total é dada por \\(n = \\sum_{h=1}^{H} n_h\\).\n\nIsso é uma ideia profundamente importante e interfere diretamente na maneira como calculamos as estatísticas amostrais.\nSó há vantagens: sabemos como alocar pessoas de antemão, facilita logística, etc. Mas, para além das questões logísticas, diminuímos muito o espaço amostral das respostas possíveis. Dessa maneira, a distribuição marginal das entrevistas espelha a distribuição marginal do quadro amostral. De fato, são amostras aleatórias dentro dos estratos, mas os estratos sempre podem ser comparados entre si. Pesquisas telefônicas, por exemplo, fazem amostras estratificadas por DDD.\nQuanto mais homogêneos são os estratos internamente, melhor. Se há pouca variabilidade, não importa quem a gente escolhe. A informação da homogeneidade é fundamental: escolher grupos que sejam semelhantes naquela dimensão que eu quero estudar. Assim, conseguimos diminuir a variância do meu desenho amostral, isto é, obtemos uma distribuição normal com caudas cada vez menos pesadas (isto é, mais centradas em torno da média).\n\n\n4.2.4 A relação entre amostragem estratificada e amostragem sistemática\nNa amostragem sistemática, se a lista for ordenada de acordo com características relevantes da população, ela também pode ser chamada de “estratificação implícita”. No fim das contas, diminuímos o espaço amostral e reduzimos a variância da amostra.\n\n\n4.2.5 Amostragem por conglomerados\n\nIdeia: a população é dividida em grupos (conglomerados ou clusters) e selecionamos aleatoriamente alguns inteiramente, com todos os seus elementos, para integrar a amostra.\nIndexação dupla:\n\n\\(y_{ij}\\) é o valor da unidade \\(j\\) no conglomerado \\(i\\)\n\\(N\\) é o total de conglomerados\n\\(M_i\\) é o tamanho do conglomerado \\(i\\)\n\\(n\\) é o número de conglomerados selecionados\n\n\nO uso de conglomerados é muito comum na prática. Em praticamente todas as entrevistas amostrais do IBGE usa-se amostragem por conglomerados, sorteando setores censitários, assim como o IPEC e a Quaest.\nVia de regra, o conglomerado precisa ser viável e tornar factível o desenho amostral. De saída, não deve ser a primeira opção. Mas, se a pesquisa parecer inviável, é uma boa opção.\nDo ponto de vista de estatística amostral, a conglomeração é o contrário da estratificação. No conglomerado, fazemos, por exemplo, \\(5\\) entrevistas em cada quarteirão. Mas fazer \\(5\\) entrevistas em um determinado prédio, por exemplo, pode dar respostas que, na média, são iguais. Conglomeração nos faz, no fim das contas, perder informação, porque eu acabo fazendo \\(5\\) entrevistas que equivalem a \\(1\\), talvez um pouco mais. Nesse caso, a margem de erro é maior, porque eu tenho mais incerteza sobre a minha coleta – fatalmente eu vou terminar com uma amostra mais extrema, porque eu vou capturar muito da mesma coisa. Quanto mais clusters eu visito, mais informação eu consigo obter2.\n\n\n4.2.6 Estratificação vs. conglomeração\n\nNa estratificação, há redução de variância se os estratos forem homogêneos\nNa conglomeração, a variância aumenta se os conglomerados forem homogêneos\n\n\n\n4.2.7 Comparando desenhos\nUma forma de entender os diferentes impactos na variância de desenhos amostrais alternativos é o design effect, que é uma comparação implícita entre a variância de um desenho amostral e a variância de uma AAS:\n\\[\n\\text{Deff} = \\dfrac{\\text{Var}(\\bar{y})}{\\text{Var}(\\bar{y}_\\text{AAS})}\n\\]\n\n\n4.2.8 AAS e viés\nEmbora AAS seja geralmente confiável contra viés, há algumas situações em que o viés pode ocorrer. A mais comum delas é o problema no quadro amostral: se ele é bom, não há viés; se é ruim, há viés.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Amostragem I</span>"
    ]
  },
  {
    "objectID": "aulas/aula-4.html#footnotes",
    "href": "aulas/aula-4.html#footnotes",
    "title": "4  Amostragem I",
    "section": "",
    "text": "O efeito do denominador, aqui, é remover duplicatas de arranjos de \\(n\\) elementos. Por exemplo, se \\(N = 3\\) e \\(n = 2\\), temos as seguintes combinações possíveis: \\((1,2)\\), \\((1,3)\\) e \\((2,3)\\). Se tivéssemos arranjos, teríamos \\((1,2)\\), \\((1,3)\\), \\((2,1)\\), \\((2,3)\\), \\((3,1)\\), \\((3,2)\\). No caso da amostragem, não importa se a amostra é \\((1,2)\\) ou \\((2,1)\\), por exemplo. Assim, o denominador remove as duplicatas. No fundo, o denominador mostra quantas vezes cada arranjo é contado. Assim, se o denominador é \\(2\\), significa que cada arranjo é contado duas vezes. Também chamamos isso de coeficiente binomial.↩︎\nSe eu faço uma única entrevista por conglomerado, há aqui um ponto de contato com a amostragem simples.↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Amostragem I</span>"
    ]
  },
  {
    "objectID": "aulas/aula-5.html",
    "href": "aulas/aula-5.html",
    "title": "5  Amostragem II",
    "section": "",
    "text": "5.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostragem II</span>"
    ]
  },
  {
    "objectID": "aulas/aula-5.html#anotações-das-leituras",
    "href": "aulas/aula-5.html#anotações-das-leituras",
    "title": "5  Amostragem II",
    "section": "",
    "text": "5.1.1 Wolf, C., Joye, D., Smith, T. W., & Fu, Y. (2016). The SAGE handbook of survey methodology. Sage. Cap. 22, Non-probability Sampling.\n\n[…] the sampling theory as basically developed for probability sampling, where all units in the population have known and positive probabilities of inclusion. The definition implicitly involves randomization, which is a process resembling lottery drawing, where all units are selected according to their inclusion probabilities. (p. 329)\n\n\n[In non-probability samples] This usually means that units are included with unknown probabilities, or, that some of these probabilities are known to be zero. (p. 329)\n\nTambém podemos enfrentar problemas de ordem “não probabilística” quando utilizamos amostras probabilísticas. Por exemplo, quando a amostra não é representativa da população alvo, mesmo que tenhamos utilizado um método probabilístico. Isso pode ocorrer devido a problemas de cobertura, não resposta ou viés de seleção.\n\nWe should underline that wrong results based on non-probability samples – including the above three examples – are typically linked to situations where the standard statistical inference assuming probability sampling was used with non-probability samples. (p. 331)\n\n\nVarious inferential approaches exist. We referred here only to the most popular frequentist one, which (a) takes into account the sampling design, (b) assumes that unknown population values are fixed and (c) builds on a sampling distribution of the estimates across all possible samples. (p. 332)\n\n\nThe approximate usages of probability sampling in a non-probability setting is sometimes understood in a sense that we first introduce certain modelling assumptions e.g. we assume that there is actually some randomization in the non-probability sample. (p. 332)\n\nMedidas indiretas para aproximar designs probabilísticos:\n\n“Spread the non-probability sample as broadly as possible”: tentar recrutar respondentes a partir de vários canais, para aumentar a diversidade da amostra. Bom exemplo: WageIndicator survey.\nAmostragem por cotas, controlando para características socio-demográficas\n\nTambém podemos fazer aproximações adotando diretamente algumas medidas de amostragem probabilística – por exemplo, incluir algum grau de aleatoriedade na seleção de respondentes (randomização de horários de coleta de resposta; randomização de qual pessoa será entrevistada, etc).\n\nOf course, none of the above-described approaches, direct or indirect, assures with some known accuracy that the corresponding statistical inference will be equivalent to the situation with probability samples. However, it is also true that these approximations usually contribute to certain improvements, while they rarely cause any serious damage. (p. 333)\n\n\nAs already mentioned, with non-probability samples, by definition, the inclusion probabilities are unknown or zero, so without further assumptions this very fact formally prevents any statistical inference calculations (e.g. estimates, variances, confidence intervals, hypothesis testing, etc.). (p. 334)\n\n\n[…] if we apply for a certain non-probability sample a standard procedure to calculate confidence intervals, we implicitly assume that probability sample selection (e.g. SRS) was used. However, at this time we are no longer confident that the interval actually contains the population value with the pre-assumed, say, 5% risk. It is very likely, that the actual risk is way above 5%, but we have no possibility to calculate it from the sample. […]. Applying standard statistical inference approaches as an approximation in non-probability samples can thus ‘seduce’ the users into believing that they have ‘real’ confidence intervals. (p. 334)\n\n\nBeing formal and strict, we should acknowledge that randomization – as well as the related probability sampling – is not a necessary precondition for valid statistical inference (Valliant et al., 2000:19). If we have clear and valid assumptions, a specific modelling approach can also provide corresponding statistical inference. In such a situation, we first assume that the data – or at least specific relations among the variables – are generated according to some statistical model. Next, additional external data are then particularly valuable (e.g. sociodemographic backrground variables in the case of online panels), so that certain model-based approaches to statistical inference can be used to build the model and then estimate the values of interest. (p. 335)\n\n\n[Sobre a definição de panels in the context of non-probability online panels] […] panels are large databases of units, which initially agreed to cooperate, provided background information and were recruited to serve for occasional or regular selection into specific sample surveys addressing related or unrelated topics. (p. 336)\n\n[on weighting] In probability samples so-called base weights are used first to compensate for the inclusion probabilities. In a second step, specific non-response weights are applied to reduce the bias – a difference between the estimate and the true value – resulting from non-response. In a similar way, specific weights can be developed for other methodological problems (e.g. non-coverage). In addition, with so-colled population weighting we usually correct for any discrepancies in auxiliary variables. Typically, these are socio-demographic variables (e.g. age, gender, region), but when avaliable, other variables are also considered (e.g. media consumption).1\n\nWe should be aware that weights may (or may not) remove the biases in target variables, but they for sure increase the sampling variance. However, the underlying expectation is of course, that the gains in reducing the bias outweigh the corresponding loss due to increased variance. This is not necessarily true, particularly in the case of weak correlations between auxiliary and target variables, where weights may have no impect on bias removal. (p. 339)\n\n\nWith non-probability samples we often have only one step, because we cannot separate sampling and non-responde mechanisms. The weighting process thus simply assigns weights to units in the sample, so that the underrepresented ones get a weight larger than \\(1\\), while the opposite happens to overrepresented ones, which is similar as in population weighing. (p. 339)\n\n\nWhen we implement approximations from probability samples into a non-probability setting, there is by the very definition little theoretical basis for running a sound statistical inference. Therefore, the justification and validation of these procedures rely solely on the accumulation of anecdotal evidence, which is sometimes ironically called ‘faith-based’ sampling. (p. 340)\n\n\nWe continuously indicated in this chapter – explicitly or implicitly – that in a non-probability setting the statistical inference based on probability sampling principes formally cannot be applied. Still, the practitioners routinely use the corresponding ‘estimates’, which should be rather labelled as ‘indications’ or even ‘approximations’. (p. 341)\n\n\nBy abandoning probability sampling principes we usually also abandon the science of statistical inference and enter instead into the art and craft of shaping optimal practical procedures. The experience based on trials and errors in thus essential here, as well as the intuition of the researcher. (p. 342)\n\n\nWe thus recommend to more openly accepting the reality of using a standard statistical inference approach as an approximation in non-probability settings. However, this comes with two warnings: first, the sample selection procedure should be clearly described, documented, presented and critically evaluated. We thus join the AAPOR recommendations that the methods used to draw a sample, to collect the data, to adjust it and to make inferences should be even more detailed compared to probability samples (Baker et al, 2013). […]. Second, we should also elaborate on the underlying assumptions (e.g. models used) and provide explanations about conceptual divergences, dangers, risks and limitations of the interpretation (AAPOR, 2015). When standard statistical inference is applied to any non-probability sample, the minimum should be thus to clearly acknowledge that estimates, confidence intervals, model fitting and hypothesis testing may not work properly or may not work at all. (p. 342)\n\n\n\n5.1.2 Baker, R., Brick, J. M., Bates, N. A., Battaglia, M., Couper, M. P., Dever, J. A., Gile, K. J., & Tourangeau, R. (2013). Summary report of the AAPOR task force on non-probability sampling. Journal of survey statistics and methodology, 1(2), 90–143.\n\nBut for at least the past 60 years, the probability-sampling framework has been used in most surveys. More recently, concerns about coverage and nonresponse coupled with rising costs have led some to wonder whether non-probability sampling methods might be an acceptable alternative, at least under some conditions (Groves 2006; Savage and Burrows 2007). (p. 90)\n\n\nWhat we have done is examine the strengths and weaknesses of various non-probability methods, considering the theoretical and (to some extent) empirical evidence. We do not claim to have produced an exhaustive study of all possible methods or fully examined all of the literatura on any one of them. However, we believe that we have at least identified the most prominent methods and examined them in a balance and objective way. (p. 91)\n\n\nHistorically, the main arguments advanced against probability-based samples have been those of cost and time efficiency. This was an easy argument to make when the most common survey mode was face-to-face interviewing. The emergence of random digit dial (RDD) telephone surveys heralded a broad expansion of probability sampling in political polling, market research, and academia (Glasser and Metzger 1972). Recently, the rapid rise in cell phone-only households raised concerns about coverage bias (Lavrakas et al. 2007) and the long-term decline in response rates raised questions about nonresponse bias (Curtin, Presser, and Singer 2005). (p. 93)\n\n\nThe challenge for non-probability methods is to identify uncontrolled covariates – what Kish (1987) called “disturbing variables” – that are related to the measures of interest and bring them under control in sample selection, estimation, or both. Probability sampling mitigates the effects for unbalanced covariates through random selection. Non-probability methods have no such advantage. The selection bias inherent in most non-probability methods creates the substantial risk that the distribution of the important covariates in the sample will differ significantly from their distribution in the target population to such an extent that inferences could be misleading if not simply wrong. To be of value non-probability samples must rely on some form of statistical adjustment to manage that risk. (p. 94)\n\nExemplos de estratégias:\n\nSample matching: the sample is matched to a known population distribution on a set of covariates. (p. 95)\nNetwork sampling: “if a small number of eligible sample members can be identified, and sufficient trust stablished, these first respondents can connect researchers to their social contacts, who can, in turn, connect researchers to others, and so on until the desired sample size is achieved.” (p. 95-96)\nEstimation and weight adjustment methods: “[…] model-based estimation, relies on a statistical model that describes the variable being estimated in the survey (Valliant, Dorfman, and Royall 2000; Pfeffermann and Rao 2009). These models treat the outcomes rather than the sampling process as being the random variables.” (p. 97). Approaches include, propensity score adjustiment, weight calibration, and raking. (p. 98)\n\nRemarks:\n\n“Unlike probability sampling, there is no single framework that adequately encompasses all of non-probability sampling” — “Thus, non-probability sampling is a collection of methods rather than a single method, and it is difficult it not impossible to ascribe properties that apply to all non-probability sampling methodologies.” (p. 100)\n“Researchers and other data users may find it useful to think of the different non-probability sample approaches as falling on a continuum of expected accuracy of the estimates” (p. 100)\n“Transparency is essential” (p. 100)\n“Making inferences for any probability or non-probability survey requires some reliance on modeling assumptions” (p. 101)\n“The most promising non-probability methods for surveys are those that are based on models that attempt to deal with challenges to inference in both the sampling and estimation stages”. (p. 101)\n“One of the reasons model-based methods are not used more frequently in surveys may be that developing the appropriate models and testing their assumptions is difficult and time-consuming, requiring significant statistical expertise.” (p. 101)\n\n\n\n5.1.3 Jerit, J., & Barabas, J. (2023). Are Nonprobability Surveys Fit for Purpose? Public Opinion Quarterly, 87 (3), 816–840.\n\nIt may not be apparent why the growing reliance on NPSs is problematic. After all, their dramatically lower cost (compared to probability samples) makes it possible for more people to collect survey data. But NPSs from commercial vendors have some distinctive features relative to other low-cost survey data. Chief among them is the provenance of the data (Krupnikov, Nam, and Style 2021). When one contracts with a commercial survey firm, the vendor cultivates and manages the sample, often through methods that are not transparent to the client. While there is variation in how specific survey organizations operate, a key commonality is that the data are culmination of a multi-step – and largely invisible – selection process. (p. 817)\n\n\nSome of the early concerns with NPSs had to do with the ‘professionalism’ of the people completing surveys. […]. In an extensive examination of online sources of polling data, researchers at Pew found that a small, but measurable, percentage of participants (i.e., 4 percent to 7 percent) should be classified as “bogus respondents” [people who tend to give affirmative responses to survey questions without reading them]. (p. 818)\n\n\nSet against research on the generalizability of treatment effects is a different body of work showing that people who participate in online surveys are systematically different from nonparticipants. […]. Several studies suggest that politically engaged respondents are overrepresented in online panels. (p. 820)\n\n\nAt present, there is no evidence that NPSs display greater fitness than traditional, probability-based methods in the domain of election polling. If anything, the practice of herding implies that organizations using NPSs benefit from the (presumed) greater accuracy of probability samples. (p. 825)\n\nThe authors suggest that using an NPS for testing the wording of a survey question is a reasonable use of the data (p. 826). Another fit for purpose is to use NPSs to collect data on “rare populations” (p. 826).\n\n[On the combination of probability and nonprobability samples] This usage may seem similar to techniques already employed by researchers (e.g., sample matching, propensity score adjustment, weighting), but there is a crucial difference. With existing methods, the researcher adjusts the composition of a NPS in reference to a probability sample or population figure but uses only the NPS in the analysis. This strategy is problematic because: (1) the researcher must assume the matching/adjustment variables fully explain the selection mechanism that leads to inclusing in the NPS; and (2) there is no formal way to measure the uncertainty (sampling error) of the resulting estimates.\nIn response to these challenges, researchers have developed estimation techniques that use Bayesian inference to combine a NPS and a smaller size probability sample. (p. 827) (see Sakshaug et al. 2019).",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostragem II</span>"
    ]
  },
  {
    "objectID": "aulas/aula-5.html#anotações-de-aula",
    "href": "aulas/aula-5.html#anotações-de-aula",
    "title": "5  Amostragem II",
    "section": "5.2 Anotações de aula",
    "text": "5.2 Anotações de aula\nContexto: diferenças entre as estimativas oferecidas pelo Ibope e pelo Gallup na ocasião da eleição de Lula e Afif em 1989. Na época, falava-se de pesquisa como uma ferramenta de “previsão”, e não um retrato do momento. Embora as diferenças estivessem relativamente dentro da margem de erro, isso chama atenção, e a capa do Globo trazia “Pesquisas têm metodologia diferente”. Ibope surge filiado ao Gallup e, na época, faziam pesquisa de cotas. Mas quando veio pro Brasil, Gallup passa a dizer que fazia pesquisa probabilística.\n\n5.2.1 Alocação\nNão há um abismo que separa desenhos probabilísticos e não probabilísticos. De saída, precisamos pensar num continuum entre desenhos mais ou menos probabilísticos. De fato, existem desenhos que são probabilísticos “puro sangue”, mas, quando pensamos do ponto de vista de implementação, não é tão simples assim.\nAlocação proporcional: dentro de cada estrato, a probabilidade de inclusão na amostra, \\(\\pi_i\\), é igual para todos os elementos da população. O tamanho de \\(n_h\\) é proporcional ao tamanho do estrato \\(N_h\\). O número de entrevistas em cada estrato é determinado pela proporção do estrato na população:\n\\[\nf_h = \\frac{n_h}{N_h} \\Rightarrow n_h = n \\times \\frac{N_h}{N}\n\\]\nEsse \\(f\\) diz o peso relativo daquele estrato em relação a todos os estratos que eu defini. Se a população das pessoas da Zona Sul é de \\(5%\\), então \\(f_h = 0.05\\).\nEsse é um desenho amplamente utilizado porque ele reduz variância. Isso é lógico, já que estamos evitando fazer muitas entrevistas (isto é, deslocar a nossa amostra) em uma única região (ou outro elemento usado para fazer a estratificação) de maneira aleatória. Intuitivamente, cada estrato é representado na amostra exatamente em proporção ao seu tamanho na população.\nO número de variáveis de estrato é, no mais das vezes, três. Afinal, se você estratifica demais, você pode gerar muitos grupos e, se há algum erro de mensuração dentro desses grupos, podemos enviesar a amostra.\nExemplo\n\nEstrato A com 38 pessoas\nEstrato B com 62 pessoas\n\\(N = 100\\), \\(n = 20\\)\n\nAlocação para o estrato A:\n\\[\n\\begin{align*}\nn_A &= n \\times \\frac{N_h}{N} \\\\\n    &= 20 \\times \\frac{38}{100} \\\\\n    &= 7.6 \\approx 8\n\\end{align*}\n\\]\nAlocação para o estrato B:\n\\[\n\\begin{align*}\nn_B &= n \\times \\frac{N_h}{N} \\\\\n    &= 20 \\times \\frac{62}{100} \\\\\n    &= 12.4 \\approx 12\n\\end{align*}\n\\]\nAo final, as frações amostrais foram:\n\\[\n\\begin{align*}\nf_A = \\frac{n_A}{N_A} = \\frac{8}{38} \\approx 0.21 \\\\\nf_B = \\frac{n_B}{N_B} = \\frac{12}{62} \\approx 0.19\n\\end{align*}\n\\]\nO que é aproximadamente \\(f = \\frac{n}{N} = \\frac{20}{100} = 0.2\\).\nImportante: não estamos violando o princípio de equiprobabilidade. A amostra ainda é aleatória e as pessoas têm a mesma chance de serem escolhidas. O que estamos fazendo é garantir que a amostra seja representativa em relação a um determinado estrato da população.\nEstratificação ainda tem um outro efeito: Temos amostras independentes dentro de cada estrato. Mas, se temos muitos estratos, o \\(n\\) em cada estrato acaba sendo pequeno e, na prática, temos margem de erro muito grandes para analisar determinados segmentos. O que podemos fazer é um desenho estratificado sobreproporcionalizado, alocando mais do que o esperado para um determinado estrato, permitindo leituras dentro dos estratos. Na academia isso normalmente não se faz, mas governo e terceiro setor fazem isso com frequência.\n\nPense numa situação em que o cliente quer uma leitura nacional, mas também uma leitura mais focada no Amazonas. Nesse caso, podemos ter um segundo desenho amostral aumentando o número de entrevistas naquele estado. Para a leitura nacional, juntamos as amostras provenientes de cada desenho – mas, claro, com a necessidade de ponderar os dados para evitar que o Amazonas “apareça demais”.\n\nA estimação deve levar em conta a fração amostral de cada estado, i.e., uma média ponderada das médias dos estratos:\n\\[\n\\bar{y}_{st} = \\sum^H_{h=1} W_h \\times \\bar{y}_h,\n\\]\nonde \\(W_h\\) é o peso do estrato \\(h\\) e \\(\\bar{y}_h\\) é a média do estrato \\(h\\). Estratos são amostras independentes, justapostas, então eu calculo a média dentro de cada uma dessas amostras e combino elas.\n\n\n5.2.2 Múltiplos estágios\nImagine o cenário em que temos um município com 1000 habitantes distribuídos em 8 bairros. Sorteamos 4 bairros (clusters) para, em cada um deles, entrevistar 10 pessoas. Supondo que os bairros têm 125 pessoas cada, um desenho por conglomerados poderia ser:\n\\[\n\\begin{align*}\nc &= 4 \\\\\nf_c &= \\frac{4}{8} = 0.5 \\\\\nf_i &= \\frac{10}{125} = 0.08 \\\\\nf &= 0.5 \\times 0.08 = 0.04 \\\\\nn &= 0.04 \\times 1000 = 40\n\\end{align*}\n\\]\nNa prática, o sorteio é realizado em algumas etapas: primeiro, sorteados os clusters; depois, dentro de cada cluster, sorteadas as pessoas. Isso é o que chamamos de amostragem em múltiplos estágios. Assim como eu tenho a probabilidade de incluir uma pessoa na amostra, eu tenho também a probabilidade de incluir um conglomerado na amostra.\n“Qual é a probabilidade de eu incluir o bairro A na amostra?” Essa probabilidade é dada pela fração amostral do conglomerado, \\(f_c\\). E, bom, se 50% dos conglomerados foram sorteados, a probabilidade de incluir o bairro A na amostra é de 50%. E, dado que a pesquisa chegou no meu bairro, qual é a probabilidade de eu ser selecionado? Nesse caso, a probabilidade de eu ser selecionado é de 8%. Então, a probabilidade de eu ser sorteado é de 50% vezes 8%, ou seja, 4%.\nNo entanto, note que se o número de pessoas em cada bairro for diferente, essa matemática quebra. A probabilidade de inclusão de cada pessoa na amostra já não é a mesma: se eu fizer o mesmo número de entrevistas em qualquer bairro que eu sortear, em alguns bairros a probabilidade de incluir a pessoa na amostra será maior ou menor do que em outros. Portanto, temos distorções no resultado – um viés sistemático na estimação.\n\n\n5.2.3 PPT\nUma maneira de resolver isso é incorporar o tamanho do bairro na amostra (em termos de número de pessoas) na hora de calcular a fração amostral. Isso é o que chamamos de Probabilidade Propocional ao Tamanho (PPT, ou PPS, em inglês). Damos maior probabilidade aos conglomerados maiores serem incluídos na amostra – mas, a condição sine qua non é que eu saiba o tamanho da população dentro de cada conglomerado. Incorporamos probabilidades desiguais de inclusão para tornar ele “justo”, proporcional.\n\n8 bairros (conglomerados)\n\nbairros 1-4: 50 pessoas cada\nbairros 5-8: 200 pessoas cada\n\nPopulação total: \\(N = 1000\\)\n\nAlocação para o bairro 1:\n\\[\n\\mathbb{P}(\\text{bairro_1}) = \\frac{50}{1000} = 0.05\n\\]\nE a probabilidade de selecionar uma pessoa dele é:\n\\[\n\\mathbb{P}(\\text{pessoa} | \\text{bairro_1}) = \\frac{1}{50} = 0.02\n\\]\nAgora pense no bairro 5:\n\\[\n\\mathbb{P}(\\text{bairro_5}) = \\frac{200}{1000} = 0.2\n\\]\nE a probabilidade de selecionar uma pessoa dele é:\n\\[\n\\mathbb{P}(\\text{pessoa} | \\text{bairro_5}) = \\frac{1}{200} = 0.005\n\\]\nAgora, note uma propriedade interessante: ao fazemos isso, estamos reintroduzindo a equiprobabilidade de seleção. Note que as probabilidades, se multiplicadas em cada caso, dão a mesma probabilidade de seleção das pessoas – \\(0.001\\). Ou seja, a probabilidade de selecionar uma pessoa do bairro 1 é de 0.05 vezes 0.02, e a probabilidade de selecionar uma pessoa do bairro 5 é de 0.2 vezes 0.005. Isso significa que, mesmo com tamanhos diferentes, as probabilidades de seleção são as mesmas. Isso é o que chamamos de equiprobabilidade.\n\nCalculamos a probabilidade de incluir um determinado bairro\nCalculamos a probabilidade de incluir uma determinada pessoa dentro do bairro\nMultiplicamos as duas probabilidades para obter a probabilidade de incluir uma pessoa do bairro \\(n\\) na amostra.\n\n\n\n5.2.4 Pesquisa não-probabilística\nNesses casos, não sabemos a probabilidade de uma pessoa ser sorteada (i.e., ser incluída na amostra). Isso pode ocorrer por diversos motivos:\n\nAmostragem por quotas\nAmostragem por conveniência\nAmostragem opt-in\nAmostragem sistemática sem \\(N\\) conhecido\n\n\nBoas variáveis para usar como estratos são aquelas que são relativamente estáveis. Região, por exemplo, é uma boa.\n\nApesar de vários desenhos na prática serem não-probabilísticos, é possível usar a amostragem probabilística como referência e falar em graus de aproximação com amostragem probabilística.\n\nPesquisas eleitorais domiciliares tipicamente implementam area sampling, determinando probabilisticamente conglomerados a serem visitados (probability sampling with quotas)\nSeleção por meio de métodos não-intencionais (e.g., seleção sistemática de domicílios, ponto de fluxo, horário de abordagem, etc.)\n\nComo cotas funcionam? Eu não sei exatamenta a distribuição de uma certa variável na população, mas existe alguma pesquisa confiável da qual eu posso me basear. Eu quero acabar com uma distribuição de sexo que siga a mesma proporção de uma determinada pesquisa. As mais comuns são sexo, idade e (um pouco mais nebuloso) idade.\n\nAlgumas interações (renda vs. raça, e.g.) podem ter de ser levadas em conta por meio de distribuições conjuntas (ou quotas cruzadas).\n\nA alocação de entrevistas é feita da mesma maneira que os estratos.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostragem II</span>"
    ]
  },
  {
    "objectID": "aulas/aula-5.html#footnotes",
    "href": "aulas/aula-5.html#footnotes",
    "title": "5  Amostragem II",
    "section": "",
    "text": "Sugestões de leitura complementar para aprofundar o tema de weighting: Valliant et al. (2013), Bethlehem (2009) e Bethlehem and Biffignandi (2012).↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Amostragem II</span>"
    ]
  },
  {
    "objectID": "aulas/aula-6.html",
    "href": "aulas/aula-6.html",
    "title": "6  Desenho de questionários",
    "section": "",
    "text": "6.1 Krosnick, J. A. (2018). Questionnaire Design. Em D. L. Vannette & J. A. Krosnick (Orgs.), The Palgrave Handbook of Survey Research (p. 439–455). Springer International Publishing.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Desenho de questionários</span>"
    ]
  },
  {
    "objectID": "aulas/aula-6.html#krosnick-j.-a.-2018.-questionnaire-design.-em-d.-l.-vannette-j.-a.-krosnick-orgs.-the-palgrave-handbook-of-survey-research-p.-439455.-springer-international-publishing.",
    "href": "aulas/aula-6.html#krosnick-j.-a.-2018.-questionnaire-design.-em-d.-l.-vannette-j.-a.-krosnick-orgs.-the-palgrave-handbook-of-survey-research-p.-439455.-springer-international-publishing.",
    "title": "6  Desenho de questionários",
    "section": "",
    "text": "I have come to the view that question wording is where the crises lies at the moment, that’s one area where we severely need more work. (p. 440)\n\n\nFirst of all, it is true that in experimental comparisons, open-ended questions take, on average, about twice as long to answer as closed questions and respondents prefer closed questions. On practical grounds, it might appear that closed questions are preferable. In studies of reliability, open questions prove to be more reliable than closed questions and in lots of different studies of validity, open questions prove to be superior to close questions across the board using these various different methods of assessing validity. (p. 443)\n\n\nThe conclusion that I take this literature to suggest is that we should ask open-ended questions when you can’t be sure of the universe of possible answer to a categorical question and the other specify option does not work. The only way to be sure that we know the universe is to pre-test, ask open questions from the population we care about, build the big list, offer it to people and we could do that, but it’s so much work we might as well as the open-ended question in the real survey. And lastly, if we’re looking for a number we should just ask for the number. That is what I take that literature to suggest and I don’t know if that’s really widely recognized and followed. (p. 445)\n\n\nThe evidence in terms of response speed as rating scales get longer, from 2 to 11 points, for example, indicates that the speed goes up. Although, it’s sort of interesting that five-point scales take significantly less time than their neighbors do, that’s a hint that it is preferable to have mid-points. This is even more direct evidence, when people are asked how difficult it is to use the scale, you can see that three points, seven points, and nine points, these are significantly less difficult than the scales of other lengths. So these are reinforcing that notion that in this case seven points is the optimal way to measure bipolar scales and that not only is it less difficult for people but that it produces more valid and reliable results. (p. 447)\n\n\nThe last thing in this arena that we have found is that it’s helpful to branch bipolar dimensions. This is one of the first branching questions that I paid attention to from the ANES: “generally speaking do you consider yourself to be a Republican, a Democrat, an independent or what?” and Republicans and Democrats are asked if they’re strong or not very strong. The independents are asked if they lean one way or another. You can produce seven-point scale like this. If you do it in those two steps up here it goes more quickly and it produces more valid and reliable results rather than presenting all seven of these, which people have to slog through and place themselves on. (p. 447)\n\n\nIf the mid-point is precise, it turns out that asking people, do you lean one way or another actually adds noise. In order to produce the seven-point scale, the people who place themselves at a precise mid-point like this belong there. We shouldn’t branch the mid-point, instead we should branch the end points into three categories. So the people who say increase should be asked, do you want it increased a little, a moderate amount, or a great deal? (p. 447)\n\n\nNow, with regard to dimensions that have a natural metric, so for example I could ask you how often do you go to the movies, very often, often, sometimes, rarely or never? I’m not going to take the time to go through all of the evidence on this so forgive me for just skipping this. What I can tell you is what this literature says is using those kinds of what you might think of as vague quantifiers actually cause many more problems than they solve. If what you want is a number just ask for the number. Other than saying, how often do you go to the movies, you ask people in the last month how many times did you go to the movies; you can get around that problem. (p. 450)\n\n\nFirst of all, use simple, direct, and comprehensible words. Don’t use jargon, be specific in your question, avoid ambiguous words, avoid double barreled questions that ask two things at the same time, and avoid negations if you can avoid the word not. Avoid leading questions, and include filter questions. The common-sense version of using filter questions is don’t ask people what brand of car they have if they might not have a car. Be sure that questions read smoothly aloud, avoid emotionally charged words, avoid prestige names. When you look at textbooks and research design, it’s almost like the later ones copied the earlier ones because they are remarkably consistent in this kind of advice. (p. 450)\n\n\nFirst, lots and lots of studies are done that help us to inform the issues of questionnaire design and yet there is much more work to be done, especially with regard to language. I think we understand a lot about structure, but we understand much less about language. There is also an issue of dissemination. NSF of course has commitment not only to making discoveries but also to disseminating those discoveries through educational efforts and outreach. I think there is a real opportunity here because there are lots of people who don’t know it’s a bad idea to offer a ‘don’t know’ response option. There are lots of people who don’t know the order of answer choices and close questions matter and how to handle that. There are lots and lots of people who think it’s fine to ask agree/disagree questions, so a major educational outreach effort to disseminate the findings of this literature would help. (p. 455)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Desenho de questionários</span>"
    ]
  },
  {
    "objectID": "aulas/aula-6.html#wolf-c.-joye-d.-smith-t.-w.-fu-y.-2016.-the-sage-handbook-of-survey-methodology.-sage.-cap.-16-designing-questions-and-questionnaires-by-jolene-d.-smyth",
    "href": "aulas/aula-6.html#wolf-c.-joye-d.-smith-t.-w.-fu-y.-2016.-the-sage-handbook-of-survey-methodology.-sage.-cap.-16-designing-questions-and-questionnaires-by-jolene-d.-smyth",
    "title": "6  Desenho de questionários",
    "section": "6.2 Wolf, C., Joye, D., Smith, T. W., & Fu, Y. (2016). The SAGE handbook of survey methodology. Sage. Cap. 16: Designing Questions and Questionnaires, by Jolene D. Smyth",
    "text": "6.2 Wolf, C., Joye, D., Smith, T. W., & Fu, Y. (2016). The SAGE handbook of survey methodology. Sage. Cap. 16: Designing Questions and Questionnaires, by Jolene D. Smyth\n\nThis is why questionnaire design texts suggest avoiding starting with boring, embarrassing, or sensitive questions and instead make the first question simple, interesting, and applicable to all sample members with a professional design and visual layout that makes the questionnaire look simple to complete. (p. 219)\n\n\nAn analysis plan can also help one refine their initial research question(s) and list of constructs to measure. For example, while age, race, and gender are not explicitly identified in the aforementioned research question, the researcher would likely want to measure them because they may be related to both engagement in risky behaviors and health outcomes,making them necessary for subgroup analyses or control variables in a regression, both of which should be expressed in the final research questions but are sometimes initially overlooked. These and other constructs may also need to be measured for weighting and adjustment purposes, which should also be planned carefully upfront in the context of the research question(s). (p. 220)\n\n\nQuestions should be written to accommodate the retrieval process, which means accounting for how people recall information. People are better able to recall events that happened recently, are distinctive (e.g., buying a new car versus buying groceries), or that are important (e.g., a wedding celebration versus a birthday celebration) (Tourangeau et al., 2000). People can also more accurately recall events when the recall period is shorter (e.g., the last week versus last year). (p. 222)\n\n\nThus, in writing questions, it is important to determine whether exact enumeration of events is needed or whether an estimate is good enough. With very frequent and mundane events, estimation might just have to be good enough. However, for events respondents are expected to be able to recall and enumerate, a shorter recall period will help. Slowing down the survey to give more time to think can also improve retrieval, and can be done by asking longer (not more complex) questions (Bradburn and Sudman, 1979; Tourangeau et al., 2000). (p. 222)\n\n\nIn this case, respondent motivation to provide quality open-ended responses can be increased by emphasizing the importance of the question. Statements like, ‘This question is very important to this research’ can increase the length and quality of open-ended responses (Oudejans and Christian, 2011; Smyth et al., 2009). (p. 223)\n\n\nUsing construct-specific rather than vague quantifier labels can also make reporting more straightforward and reduce measurement error (e.g., asking ‘How would you rate the quality of your new car?’ instead of ‘How much do you agree or disagree that your new car is high quality?’) (Saris et al., 2010). (p. 223)\n\n\nReliability and validity seem to be maximized at 5 to 7 scale points for bipolar scales (i.e., scales that measure both direction and magnitude like very satisfied/ very dissatisfied) and 4 to 5 scale points for unipolar scales (scales that measure only magnitude, like never to always) (Krosnick and Fabrigar, 1997). In addition, ensuring that the scale points are conceptually equidistant can also help. (p. 223)\n\n\nQuestionnaire designers also have to make sure respondents are willing to provide accurate answers at the reporting stage. Some respondents may be hesitant because of social or normative concerns. Social desirability is a tendency to answer questions in a way that makes one look good (or not look bad) rather than providing the most accurate answer (Tourangeau and Yan, 2007). It can take the form of underreporting negative behaviors like illicit drug use, abortion, and poor college performance or overreporting positive behaviors like voting and church attendance (Bernstein et al., 2001; Hadaway et al., 1993; Kreuter et al., 2008; Tourangeau and Smith, 1996). Respondents are more susceptible to social desirability in interviewer-administered surveys (Aquilino, 1994; de Leeuw, 1992; Dillman et al., 1996), but it is a concern in all modes. (p. 224)\n\n\nAside from increasing response privacy by asking sensitive questions in a private, self- administered mode, question design strategies to combat social desirability sometimes focus on making it either more acceptable or safe to answer honestly. Question wording is commonly changed to make the question less threatening such as asking ‘Have you happened to …’, ‘Some people believe  and others believe ’, ‘Do you believe  or ?’, and ‘There are many reasons people might not do  such as not having time, not having transportation, or being sick. How about you, did you do ?’ (Bradburn et al., 2004). While these strategies make intuitive sense, the few empirical evaluations that have been done suggest they are not more effective than direct inquiries (Bradburn et al., 2004; Schuman and Presser, 1981; Yeager and Krosnick, 2012). (p. 224)\n\n\nFor example, an important word may be made more visible by increasing its font size or bolding or italicizing it. Likewise, nonessential information (e.g., office use only, data entry codes, etc.) can be deemphasized by making it smaller and lighter in color and placing it in areas respondents are less likely to look. Applying properties allows designers to impact how respondents understand the relationship between elements on a page. (p. 226)\n\n\nThere are several ways question ordering can help with retrieval. For example, grouping topically similar questions gains efficiencies because respondents can use retrieved information to answer all questions on a topic before moving to a different topic (Dillman et al., 2014). However, one also has to be watchful for unintentional priming effects, which occur when information retrieved for an early question is used to answer a later question simply because it is more easily accessible. For example, Todorov (2000) found that asking respondents about specific chronic conditions early in the National Health Interview Survey on Disability increased the likelihood that they would identify one of the chronic conditions asked about as a cause of their disability in a later question. (p. 229)\n\n\nOrienting oneself to how respondents will experience the questionnaire and their response process can provide a useful framework for making these design decisions. This includes keeping in mind the goals of encouraging response, promoting optimizing, providing a clear navigational path, and, of course, collecting high quality measurements. It also requires one to think about how their design will impact each stage of the response process. In addition, understanding the response process and where it can break down can help one determine what pretesting method(s) are most appropriate for a given questionnaire and troubleshoot problems. (p. 231)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Desenho de questionários</span>"
    ]
  },
  {
    "objectID": "aulas/aula-6.html#anotações-de-aula",
    "href": "aulas/aula-6.html#anotações-de-aula",
    "title": "6  Desenho de questionários",
    "section": "6.3 Anotações de aula",
    "text": "6.3 Anotações de aula\nTeoria do respondente guarda muita semelhança com o que vimos do Zaller, mas não exatamente. Zaller foca na recepção e no fato de que as pessoas exercem um filtro na resposta, exercendo algum grau de amostragem mental para responder.\nA teoria do respondente assume que as pessoas possuem valores mais ou menos lantentes, mas não valores inatos, mas memórias de longo prazo positivas ou negativas relacionadas a certos aspectos. Tenho uma série de memórias que não me cruzam diariamente, mas que surgem de maneira mais ou menos clara da ideia. Respondentes passam por etapas cognitivas para acessar atitudes da memória de longo-prazo (Touranngeau e Rasinski, 1988).\n\nCompreensão: entender o enunciado da pergunta\nRecuperação: buscar na memória informações necessárias\nJulgamento: avaliar e sintetizar informações\nRelato: mapear julgamento para resposta\n\nNão existe uma teoria unificada de respostas, mas abordagens enfatizam dois tipos possíveis de processos (Krosnick 1991):\n\nOtimizar: quando pessoas respondem cuidadosa e completamente no mínimo de tempo\nSatisficing: quando usam atalhos ou dão respostas sem esforça\n\nBasicamente existiam dois grupos de pessoas respondendo: algumas que respondem “bruscamente”, e outras muito detalhistas. As pessoas que respondiam de forma brusca tendiam, por exemplo, a responder de maneira aquiescente, ou de maneira aleatória, ou lendo apenas as primeiras alternativas… enfim. Quando desenhamos surveys, precisamos minimizar as respostas do tipo “satisficing” e maximizar as respostas ótimas.\nNo caso do satisficing, as pessoas respondem muito rapidamente, sem pensar; aderem às primeiras opções de resposta razoáveis (e.g., Sim, Muito); optar por alternativas modais (0, 5, 10); ignoram perguntas difíceis ou complexas; e respondem “não sei” com frequência, mesmo que não seja uma alternativa. Rapidamente as pessoas ficam cansadas e vão até ao final da pesquisa respondendo “de qualquer maneira”.\nPara Krosnick, a probabilidade de satisficing é dada por:\n\\[\n\\mathbb{P}(\\text{Satisficing}) = \\dfrac{\\text{Dificuldade}}{\\text{Habilidade} \\times \\text{Motivação}}\n\\]\nSolução: desenhar questionários de forma a incentivar otimização, que é oposto de satisficing. Podemos controlar três aspectos em diferentes medidas.\n\nCompreensão, reduzindo a dificuldade do instrumento\nRecuperação, oferecendo cues – ganchos ou frases que ajudam a puxar itens da memória. “Pensando nos últimos 7 dias…”; “Falando agora sobre os militares no Haiti que estão no Brasil…”.\n\n\n6.3.1 Formulação das perguntas\nOs princípios gerais de escrita das perguntas:\n\nSimplicidade\nEspecificidade\nInstruções\nExemplos\n\nEvitar conceitos vagos, negativas (nunca, nunca usar dupla negação), redação complexa e longa. Pré-teste, feito idealmente com uma pessoa de cada segmento, é ideal.\nKrosnick sugere também branching: Você concorda, sim ou não? E dada a resposta, con/discorda muito ou pouco?\n\n\n6.3.2 Texto de recrutamento\nSurveys opt-in naturalmente resolvem parte do problema de motivação: auto-seleção recruta em função de motivação. Para outras abordagens, recrutamento importa:\n\nEvitar abordar pessoas ocupadas\nCriar ambiente amigável\nMensagem de recrutamento persuasiva (tentar motivar a pessoa a participar)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Desenho de questionários</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html",
    "href": "aulas/aula-7.html",
    "title": "7  Inferência com Surveys",
    "section": "",
    "text": "7.1 Groves, R. M., Fowler Jr, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., & Tourangeau, R. (2011). Survey methodology. John Wiley & Sons. Cap. 10.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inferência com Surveys</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#groves-r.-m.-fowler-jr-f.-j.-couper-m.-p.-lepkowski-j.-m.-singer-e.-tourangeau-r.-2011.-survey-methodology.-john-wiley-sons.-cap.-10.",
    "href": "aulas/aula-7.html#groves-r.-m.-fowler-jr-f.-j.-couper-m.-p.-lepkowski-j.-m.-singer-e.-tourangeau-r.-2011.-survey-methodology.-john-wiley-sons.-cap.-10.",
    "title": "7  Inferência com Surveys",
    "section": "",
    "text": "The outcome of the reordering of activities is that computer-assisted surveys must make all the design decisions about data entry protocols and editing during the process of developing the questionnaire. A further implication of the combination of collection, entry, and editing is that the surveys collecting only numeric answers can entirely skip the coding step. (p. 330)\n\n\n\nCoding – the process of turning word answers into numeric answers\nData entry – the process of entering numeric data into files\nEditing – the examination of recorded answers to detect errors and inconsistencies\nInputation – the repair of item-missing data by placing an answers in a data field\nWeighting – the adjustment of computations of survey statistics to counteract harmful effects of noncoverage, nonresponse, or unequal probabilies of selection into the sample\nSampling variance estimation – the computation of estimates of the instability of survey statistics (from any statistical error that are measurable under the design) (p. 330-331)\n\n\n\n7.1.1 Coding\n\nTo be useful, codes must have the following attributes:\n\nA unique number, used later for statistical computing\nA text label, designed to describe all the answers assigned to the category\nTotal exhaustive treatment of answers (all responses should be able to be assigned to a category)\nMutual exclusivity (no single response should be assignable to more than one category)\nA number of unique categories that fit the purpose of the analyst (p. 332)\n\n\n\n\n7.1.2 Editing\n\nThere is some consensus on desirable properties of editing systems. These include the use of explicit rules linked to concepts being measured; the ability to replicate the results; editing shifted to rule-based, computer-assisted routines to save money; minimal distortion of data recorded on the questionnaire; the desirability of coordinating editing and imputation; and the criterion that when all is finished, all records pass all edit checks. The future of editing will not resemble in its past. Editing systems will change as computer assistance moves to earlier points in the survey process and becomes integrated with other steps in the survey. It is likely that editing after data collection will decline. It is likely that software systems will increasingly incorporate the knowledge of subject matter experts. (p. 347)\n\n\n\n7.1.3 Weighting\n\nSurveys with complex sample designs often also have unequal probabilities of selection, variation in response rates across important subgroups, and departures from distributions on key variables that are known from outside sources for the population. It is common within complex sample surveys to generate weights to compensate for each of these features. (p. 348)\n\n\n7.1.3.1 Weighting for Differential Selection Probabilities\n\nTHe growing Latino population, and differences in crime victimization between Latino and non-Latino populations, raise the question whether the number of Latino persons in the NCVS, under the overall sample of \\(125,000\\), is sufficient. Suppose that one person in eight aged 12 years and older is Latino, or a total population of just about \\(25\\) million. If an epsem sample were selection, the sample would have \\(15,625\\) Latinos and \\(109,375\\) non-Latinos. That is, under proportionate allocation, one person in eight in the sample would be Latino as well. The sample results could be combined across Latino and non-Latino populations with no need for weights to compensate for overrepresentation of one group. (p. 349)\n\nSuppose we want to estimate statistics of interest with more precision for Latinos. So we increase the the number of Latinos in the sample to \\(62,500\\). This is totally okay when our goals is to compute statistics for each group separatelly. However, …:\n\nThere is a problem though, when the data need to be combined across the groups. The need for combining across groups might arise because of a call for national estimates that ignore ethnicity, or the need for data on a “crossclass” that has sample persons who are Latino as well as non-Latino, such as women. (p. 349-350)\nWhen combining across these groups to get estimates for the entire population, ignoring ethnicity, something must be done to compensate for the substantial overrepresentation of Latinos in the sample. Weights in a weighted analysis applied to individual values are one way to accomplish this adjustment. Recall that the weighted mean can be computed, when individual level weights are available, as\n\\[\n\\bar{y}_n = \\dfrac{\\sum^{i = 1}^n w_i \\times y_i}{\\sum_{i = 1}^n w_i}\n\\]\n\n\nWhen data are combined across groups, this weighting decreases the contribution of values for a variable for Latinos the 1/7th the contribution of non-Latinos. This adjustment allows the sample cases to contribute to estimates for the total population in a correct proportionate share. (p. 350)\n\n\n\n7.1.3.2 Weighting to Adjust for Unit Nonresponse\n\nIn the initial sample, there are an equal number of persons aged 12-44 and 4 years or older. Thus, the nonresponse mechanism has led to an overrepresentation of older persons among the respondents.\nTo compensate for this overrepresentation, an assumption is made in survey nonresponse weighting that generates the same kind of weighting adjustments discussed for unequal probabilities of selection. If one is willing to assume that within subgroups (in this case, age groups) the respondents are a random sample of all sample persons, then the response rate in the group represents a sampling rate. This assumption is referred to as the “missing at random” assumption, and is the basis for mich nonresponse adjusted weights. The inverse of the response rate can thus be used as a weight to restore the respondent distribution to the original sample distribution. (p. 352)\n\n\n\n7.1.3.3 Putting All the Weights Together\n\nFinally, to obtain a final weight than can be incorporate the first-stage ratio adjustment (\\(W_{i1}\\)), the unequal probability of selection adjustment (\\(w_{i2}\\)), nonresponse adjustment (\\(w_{i3}\\)), and poststratification (\\(W_{i4}\\)), a final product of all four weights is assigned to each of eight classes […]. (p. 352)\n\n\n\n7.1.3.4 Sampling Variance Estimation for Complex Samples\n\nSurvey datasets based on stratified multistage samples with weights designed to compensate for unequal probabilities of selection or nonresponse are sometimes referred to as “complex survey data”. Estimation of sampling variances for statistics from these kinds of data require widely available specialized software systems using one of three procedures: the Taylor series approximation, balanced repeated replication, or jackknife repeated replication.\n\n\nTaylor Series Estimation […]. The Taylor series approximation handles this difficulty by converting a ratio into an approximation that does not involve ratios, but instead is a function of sums of sample values. […]. For example, the variance for a simple ration mean like\n\\[\n\\bar{y}_n = \\dfrac{\\sum^{i = 1}^n w_i \\times y_i}{\\sum_{i = 1}^n w_i}\n\\]\nusing a Taylor series approximation (assuming simple random sampling, for simplicity) is\n\\[\n\\dfrac{1}{(\\sum w_i)^2} [ \\text{Var}(\\sum w_i y_i) + \\bar{y}_w^2 \\text{Var}(\\sum w_i) - 2 \\bar{y}_w \\text{Cov}(\\sum w_i y_i, \\sum w_i) ]\n\\]\nThis looks complicated but is just a combination of the types of calculations that are made for the simple random sample variance estimates (see for example the computations on page 99). (p. 360)\n\n\nBalanced Repeated Replication and Jackknife Replication The balanced repeated and jackknife repeated methods take and entirely different approach. Rather tha attempting to find an analytic solution to the problem of estimating the sampling variance of a statistic, they rely on repeated subsampling. One can think of repeated replication as being similar to drawing not one but many samples from the same population at the same time. For each sample, an estimate of same statistic, say the mean \\(\\bar{y}_\\gamma\\), is computed for each sample \\(\\gamma\\).\n\nIn that case, the mean is \\(\\bar{y} = \\frac{1}{c} \\sum^c_{\\gamma = 1} = \\bar{y}_\\gamma\\), and the variance is \\(v(\\bar{y}) = \\frac{1}{c(c-1)} \\sum^c_{\\gamma = 1} (\\bar{y}_\\gamma - \\bar{y})^2\\)\n\nThe strength of this approach to estimating the sampling variance of a statistic is that it can be applied to almost any kind of statistic: means, proportions, regression coefficients, and medians. Using these procedures requires thousands of calculations made feasible only with high-speed computing. (p. 361)\n\nThere are differences between the balanced repeated replication and the jackknife replication, but the results are remarkably similar (p. 361). “The Taylor series approximation is the most commonly used approach, but that does not mean that it provides a more precise or accurate estimate of variance than the other procedures.” (p. 361)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inferência com Surveys</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#lumley-t.-2011.-complex-surveys-a-guide-to-analysis-using-r.-john-wiley-sons.-caps.-12",
    "href": "aulas/aula-7.html#lumley-t.-2011.-complex-surveys-a-guide-to-analysis-using-r.-john-wiley-sons.-caps.-12",
    "title": "7  Inferência com Surveys",
    "section": "7.2 Lumley, T. (2011). Complex surveys: a guide to analysis using R. John Wiley & Sons. Caps. 1–2",
    "text": "7.2 Lumley, T. (2011). Complex surveys: a guide to analysis using R. John Wiley & Sons. Caps. 1–2\n\n7.2.1 Chapter 1: Basic Tools\n\nThe mathematical development for most of statistics is model-based, and relies on specifying a probability model for the random process that generates the data. […]. To the extent that the model represents the process that generated the data, it is possible to draw conclusions that can be generalized to other situations where the same process operates. As the model can only ever be an approximation, it is important (but often difficult) to know what sort of departures from the model will invalidate the analysis. (p. 1)\nThe analysis of complex survey samples, in contrast, is usually design-based. The researcher specifies a population, whose data values are unknown but are regarded as fixed, not random. The observed sample is random because it depends on the random selection of individuals from this fixed population. The random selection procedure of individuals (the sample design) is under the control of the reseacher, so all the probabilities involved can, in principle, be known precisely. The goal of the analysis is to estimate features of the fixed population, and design-based inference does not support generalizing findings to other populations. (p. 2)\n\n\nIt is important to remember that what makes a probability sample is the procedure for taking samples from a population, not just the data we happen to end up with. (p. 3)\n\n\nThe fundamental statistical idea behind all of design-based inference is that an individual sampled with a sampling probability of \\(\\pi_i\\) represents \\(1/\\pi_i\\) individuals in the population. The value \\(1/\\pi_i\\) is called the sampling weight. (p. 4)\n\nPor exemplo, se amostramos 3.500 indivíduos de uma população de 350.000, então a probabilidade de seleção de cada indivíduo é \\(\\pi_i = 3.500/350.000 = 0.01\\). Assim, cada indivíduo amostrado representa \\(1/\\pi_i = 100\\) indivíduos na população.\nO Estimador de Horvitz-Thompson é um estimador de média populacional que leva em consideração os pesos amostrais. Ele é dado por:\n\nIf X_i is a measurement of variable X on person \\(i\\), we write:\n\\[\n\\hat{X_i} = \\frac{1}{\\pi_i} X_i\n\\]\n\nCom isso, o estimador para o total populacional \\(\\hat{T}\\) é dado por:\n\\[\n\\hat{T}_X = \\sum_{i=1}^n \\hat{X_i} = \\sum_{i=1}^n \\frac{1}{\\pi_i} X_i\n\\]\nA variância do estimador de Horvitz-Thompson é dada por:\n\\[\n\\text{Var}(\\hat{T}_X) = \\sum_{i, j} \\left( \\dfrac{X_i X_j}{\\pi_{ij}} - \\dfrac{X_i}{\\pi_i} \\dfrac{X_j}{\\pi_j} \\right)\n\\]\n\nKnowing the formula for the variance estimator is less important to the applied user, but it is useful to note two things. The first is that the formula applies to any design, however complicated, where \\(\\pi_i\\) and \\(\\pi_{ij}\\) are known for the sampled observations. The second is that the formula depends on the pairwise sampling probabilities \\(\\pi_{ij}\\), not just on the sampling weights; this is how correlations in the sampling design enter the computations. (p. 5)\n\n\nIf the necessary sample size for a given level of precision is known for a simple random sample, the sample size for a complex design can be obtained by multiplying by the design effect. While the design effect will not be known in advance, some useful guidance can be obtained by looking at design effects reported for other similar surveys. (p. 6)\n\n\n\n7.2.2 Chapter 2: Simple and Stratified Sampling\n\nThe population mean of X can be estimated by dividing the estimated total by the population size, N\n\\[\n\\hat{\\mu}_X = \\frac{1}{N} \\sum^n_{i = 1} \\hat{X}_i = \\frac{1}{n} \\sum^n_{i = 1} X_i\n\\]\nso the estimate is just the sample average. The variance estimate is obtained by dividing the variance estimate for the total by \\(N^2\\): \\[\n\\hat{\\text{var}}[\\hat{\\mu}_X] = \\frac{N - n}{N} \\times \\frac{\\hat{\\text{var}}[X]}{n}\n\\]\nand the standard error of the mean is the square root of \\(\\hat{\\text{var}}[\\hat{\\mu}_X]\\). This formula shows that the uncertainty in the mean is not very sensitive to the population size as long as the population is much larger than the sample. A sample of 100 people gives the same uncertainty about the mean of a population of 10.000 or 100.000.0000. (p. 18)\n\n\nConfidence intervals for estimates are computed by using a Normal distribution for the estimate, ie, for a 95% confidence interval adding and subtracting 1.96 standard errors. This is not the same as assuming a Normal distribution for the data. Under the simple random sampling and the other sampling designs in this book the distribution of estimates across repeated surveys will be close to a Normal distribution (from the Central Limit Theorem) as long as the sample size is large enough and the estimate is not too strongly influenced by the values of just a few observations. (p. 19)\n\n\n[Caso da amostragem estratificada] Since a stratified sample is just a set of simple random samples from each stratum, the Horvitz-Thompson estimator of the total is just the sum of the estimated totals in each stratum and its variance is the sum of the estimated variances in each stratum. The population mean is estimated by dividing the estimated population total by the population size \\(N\\).",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inferência com Surveys</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#shirani-mehr-h.-rothschild-d.-goel-s.-gelman-a.-2018.-disentangling-bias-and-variance-in-election-polls.-journal-of-the-american-statistical-association-113522-607614.",
    "href": "aulas/aula-7.html#shirani-mehr-h.-rothschild-d.-goel-s.-gelman-a.-2018.-disentangling-bias-and-variance-in-election-polls.-journal-of-the-american-statistical-association-113522-607614.",
    "title": "7  Inferência com Surveys",
    "section": "7.3 Shirani-Mehr, H., Rothschild, D., Goel, S., & Gelman, A. (2018). Disentangling bias and variance in election polls. Journal of the American Statistical Association, 113(522), 607–614.",
    "text": "7.3 Shirani-Mehr, H., Rothschild, D., Goel, S., & Gelman, A. (2018). Disentangling bias and variance in election polls. Journal of the American Statistical Association, 113(522), 607–614.\n\nIt has long been known that the margins of errors provided by survey organizations, and reported in the news, understate the total survey error. This is an important topic in sampling but is difficult to address in general for two reasons. First, we like to decompose error into bias and variance, but this can only be done with any precision if we have a large number of surveys and outcomes – not merely a large number of respondents in an individual survey. Second, assessment of error requires a ground truth for comparison, which is tipically not available, as the reason for conducting a sample survey in the first place is to estimate some population characteristic that is not already known. (p. 607)\n\n\nElection polls typically survey a random sample of eligible or likely voters, and then generate population-level estimates by taking a weighted average of responses, where the weights are designed to correct for known differences between sample and population. This general analysis framework yields both a point estimate of the election outcome, and also an estimate of the error in that prediction due to sample variance which accounts for the survey weights (Lohr 2009). In practice, however, polling organizations often use the weights only in computing estimates, ignoring them when computing standard errors and instead reporting 95% margins of error based on the formula for simple random sampling (SRS) – for example, \\(\\pm 3.5\\) percentage points for an election survey with 800 people. Appropriate correction for the “design effect” corresponding to unequal weights would increase margins of error (see, for example, Mercer (2016)).\n\n\nFigure 1 shows the distribution of these differences, where positive values on the \\(x\\)-axis indicate the Republican candidate received more support in the poll than in the election. We repeat this process separately for senatorial, gubernatorial, and presidential polls. For comparison, the dotted lines show the theoretical distribution of polling errors assuming SRS. Specifically, for each senate poll \\(i\\), we first simulate an SRS polling result by drawing a sample from a binomial distribution with parameters \\(n_i\\) and \\(v_{r[i]}\\), where \\(n_i\\) is the number of respondents in poll \\(i\\) who express a preference for one of the two major-party candidates, and \\(v_{r[i]}\\) is the final two-party vote share of the Republican candidate in the corresponding election \\(r[i]\\). (p. 609)\n\n\nThe senatorial and gubernatorial polls, in particular, have substantially larger RMSE (3.7% and 3.9%, respectively) than SRS (2.0% and 2.1%, respectively). In contrast, the RMSE for state-level presidential polls is 2.5%, not much larger than one would expect from SRS (2.0%). (p. 609)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inferência com Surveys</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#anotações-de-aula",
    "href": "aulas/aula-7.html#anotações-de-aula",
    "title": "7  Inferência com Surveys",
    "section": "7.4 Anotações de aula",
    "text": "7.4 Anotações de aula\n\n7.4.1 Conceitos importantes\n\nParâmetro: medida na população\nEstatística: medida calculada na amostra\nEstimador: procedimento para calcular uma estatística a partir de dados amostrais\n\nA gente aproxima o valor de um determinado parâmetro usando estatísticas amostrais. Podemos falar, na prática, de estimadores para calcular parâmetros e estatísticas.\n\n\n7.4.2 Teorema Central do Limite\nEstimadores são variáveis aleatórias que dependem do resultado da amostra, o que significa que há uma distribuição associada a cada estimador. Pelo Teorema Central do Limite, para amostras geralmente \\(n&gt;30\\), começamos a nos aproximar de uma distribuição normal.\nTemos um processo gerador de dados desconhecido e queremos estimar, no caso de uma Binomial, por exemplo, o parâmetro de sucesso. Mas existe algo que eu tenho: o \\(n\\). A maior parte das distribuições que tem a ver com a amostragem possuem dois parâmetros: o tamanho da amostra (que eu tenho, e eu que defino, e portanto consigo calcular incerteza) e o \\(\\bar{y}\\), que eu não tenho.\nQuando estamos falando de um desenho amostral, podemos gerar um número finito de amostras. Cada uma dessas amostras me gera uma estatística e, quando plotamos essas \\(n\\) estatísticas, para \\(n &gt; 30\\), a distribuição da estatística amostral se aproximará de uma distribuição amostral.\n\n\n7.4.3 Incerteza\nUsamos a variância amostral para medir a dispersão das estatísticas amostrais:\n\\[\ns^2 = \\dfrac{\\sum (y_i - \\bar{y})^2}{n-1}\n\\]\nNote que, para fazer isso, calculamos primeiro a média.\nOutra forma de medir incerteza é usando o erro-padrão. A variância amostral penaliza pontos mais distantes da média elevando-os ao quadrado e, além disso, é uma medida positiva. O erro-padrão é uma espécie de correção pra isso, mas não só: a gente também leva em consideração o tamanho da amostra.\n\\[\n\\text{SE}_\\bar{y} = \\sqrt{\\dfrac{s^2}{n}} = \\dfrac{s}{\\sqrt{n}},\n\\] onde \\(s = \\sqrt{s^2}\\) é o desvio-padrão da amostra. Na prática, voltamos para a escala original do dado, então a gente consegue calcular uma margem de erro de maneira mais simples.\nPonto essencial: se a distribuição amostral de vários estimadores parece normal, usamos a função de densidade normal para calcular a probabilidade de uma estimativa cair em um intervalo. A PDF da normal é:\n\\[\nf(x) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}},\n\\]\nonde \\(\\mu\\) é a média da distribuição e \\(\\sigma^2\\) é a variância. Na função de probabilidade, você pluga um número e ela te diz a probabilidade de aquele número ter sido gerado por aquela função. Mais especificamente, quando plugamos a média e o desvio padrão, a PDF da normal retorna a probabilidade de um número pertencer àquele intervalo.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmacae &lt;- readRDS(\"C:/Users/felip/OneDrive/Área de Trabalho/IESP/Survey-Research/listas/lista1/dados/macae.rds\")\n\n# extraímos uma amostra n=50\nam &lt;- macae %&gt;%\n  slice_sample(n = 50)\n\n\n# media, \\bar{y}\nmedia &lt;- sum(am$alfabetizada) / nrow(am)\nprint(media)\n\n[1] 0.94\n\n\n\n# variancia, $s^2$\nvar &lt;- (am$alfabetizada - media)^2\nvar &lt;- sum(var) / (nrow(am) - 1) # tiramos 1 se for uma amostra\nprint(var)\n\n[1] 0.05755102\n\n\n\n# erro padrao, SE_\\bar{y}, na escala do dado original\nsd_y &lt;- sqrt(var)\nse_y &lt;- sd_y / sqrt(nrow(am))\nprint(se_y)\n\n[1] 0.03392669\n\n\nCom isso temos uma margem de erro e um intervalo de confiança:\n\\[\n\\text{ME} = 1.96 \\times \\text{SE}_\\bar{y} \\\\\n\\text{IC}_{95\\%} = \\bar{y} \\pm\\ \\text{ME}\n\\] Parece bom, mas é uma aproximação.\nNo caso de proporções, margens de erro não são simétricas; isto é, estimativas mais próximas de \\(0\\) ou \\(1\\) têm margens de erro menores (porque os limites forçam redução nas margens). Em termos práticos, a variância de proporção assume uma distribuição binomial, e não normal.\nVamos usar um exemplo da taxa de alfabetização em Macaé. Imagine que tiramos a seguinte amostra \\(n=50\\). A variância aqui é \\(\\frac{p(1-p)}{n}\\), onde \\(p\\) é a proporção (ou seja, a média) da amostra. Temos a estatística \\(\\hat{p}\\) e \\(v(\\hat{p})\\). O erro padrão nesse caso é apenas a raiz quadrada da variância, sem dividir pelo \\(\\sqrt{n}\\).\nA depender do meu valor de \\(p\\), a coisa vai mudar. Quando eu estimo alguma coisa que é muito pouco prevalente, \\(p\\) é muito pequeno e a margem de erro também fica muito pequena, porque eu não tenho probabilidades menores que zero, então eu só tenho incerteza “do outro lado”.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Inferência com Surveys</span>"
    ]
  },
  {
    "objectID": "aulas/aula-8.html",
    "href": "aulas/aula-8.html",
    "title": "8  Pós-estratificação",
    "section": "",
    "text": "8.1 Lumley, T. (2011). Complex surveys: a guide to analysis using R. John Wiley & Sons. Chapter 7: Post-stratification, raking, and calibration.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pós-estratificação</span>"
    ]
  },
  {
    "objectID": "aulas/aula-8.html#lumley-t.-2011.-complex-surveys-a-guide-to-analysis-using-r.-john-wiley-sons.-chapter-7-post-stratification-raking-and-calibration.",
    "href": "aulas/aula-8.html#lumley-t.-2011.-complex-surveys-a-guide-to-analysis-using-r.-john-wiley-sons.-chapter-7-post-stratification-raking-and-calibration.",
    "title": "8  Pós-estratificação",
    "section": "",
    "text": "This chapter deals with techniques for using known population totals for a set of variables (auxiliary variables) to adjust the sampling weights and improve estimation for another set of variables. All of these techniques have the same idea: adjustments are made to the sampling weights so that estimated population totals for the auxiliary variables match the known population totals, making the sample more representative of the population. A second benefit is that the estimates are forced to be consistent with the population data, improving their credibility with people who may not understand the sampling process. (p. 135-136)\n\n\nPost-stratification adjusts the sampling weights so that the estimated population group sizes are correct, as they would be in stratified sampling. The sampling weights \\(\\frac{1}{\\pi_i}\\) are replaced by weights \\(\\frac{g_i}{\\pi_i}\\), where \\(g_i = \\frac{N_i}{\\hat{N}_i}\\) for the group containing individual \\(i\\).",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pós-estratificação</span>"
    ]
  },
  {
    "objectID": "aulas/aula-8.html#anotações-de-aula",
    "href": "aulas/aula-8.html#anotações-de-aula",
    "title": "8  Pós-estratificação",
    "section": "8.2 Anotações de aula",
    "text": "8.2 Anotações de aula\nEm amostragem, cada unidade amostral tem um peso. Lemos isso como “quantas unidades na população cada unidade amostrada representa”. Se o desenho amostral tiver equiprobabilidade, cada unidade amostrada representa a mesma quantidade de unidade da população e pesos não são necessários.\nO estimador ponderado é:\n\\[\n\\hat{Y} = \\sum_{i \\in s} w_i y_i\n\\]\nonde \\(w_i\\) é o peso para a unidade \\(i\\) e \\(y_i\\) é a medida extraída do survey.\nConsidere o seguinte exemplo:\n\n\n\nÁrea\nIdade\nQuantas pessoas ela representa\n\n\n\n\nZS\n25\n349\n\n\nZS\n30\n482\n\n\n\nNesse caso, indicamos quantas pessoas cada unidade amostral representa. Esse valor é calculado a partir da probabilidade de inclusão da unidade amostral. O peso é o inverso da probabilidade de inclusão. A ponderação pelo desenho ajusta amostras quando temos probabilidades desiguais de inclusão: \\(w_i = \\frac{1}{\\pi_i}\\), onde \\(\\pi_i\\) é a probabilidade de inclusão da unidade amostral \\(i\\). Supondo que a probabilidade de inclusão é de 0.25, o peso da unidade amostral é 4. Isso significa que ela representa 4 unidades na população.\nEsse estimador também é chamado de estimador de expansão – o estimador de Horwitz-Thompson. O peso absoluto me diz quantas pessoas de perfil similar uma única pessoa da amostra representa.\n\\[\n\\hat{N} = \\sum_{i = 1}^n \\frac{y_i}{w_i}\n\\]\nNote que esse peso é útil para calcular totais populacionais, mas ele é diferente do peso usado para fazer ponderação. Por que ponderar?\n\nPonderar pela probabilidade de inclusão quando não temos equiprobabilidade é importante para não incorporar viés na amostra (claro, isso depende de uma amostragem probabilística);\nDar maior peso a subgrupos que responderam menos ao survey (não-resposta);\nAjustar estimativas com totais populacionais conhecidos (aka calibração)\n\n\n8.2.1 Ponderação de ajuste\nProblemas com quadro amostral (cobertura, e.g.) e não-resposta de unidade podem introduzir viés se respondentes diferem sistematicamente de não-respondentes:\n\nEntrevistamos muito menos pessoas mais ricas do que deveríamos;\nUsamos estratificação com alocação não-proporcional (i.e., regiões pequenas e grandes recebem o mesmo número de entrevistas);\nBolsonaristas ou lulistas se recusam mais a responder.\n\nA ponderação duplica, triplica, às vezes quadruplica a informação que temos. Na prática, isso lembra a amostragem por conglomerados, na perspectiva de que estamos aumentando a margem de erro ao não incorporar informações efetivamente novas.\nIdeia básica: ajustar os pesos dos respondentes para compensar as unidades ausentes ou sub-representadas. Pesquisa domiciliar probabilística com \\(n=1000\\) entrevistou \\(380\\%\\) homens; na população, homens são \\(50\\%\\):\n\\[\n\\frac{0.5}{0.38} \\ \\text{ou} \\ \\frac{500}{380} \\approx 1.315789474\n\\]\nCada homem na amostra representa 1.315789474 homens na população. O mesmo vale para mulheres, mas o peso é menor.\n\n\n8.2.2 Calibração vs. ponderação\nQuando já temos pesos (geralmente do desenho), podemos forçá-los a corresponder a totais populacionais conhecidos em variáveis auxiliares.\nPonderar é pegar a amostra que eu tenho e tentar aproximá-la de algum benchmark (por exemplo, a população). Calibrar, por outro lado, é pegar um peso que eu já tenho (no mais das vezes, a probabilidade de inclusão) – supondo que a gente calcule o total populacional e, no entanto, a população brasileira deu \\(205\\) milhões, mas sabemos que a população brasileira é de \\(203\\) milhões. A calibragem ajusta o esquema de pesos para fazer com que o total bata com algum total conhecido da população.\nNesse caso, precisamos encontrar novos pesos \\(w_i^{\\text{Cal}}\\) próximos aos pesos iniciais de forma que:\n\\[\n\\sum_{i \\in s} w_i^{\\text{Cal}} x_i = \\sum_{i \\in U} x_i = X,\n\\]\nonde \\(X\\) é o total populacional (\\(U\\) é a população) conhecido (e.g., total de eleitores) para a variável auxiliar \\(x_i\\). Esse ajuste pode ser feito de várias maneiras: algoritmos iterativos, regressão, etc.\n\n\n8.2.3 Pesos e inflação da variância\nPesos de desenho muito grande ou desiguais inflam estimativas.\n\nIntuição: uma unidade com peso relativo de \\(2\\) equivale a duplicar a sua entrevista na amostra; para isso, no entanto, precisamos retirar uma unidade de outras unidades da amostra. A solução geralmente é usar tetos ou ajustes para limitar pesos extremos.\n\n\n\n8.2.4 Outros tipos de pós-ajustes em R\n\nRake e iterative proportional fitting, que ajustam pesos iterativamente para que totais populacionais conhecidos sejam respeitados (pesos relativos);\nGeneralized regression estimation (GREG), que usa modelos de regressão para calibrar pesos;\nMultilevel regression and poststratification (MrP), que projeta resposta em subgrupos da população (e.g., regiões, idade, sexo) usando modelos de regressão\n\n\n\n8.2.5 Usando o pacote survey no R\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(survey)\n\nWarning: pacote 'survey' foi compilado no R versão 4.4.3\n\n\nCarregando pacotes exigidos: grid\nCarregando pacotes exigidos: Matrix\n\nAnexando pacote: 'Matrix'\n\nOs seguintes objetos são mascarados por 'package:tidyr':\n\n    expand, pack, unpack\n\nCarregando pacotes exigidos: survival\n\nAnexando pacote: 'survey'\n\nO seguinte objeto é mascarado por 'package:graphics':\n\n    dotchart\n\nset.seed(123)\n\nmacae &lt;- readRDS(\"C:/Users/felip/OneDrive/Área de Trabalho/IESP/Survey-Research/listas/lista1/dados/macae.rds\")\n\nverdade &lt;- mean(macae$alfabetizada)\nprint(verdade)\n\n[1] 0.8720686\n\n\nDe maneira naive, podemos estimar a média apenas tirando a média simples da amostra. Devemos esperar algo razoavelmente distoante do parâmetro populacional.\n\namostra &lt;- macae %&gt;%\n  group_by(subdistrito) %&gt;%\n  mutate(pi = 100 / n()) %&gt;%\n  slice_sample(n = 100)\n\nmean(amostra$alfabetizada)\n\n[1] 0.8485714\n\n\nUsando o estimador de Horwitz-Thompson, podemos recuperar os pesos absolutos usando a probabilidade de inclusão (i.e., ponderação pela probabilidade de inclusão):\n\namostra &lt;- amostra %&gt;%\n  mutate(peso = 1 / pi)\n\nVamos fazer a mesma coisa usando o pacote survey, que nos permite declarar o desenho amostral. Vamos usar as probabilidades de inclusão para “acertar” o cálculo, assim como no exemplo anterior:\n\n# todas as observacoes sao intercambiaveis dentro de um estrato, entao ignoramos id (~1)\nmetodo1 &lt;- svydesign(~1,\n                     strata = ~subdistrito, # estrato\n                     probs = ~pi, # probabilidades de inclusao para ponderar\n                     data = amostra)\n\nsvytotal(~alfabetizada, metodo1)\n\n              total     SE\nalfabetizada 176920 3347.8\n\nsvymean(~alfabetizada, metodo1)\n\n                mean     SE\nalfabetizada 0.85581 0.0162\n\n\nAgora, vejamos o método 2: não temos a probabilidade de inclusão, mas tenho os totais populacionais extraídos a partir de alguma outra fonte – como, por exemplo, o censo demográfico. Eu sei, por exemplo, quantas pessoas moram em cada distrito.\n\n# MÉTODO 2: pós-estratificação\n\n# vamos ver o caso naive\ndesenho &lt;- svydesign(~id_pessoa,\n                     data = amostra)\n\nWarning in svydesign.default(~id_pessoa, data = amostra): No weights or\nprobabilities supplied, assuming equal probability\n\n# estimamos a média de maneira naive sem ponderar\nsvymean(~alfabetizada, desenho)\n\n                mean     SE\nalfabetizada 0.84857 0.0136\n\n# agora vamos estratificar\nestratos &lt;- macae %&gt;%\n  group_by(subdistrito) %&gt;%\n  summarise(Freq = n())\n\n# criamos um objeto do tipo postStratify\nmetodo2 &lt;- postStratify(desenho, ~subdistrito, estratos)\n\n# calculamos a media usando esse meotod, que estratificou os resultados\nsvymean(~alfabetizada, metodo2)\n\n                mean     SE\nalfabetizada 0.85581 0.0161\n\n\nAgora passemos ao método 3, que usa Rake:\n\ndesenho &lt;- svydesign(~id_pessoa,\n                     data = amostra)\n\nWarning in svydesign.default(~id_pessoa, data = amostra): No weights or\nprobabilities supplied, assuming equal probability\n\n# se fossem varias variaveis desbalanceadas, criaria um banco pra cada e \n# passaria cada um deles na lista\nmetodo3 &lt;- rake(desenho, \n                list(~subdistrito),\n                list(estratos))\n\nsvymean(~alfabetizada, metodo3)\n\n                mean     SE\nalfabetizada 0.85581 0.0161\n\n\nPodemos extrair os pesos calculados por cada desenho da seguinte maneira:\n\namostra &lt;- amostra %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    peso_strat = weights(metodo1),\n    peso_pos_strat = weights(metodo2),\n    peso_rake = weights(metodo3)\n  )\n\nhead(amostra)\n\n# A tibble: 6 × 11\n  codigo_setor  id_pessoa distrito subdistrito bairro alfabetizada      pi  peso\n  &lt;chr&gt;         &lt;glue&gt;    &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 330240305020… 33024030… Macaé    Aeroporto   Parqu…            1 0.00265  378.\n2 330240305020… 33024030… Macaé    Aeroporto   Ajuda             1 0.00265  378.\n3 330240305020… 33024030… Macaé    Aeroporto   Ajuda             0 0.00265  378.\n4 330240305020… 33024030… Macaé    Aeroporto   Ajuda             1 0.00265  378.\n5 330240305020… 33024030… Macaé    Aeroporto   Parqu…            1 0.00265  378.\n6 330240305020… 33024030… Macaé    Aeroporto   Parqu…            1 0.00265  378.\n# ℹ 3 more variables: peso_strat &lt;dbl&gt;, peso_pos_strat &lt;dbl&gt;, peso_rake &lt;dbl&gt;\n\n\nObserve que chegamos \\(\\pm\\) aos mesmos pesos com cada approach. De fato, são diferentes maneiras de resolver o mesmo problema.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pós-estratificação</span>"
    ]
  },
  {
    "objectID": "aulas/aula-9.html",
    "href": "aulas/aula-9.html",
    "title": "9  Pós-estratificação com modelos",
    "section": "",
    "text": "9.1 Wang, W., Rothschild, D., Goel, S., & Gelman, A. (2015). Forecasting elections with non-representative polls. International Journal of Forecasting, 31(3), 980–991\nO autor nota analogamente que podemos derivar a estimativa \\(\\hat{y}^{\\text{PS}}\\) para qualquer nível subpopulacional \\(s\\) apenas limitando \\(j\\) a \\(j \\in J_s\\), obtendo \\(\\hat{y}^{\\text{PS}}_s\\).\nO modelo descrito é o seguinte:\n\\[\n\\begin{align*}\n\\text{Pr}&(Y_i \\in \\{\\text{Obama, Romney}\\}) \\\\\n&= \\text{logit}^{-1}(\\alpha_0 + \\alpha_1 (\\text{state last vote share}) \\\\\n&+ a_{j[i]}^\\text{state} + a_{j[i]}^\\text{edu} + a_{j[i]}^\\text{sex} + a_{j[i]}^\\text{age} + a_{j[i]}^\\text{race} + a_{j[i]}^\\text{party ID}  \\\\\n&+ a_{j[i]}^\\text{ideology} + a_{j[i]}^\\text{last vote})\n\\end{align*}\n\\]\nAs prioris dos coeficientes \\(a_{j[i]}^\\text{var}\\) são definidos como distribuições normais com média zero e variância \\(\\sigma^2_{a_{j[i]}^\\text{var}}\\). A variância \\(\\sigma^2_{a_{j[i]}^\\text{var}}\\) é estimada a partir de uma distribuição inversa-gama com parâmetros \\((v, \\sigma_0^2)\\).\n\\[\n\\begin{align*}\n\\text{Pr}&(Y_i = \\text{Obama} \\ | \\ Y_i \\in \\{\\text{Obama, Romney}\\}) \\\\\n&= \\text{logit}^{-1}(\\beta_0 + \\beta_1 (\\text{state last vote share}) \\\\\n&+ b_{j[i]}^\\text{state} + b_{j[i]}^\\text{edu} + b_{j[i]}^\\text{sex} + b_{j[i]}^\\text{age} + b_{j[i]}^\\text{race} + b_{j[i]}^\\text{party ID}  \\\\\n&+ b_{j[i]}^\\text{ideology} + b_{j[i]}^\\text{last vote})\n\\end{align*}\n\\]",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pós-estratificação com modelos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-9.html#wang-w.-rothschild-d.-goel-s.-gelman-a.-2015.-forecasting-elections-with-non-representative-polls.-international-journal-of-forecasting-313-980991",
    "href": "aulas/aula-9.html#wang-w.-rothschild-d.-goel-s.-gelman-a.-2015.-forecasting-elections-with-non-representative-polls.-international-journal-of-forecasting-313-980991",
    "title": "9  Pós-estratificação com modelos",
    "section": "",
    "text": "While representative polling has historically proven to be quite effective, it comes at considerable costs of time and money. Moreover, as response rates have declined over the past several decades, the statistical benefits of representative sampling have diminished. […]. After adjusting the Xbox responses via multilevel regression and poststratification, we obtain estimates which are in line with the forecasts from leading poll analysts, which are based on aggregating hundreds of traditional polls conducted during the election cycle. (p. 1)\n\n\nBy one measure, RDD response rates have decreased from 36% in 1997 to 9% in 2012, and other studies confirm this trend. (p. 2)\n\n\nEven if the selection issues are not yet a serious problem for accuracy, as some have argued (Holbrook et al., 2007), the downward trend in response rates suggests an increasing need for post-sampling adjustments; indeed, the adjustment methods we present here should work just as well for surveys obtained by probability sampling as for convenience samples. The second trend driving our research is the fact that, with recent technological innovations, it is increasingly convenient and cost-effective to collect large numbers of highly non-representative samples via online surveys. (p. 2)\n\n\nDespite the large sample size [30,000], the pool of Xbox respondents is far from being representative of the voting population. […]. The most striking differences are for age and sex.\n\n\nPoststratification is a popular method for correcting for known differences between sample and target populations (Little, 1993); The core idea is to partition the population into cells based on combinations of various demographic and political attributes, use the sample to estimate the response variable within each cell, and finally aggregate the cell-level estimates up to a population-level estimate by weighting each cell by its relative proportion in the population. Using \\(y\\) to indicate the outcome of interest, the poststratification estimate is defined by\n\\[\n\\hat{y}^{\\text{PS}} = \\dfrac{\\sum^J_{j = 1} N_j \\hat{y}_j}{\\sum^J_{j = 1} N_j}\n\\]\nwhere \\(\\hat{y}_j\\) is the estimate of \\(y\\) in cell \\(j\\), and \\(N_j\\) is the size of the \\(j\\)th cell in the population. (p. 3)\n\n\n\nOne of the most common ways to generate cell-level estimates is to simply average the sample responses within each cell. If we assume that, within a cell, the sample is drawn at random from the larger population, this yields an unbiased estimate. However, this assumption of cell-level simple random sampling is only reasonable when the partition is sufficiently fine; on the other hand, as the partition becomes finer, the cells become sparse, and the empirical sample averages become unstable. We address these issues by instead generating cell-level estimates via a regularized regression model, namely multilevel regression. This combined model-based poststratification strategy, known as multilevel regression and poststratification (MRP), has been used to obtain accurate small-area subgroup estimates, such as for public opinion and voter turnout in individual states and demographic subgroups (Ghitza & Gelman, 2013; Lax & Phillips, 2009; Park, Gelman, & Bafumi, 2004). (p. 4)\nMore formally, applying MRP in our setting comprises two steps. First, we fit a Bayesian hierarchical model to obtain estimates for sparse poststratification cells; second, we average over the cells, weighting the values by a measure of forecasted voter turnout, to get state- and national-level estimates. (p. 4)\n\n\nWe fit two nested multilevel logistic regressions for estimating candidate support in each cell. The first of the two models predicts whether a respondent supports a major-party candidate (i.e., Obama or Romney), and the second predicts support for Obama, given that the respondent supports a major-party candidate.\n\n\n\n\nwhere \\(\\alpha_0\\) is the fixed baseline intercept and \\(\\alpha_1\\) is the fixed slope for Obama’s fraction of the two-party vote share in the respondent’s state in the last presidential election. The terms \\(a_{j[i]}^\\text{state}, a_{j[i]}^\\text{edu}, a_{j[i]}^\\text{sex}\\) and so on – which we denote in general by \\(a_{j[i]}^\\text{var}\\) – correspond to the varying coefficients associated with each categorical variable. Here, the subscript \\(j[i]\\) indicates the cell to which the \\(i\\)th respondent belongs. For example, \\(a_{j[i]}^\\text{age}\\) takes values from \\(\\{ a_{18-29}^\\text{age}, a_{30-44}^\\text{age}, a_{45-64}^\\text{age}, a_{65+}^\\text{age} \\}\\) depending on the cell membership of the \\(i\\)th respondent. (p. 4)\n\n\n\nThe benefit of using a multilevel model is that the estimates for relatively sparse cells can be improved through “borrowing strength” from demographically similar cells that have richer data. Similarly, the second model is defined by:\n\n\n\nHaving detailed the multilevel regression step, we now turn to poststratification, where the cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated ti the appropriate level (i.e., state or national). To compute cell weights, we require cross-tabulated population data. (p. 4-5)\n\n\nHowever, not only are out estimates intuitively reasonable, they are also in line with the prevailing estimates based on traditional, representative polls. In particular, our estimates rougthly track – and are even arguably better than – those from Pollster.com, one of the leading poll aggregators during the 2012 campaign. (p. 5)\n\n\nAs was mentioned above, daily estimates of voter intent do not translate directly of vote share on election day. (p. 8)\n\n\nTo convert daily estimates of vote intent into election day predictions – which we refere to hereafter as calibrating voter intent – we compare the daily voter intent in previous elections to the ultimate outcomes in those elections. Specifically, we collect historical data from three previous US presidential elections, in 2000, 2004, and 2008. […]. We rely on traditional, representative polls to reconstruct historical vote intent; in principle, however, we could have started with non-representative polls if such data had been available in previous election cycles. (p. 8)\nWe next infer a mapping from voter intent to election outcomes by regressing the election day vote share on the historical time series of voter intent. The key difference between our approach and previous related work (Erikson & Wlezien, 2008; Rothschild, 2009) is that we model state-level correlations explicitly, via nested national and state models and correlated errorterms. Specifically, we first fit a national model, given by:\n\\(y_e^{\\text{US}} = a_0 + a_1 x^{\\text{US}}_{t, e} + a_2 | x^{\\text{US}}_{t, e} | x^{\\text{US}}_{t, e} + a_3 t x^{\\text{US}}_{t, e} + \\eta(t, e)\\)\nwhere \\(y_e^{\\text{US}}\\) is the national election day vote share of the incumbent party candidate in election year \\(e\\), \\(x^{\\text{US}}_{t, e}\\) is the national voter intent of the incumbent party candidate \\(t\\) days before the election in year \\(e\\), and \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\) is the error term. (p. 8)\n\n\nIn summary, the procedure for generating election day forecasts proceeds in three steps:\n\nEstimate the joint distribution of state and national voters intent by applying MRP to the Xbox data, as described in Section 3.\nFit the nested calibration model described above to historical data in order to obtain point estimates for the parameters, including estimates for the error terms.\nConvert the distribution of voter intent to election day forecasts via the fitted calibration model. (p. 8)\n\n\n\nNevertheless, we show that the MRP-adjusted and calibrated Xbox estimates are intuitively reasonable, and are also similar to those generated by more traditional means. (p. 11)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pós-estratificação com modelos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-9.html#hanretty-c.-2020.-an-introduction-to-multilevel-regression-and-post-stratification-for-estimating-constituency-opinion-political-studies-review-184-630645",
    "href": "aulas/aula-9.html#hanretty-c.-2020.-an-introduction-to-multilevel-regression-and-post-stratification-for-estimating-constituency-opinion-political-studies-review-184-630645",
    "title": "9  Pós-estratificação com modelos",
    "section": "9.2 Hanretty, C. (2020). An Introduction to Multilevel Regression and Post-Stratification for Estimating Constituency Opinion: Political Studies Review, 18(4), 630–645",
    "text": "9.2 Hanretty, C. (2020). An Introduction to Multilevel Regression and Post-Stratification for Estimating Constituency Opinion: Political Studies Review, 18(4), 630–645\n\nMultilevel regression and post-stratification (MRP) is a technique for estimating public opinion in small areas using large national samples. ‘Small areas’ are usually anything smaller than nations, and past work using MRP has produced estimates for areas as large as US states (average population: 6.5 million) to areas as small as Westminster constituencies (average population: 100,000). (p. 630)\n\n\nMRP is used because the alternatives are either very poor or very expensive. A poor alternative is simply splitting a large sample into (much) smaller geographic subsamples. This is a poor alternative because there is no guarantee that a sample which is representative at the national level will be representative when it is broken down into smaller groups. […].\nAn expensive alternative is conducting polls in each small area for which we want to estimate public opinion. This strategy is possible where the number of small areas is relatively small. […]. (p. 631)\n\n\nThere are four stages which must be carried out when conducting an analysis of local opinion using MRP:\n\nConduct or compile survey information which contains information on respondents’ opinions regarding some political or social issue, and information on respondents’ background characteristics, and information on which small area the respondent lives in;\nCompile information on relevant charactertistics of the small areas in questions;\nEstimate a multilevel regression model using the information from the first and second stages;\nObtain or construct a post-stratification frame which contains information on the joint distribution of respondent background characteristis by small area;\nMake predictions from the multilevel regression model estimated in stage 3 for each row in the post-stratification frame, and aggregate these predictions to the level of the small area. (p. 632)\n\n\n\nEstimates of opinion in small areas has to be based on survey responses. Researchers may be in the fortunate position of being able to commission original survey research. Or, researchers may have access to the raw data from an existing large survey. Finally, researchers may have access to several smaller surveys which they can combine. (p. 632)\n\n\nVery often descriptions of MRP do not discuss the process of gathering constituency information. This is unfortunate. The accuracy of MRP estimates can be more strngly affected by the inclusion of good constituency predictions than by the details of the poststratification frame. (p. 633)\n\n\nMultilevel regression models are models where the parameters in the model apply to different levels. In MRP models, these levels are usually hierarchically organised. That is, the model uses information about respondents (level \\(1\\) information), but also information about the small areas in which respondents are located (level \\(2\\) information), and possibly also information about broader groupings of small areas like regions (level \\(3\\) information).\nOne key part of MRP models is the effect associated with each small area, which some researchers describe as a random intercept. These constituency effects are drawn from a common distribution. This allows for small area estimates to be idiosyncratic, given what we know and can measure about the people who live in them and their other characteristics. Crucially, it allows for these idiosyncrasies to borrow strength from one another. Because random intercepts are drawn from a common distribution, information about a different small area can affect our estimate of the effect associated with one respondent’s area. If evidence from another area suggests that the effect associated with that area is very large, it can mean that the distribution of area effects generally contains very large values. This might in turn mean that we estimate a larger value of the area effect for the area we started with. (p. 633-634)\n\n\nIn the context of MRP, a post-stratification frame is a large rectangular data frame which contains, for each small area, all the possible combinations of respondents characteristics, together with either the count or the proportion of residents of each area who have those chacaracteristics. (p. 634)\n\n\nThe simple way of producing a joint distribution is to use information on the marginal distribution of variables (which Census authorities do release at small area level), and assume that these variables are independent. Thus, the proportion of women who are aged 25-34 is equal to the proportion of women, times the proportion of people aged 25-34. This is a poor way of producing joint distributions, because important social and political variables are often associated with one another. In the UK, the proportion of people aged 55-64 with a university degree is substantially lower than the proportion of people aged 55-64 times the proportion of people with a university degree, because university education has become more popular over time. (p. 634-635)\nThe elaborate way of producing a joint distribution for each small area is to take some existing joint distribution (perhaps the join distribution at national level, or the joint distribution from the survey), and ‘rake’ these join distributions to match the known marginal distribution at the level of the small area. (p. 634-635)\n\n\nIf researchers are able to estimate a model, and able to obtain a post-stratification frame, then the last stage should be comparatively easy. All that is required is to generate predicted values for each row in the post-stratification frame using the parameter values from the estimated regression model. These might be predicted probabilities (for a dichotomous outcome) or predicted values (for a continuous outcome). These predicted values can then be multiplied by the proportion given in the post-stratification frame, and added together to give the proportion or count at the level of the constituency.\nWhen estimates of uncertainty are required (and estimates of uncertainty are always useful) then predicted values can be repeatedly generated using different draws from the posterior distribution of the model. (p. 635)\n\n\nSurveys might ask about rates of satisfaction with ‘your local MP’, or ‘your local police service’. It would be wrong to model these kinds of opinions using MRP, because the same process would not govern the responses given. (p. 636)\n\n\nIf the sample is non-representative because certain characteristics are over- (under-) represented, and those characteristics are included in the post-stratification frame, then MRP can do well. Wang et al. (2015) used data from a survey of Xbox users to predict the results of the 2012 presidential election. Xbox users are not a representative sample of the population: they are much younger and much more likely to be male. However, those demographic characteristics are present in most post-stratification frames. (p. 637)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pós-estratificação com modelos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-9.html#lax-j.-r.-phillips-j.-h.-2009.-how-should-we-estimate-public-opinion-in-the-states.-american-journal-of-political-science-531-107121",
    "href": "aulas/aula-9.html#lax-j.-r.-phillips-j.-h.-2009.-how-should-we-estimate-public-opinion-in-the-states.-american-journal-of-political-science-531-107121",
    "title": "9  Pós-estratificação com modelos",
    "section": "9.3 Lax, J. R., & Phillips, J. H. (2009). How Should We Estimate Public Opinion in the States. American Journal of Political Science, 53(1), 107–121",
    "text": "9.3 Lax, J. R., & Phillips, J. H. (2009). How Should We Estimate Public Opinion in the States. American Journal of Political Science, 53(1), 107–121\n\nUnfortunately, measuring state opinion is not easy. Despite the proliferation of public opinion polls, state-level surveys are still quite rare. Finding comparable surveys across all (or even many) states is nearly impossible. And, while most national poll data include the home state of the respondents, there are almost always too few respondents within each state to be considered an adequate sample. (p. 107)\n\n\nAs an alternative [to disaggregation], recent work by Park, Gelman, and Bafumi (2006) presents a new version of simulating state opinion, based on multilevel regression and poststratification (MRP). This has the potential to combine the best features of both disaggregation and simulation techniques. It revives the old simulation method, incorporating demographic information to improve state estimation, while allowing for nondemographic differences between states. That is, opinion is modeled s a function of both demographics and state-specific effects. The estimation of these effects is improved by using a multilevel model and partially pooling data across states (to an extent warranted by the data). Predictions are made for each demographic-geographic respondent type, and these predictions are then poststratified (weighted) by population data. The drawback here is the need to detailed demographic data on respondents and states, along with greater methodological complexity. (p. 108)\n\n\nMost importantly, we show that (1) MRP should be employed when data samples are small to medium size; (2) for very large samples, the gain from MRP are less likely to be worth its implementation costs; (3) relatively simple demographic typologies can suffice for MRP, but additional demographic information improves estimation; and (4) MRP can be used successfully even on small samples, such as individual national polls. (p. 108)\n\n\nIt [multilevel modeling with Bayesian statistics] improves upon the estimation of the effects of individual- and state-level predictors by employing recent advances in multilevel modeling, a generalization of linear and generalized linear modeling, in which relationships between grouped variables are themselves modeled and estimated. This partially pools information about respondents across states to learn about what drives individual responses. Whereas the disaggregation method copes with insufficient samples withing states by combining many surveys, MRP compensates for small within-state samples by using demographic and geographic correlations. (p. 109)\n\n\nA multilevel model pools group-level parameters towards their mean, with greater pooling when group-level variance is small and more smoothing for less populated groups. The degree of pooling emerges from the data, with similarities and differences across groups estimated endogenously. (p. 111)\n\nO modelo utilizado pelos autores é o seguinte:\n\\[\n\\text{Pr}(y_i = 1) = \\text{logit}^{-1}(\n    \\beta_0 +\n    \\alpha^{\\text{race, gender}}_{j[i]} +\n    \\alpha^{\\text{age}}_{k[i]} +\n    \\alpha^{\\text{edu}}_{l[i]} +\n    \\alpha^{\\text{state}}_{s[i]} +\n    \\alpha^{\\text{year}}_{p[i]}\n    )\n\\]\nAlém disso, cada um dos parâmetros \\(\\alpha\\) é modelado como uma distribuição normal com média \\(0\\) e variância \\(\\sigma^2\\). Os autores utilizam a seguinte notação:\n\\[\n\\begin{align*}\n\\alpha_j^{\\text{race, gender}} &\\sim \\mathcal{N}(0, \\sigma^2_{\\text{race, gender}}), \\ \\text{for } j = 1, \\ldots, 6 \\\\\n\\alpha_k^{\\text{age}} &\\sim \\mathcal{N}(0, \\sigma^2_{\\text{age}}), \\ \\text{for } k = 1, \\ldots, 4 \\\\\n\\alpha_l^{\\text{edu}} &\\sim \\mathcal{N}(0, \\sigma^2_{\\text{edu}}), \\ \\text{for } l = 1, \\ldots, 4 \\\\\n\\alpha_p^{\\text{year}} &\\sim \\mathcal{N}(0, \\sigma^2_{\\text{year}}), \\ \\text{for } p = 1, \\ldots, 7\n\\end{align*}\n\\]\nOs efeitos dos estados, em particular, são modelados como uma função da região do estado e o percentual de evangélicos/mórmons no estado:\n\\[\n\\alpha_s^{\\text{state}} \\sim \\mathcal{N}(\\alpha_{m[s]}^{\\text{region}} + \\beta^{\\text{relig}} \\cdot \\text{relig}_s, \\sigma^2_{\\text{state}}), \\text{for } s = 1, \\ldots, 49\n\\]\nA variável da região, portanto, também é modelada como uma distribuição normal com média \\(0\\) e variância \\(\\sigma^2\\):\n\\[\n\\alpha_m^{\\text{region}} \\sim \\mathcal{N}(0, \\sigma^2_{\\text{region}}), \\ \\text{for } m = 1, \\ldots, 5\n\\]\n\nIn short, the bulk of gains from MRP over disaggregation are achieved not from simple pooling of states towards the national mean, but due to the more accurate modeling of responses by demographic-geographic type. We, of course, recognize that this is not the last word on the subject but rather that others might wish to explore how much further one can push the envelope and to assess how large the gains are from MRP in varying contexts (we begin this process in the replication section below). (p. 116)\n\n\nFinally, and perhaps most importantly, MRP does well in an absolute sense regardless of sample size. This approach, if implemented using a single, large national survey, produces estimates of state-level public opinion that are virtually as accurate as those it generates using 10 or more surveys. (Note that these gains from MRP exist even when we use disaggregation itself to set the baseline measure of opinion.) (p. 120)\n\n\nThis will greatly enhance research into the responsiveness of state governments. Additionally, since MRP can effectively be used with relatively little data and simple demographic typologies, it can also be applied to studies of public opinion over smaller time periods or in smaller geographic units, such as congressional districts or school districts, for which detailed demographic data are limited, or for other subsets of the population. (p. 121)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pós-estratificação com modelos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-9.html#buttice-m.-k.-highton-b.-2013.-how-does-multilevel-regression-and-poststratification-perform-with-conventional-national-surveys-political-analysis-2104-449467.",
    "href": "aulas/aula-9.html#buttice-m.-k.-highton-b.-2013.-how-does-multilevel-regression-and-poststratification-perform-with-conventional-national-surveys-political-analysis-2104-449467.",
    "title": "9  Pós-estratificação com modelos",
    "section": "9.4 Buttice, M. K., & Highton, B. (2013). How does multilevel regression and poststratification perform with conventional national surveys? Political Analysis, 21(04), 449–467.",
    "text": "9.4 Buttice, M. K., & Highton, B. (2013). How does multilevel regression and poststratification perform with conventional national surveys? Political Analysis, 21(04), 449–467.\n\nTo investigate this aspect of MRP and explain its variation in performance, we analyze MRP opinion estimates for eighty-nine survey items and conduct a host of Monte Carlo simulations. We find that when MRP is used with national surveys of typical size, its performance is highly variable. The sources of the variation in MRP performance derive from properties of the MRP model and the nature of the distribution of the opinion being estimated. The findings of this article therefore imply important qualifications to the initial views. (p. 450)\n\n\nSome common limitations keep survey disaggregation from more widespread use. First, large national surveys are rare and expensive. Second, in instances where large national surveys that include the desired measures and geographic identifiers are available, scholars are often limited to cross-sectional analyses due to the lack of over-time data. Third, pooling surveys requires that identical (or very similar) questions be asked in repeated national samples, and this is an uncommon occurrence. […]. It is against this backdrop that Lax and Phillips (2009b) ask “How Should We Estimate Public Opinion in the States?” and Warshaw and Rodden (2012) ask “How Should We Measure District-Level Public Opinion on Individual Issues?” Both provide the same answer: MRP. (p. 450)\n\n\nAs sample sizes get smaller and approach those of typical national surveys, the sampling errors associated with the DM estimates grow, thereby driving down their performance. In contrast, the multilevel model in MRP places less weight on group-level variation as sample sizes decline, thereby limiting the effect of sampling error and avoiding the cause of performance falloff for DM. Thus, while DM necessarily suffers as sample sizes decline, MRP may not. (p. 452)\n\n\nIn light of the uses for which MRP has already been employed and to which it could be employed in the future, a critical question is whether MRP consistently performs well with samples the size of typical national surveys. (p. 453)\n\n\nThe analysis of eighty-nine policy items and Monte Carlo simulations substantiates the key proposition that with samples the size found in typical national surveys, the performance of MRP is highly variable. Sometimes MRP produces opinion estimates that correlate strongly with true values, have little bias, modest error, and reasonable coverage; sometimes they do not. […]. In contemporary American politics, there is greater geographic heterogeneity on cultural issues than on other issues, and this variation is more readily accounted for by the sort of geographic covariates typically used in MRP analyses. (p. 462-463)\n\n\nTherefore, when Lax and Phillips (2012) report that the relationship between MRP opinion estimates and policy adoption varies in strength across the policies, it is unclear whether it represents true variation in responsiveness, as Lax and Phillips (2012) conclude, or whether it only appears that responsiveness varies because of the variation in the quality of the MRP estimates across policies. (p. 463)\n\n\nThe difficulty posed by the need for strong geographic predictors is increased because in any applied MRP analysis, one only has a sample of opinions from the population. The population-level parameters necessary for assessing the quality of the MRP estimates are, of course, unavailable. Researchers cannot empirically determine the strength of their geographic-level covariates. 22 The implication is that when MRP is used with conventional national survey samples, a persuasive theoretical argument should accompany the description of the covariates included in the multilevel model. Researchers should explain why there is good reason to believe that the geographic covariates are strong proxies for the opinion being estimated. (p. 464)\n\n\nAfter extensive analysis with actual and simulated survey data, the key factors we have identified that determine how well MRP performs are the strength of the geographic-level covariates included in the multilevel model of opinion and the ratio of opinion variation across geographic units relative to opinion variation within units. When these values are sizable, then MRP will often produce reliable estimates from national surveys of conventional size. However, the empirical analysis suggests that often these conditions will not be satisfied. Certainly scholars should not assume that they are met. One should therefore not presume that the properties of the MRP model are sufficient to produce the desired opinion estimates from conventional national survey samples. (p. 465)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pós-estratificação com modelos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-9.html#other-materials",
    "href": "aulas/aula-9.html#other-materials",
    "title": "9  Pós-estratificação com modelos",
    "section": "9.5 Other materials",
    "text": "9.5 Other materials\n\nMultilevel regression and post-stratification case studies, by Lopez-Martin, J., Phillips, J. H., & Gelman, A. (2022). O que achei legal desse material é que ele inclui uma série de estudos de caso, com dados e código, que mostram como aplicar MRP em diferentes contextos. Vale voltar nele depois, especialmente quando for aplicar MRP.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pós-estratificação com modelos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-9.html#anotações-de-aula",
    "href": "aulas/aula-9.html#anotações-de-aula",
    "title": "9  Pós-estratificação com modelos",
    "section": "9.6 Anotações de aula",
    "text": "9.6 Anotações de aula",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pós-estratificação com modelos</span>"
    ]
  },
  {
    "objectID": "listas/listas.html",
    "href": "listas/listas.html",
    "title": "10  Listas",
    "section": "",
    "text": "10.1 Lista 1\nNesta tarefa implementamos 3 desenhos amostrais (AAS, AAS com estratificação e por conglomerados) usando dados de Macaé extraídos do Censo de 2010. O objetivo é avaliar o desempenho de cada desenho em vista da estimação da proporção de indivíduos alfabetizados no município. Observamos que a estimação no desenho estratificado é melhor que no desenho AAS do ponto de vista da variância da estimativa, mas há espaço para melhora a partir da escolha de variáveis de estratificação mais adequadas. O desenho por conglomerado, como esperado, possui variância amostral maior que o desenho AAS. A magnitude dessa diferença varia de acordo com diferentes escolhas de tamanho de conglomerados e número de entrevistas em cada conglomerado. [link do pdf]\nNota: 100/100",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Listas</span>"
    ]
  },
  {
    "objectID": "listas/listas.html#lista-2",
    "href": "listas/listas.html#lista-2",
    "title": "10  Listas",
    "section": "10.2 Lista 2",
    "text": "10.2 Lista 2\nNesta tarefa, reformulei para o formato online o questionário da pesquisa “Jogo do Bicho” do Datafolha (1988), com adaptações que o tornaram mais curto e focado na percepção sobre a legalização do jogo e na quantificação de quem já jogou. Evitei menções à ilegalidade e destaquei o anonimato para reduzir o viés de desejabilidade social. Também utilizei técnicas de branching, linguagem suavizada e perguntas mais concretas, seguindo recomendações metodológicas de @smyth2016designing e @krosnick2018questionnaire. As perguntas de perfil e mais sensíveis foram deixadas para o fim, com exceção da cidade e idade, que servem para filtrar os respondentes. [link do pdf]",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Listas</span>"
    ]
  }
]